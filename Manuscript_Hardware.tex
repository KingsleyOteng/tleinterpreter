\documentclass[10pt]{article}[draft]
%% \usepackage{natlib}
\usepackage[T1]{fontenc}
\usepackage{physics}
\usepackage{amssymb}
\newlength{\rulethickness}
\setlength{\rulethickness}{1.2pt}
\newlength{\rulelength}
\setlength{\rulelength}{15cm}
\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amscd}
\usepackage[colorlinks]{hyperref}
\usepackage{setspace}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{soul}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{blindtext}
\usepackage{lscape}
\usepackage[linesnumbered,ruled]{algorithm2e}

%%\bibliography{bibtex.bib}

%% 'mass' instead of 'measure'
%% 'approaches with probability one' instead of 'tends' 'in the limit'
%% Please check - the probability mass function is used for discrete vaeriables and the probability density function is used for continous variables

%%denoising chapter
%% should we use the phrase Quantum Mechanics (QM)


\linespread{1.25}
\NewDocumentCommand{\inn}{mo}{%
	\langle #1\rangle
	\IfValueT{#2}{^{}_{\mspace{-3mu}#2}}%
}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\begin{document}
%%	\begin{itemize}
%\title{Introduction:  An Overview of High Performance Computing.}
\title{Overview.}
\date{}
\maketitle


\begin{center}
	\subsection*{Application Matters.}
\end{center}
	
The goals of pure scientific research differ from that of engineering research; an engineers intentions may be distinguished from \st{the intentions} that of a mathematician who undertakes a study in a fundamental problem that remains yet unsolved or the intentions of a scientist, whose goal is to validate a standing hypotheses or otherwise therefore partake in the intellectual pursuit of knowledge. 	Although the classic engineering method can often be difficult to separately distinguish, to an engineer gaining practical applications from research is usually the ultimate goal; in  engineering research, there is typically less of a concern on intellectual pursuits - although engineering should never be discounted as being wholly less intellectual.  

\vspace{0.25in}
\subsection*{A Good Model Matters.}
%%https://arxiv.org/pdf/2008.01069.pdf

One of the central problems in machine learning is that of \emph{model selection} and \emph{model calibration}. {Model selection} allows researchers to minimize a certain risk function in a statistically valid way and thereby most account for the underlying mechanism generating the data ; this may also give due consideration for other model properties such as parsimony, efficiency, consistency and  the specific use of the model. Model selection considers narrow data questions in the research  problems, such as whether the data has been generated by a normal distribution or from other non-normal distributions. Conditioned on the model, the process of fitting the various coefficients is called \emph{ model calibration}. Making analysis of machine learning algorithms even more challenging, models require an ever-increasing number of parameters as \st{simulations} forecasting needs become more precise. For example, deep learning is an effective algorithm which will essentially break down a neural network into a single layer with two or more hidden layers; although it may contains (in principle) an infinite number of hidden layers. Model calibration unfortunately is usually a process of trial and error, where the most appropriate values for the  coefficients are found.  

\subsubsection*{Model Averaging.}

 Another technique termed \emph{model averaging} avoids the selection of one specific model altogether, and instead uses a probability weighted averages over all the different models by weighting each model with a posterior probability. In addition to an apriori probability for each model, an apriori probability has to be specified for the distribution of each model’s parameters.  In general,  model weights in model averaging are complicated integrals, but there  approximate formulae which may be used when there is  sufficiently large sample sizes. The combining statistical results for an arbitrary function across a space of models $\{M\}$ relies on the estimation of the model weight $P[M|D]$:
\begin{equation}
	\inn{f(a_c)} = \sum_M 	\inn{f(a_c)}_M P[M|D]
\end{equation}
where given a base model $M_0$  we denote $\{a_c\}$ as the set of one or more common parameters used to describe a dataset $\mathcal{D}$. Note that therefore  all  models in $\{M\}$ will 
contain $M_0$, in the sense that marginalizing over additional parameters $\{a_m\}$ reduces them 
to $M_0$; the set of $a_M$ will therefore depend on our choice of model $M$.  The set of all parameters $\{a\}$ will be the union of $\{a_c\}$ and $\{a_m\}$. Should we therefore wish to estimate the model parameter $a_0$ over the set $\{M\}$ with $N_M$ models, we determine the mean $\inn{a_0}$ \cite{kass1995bayes}:
\begin{equation}
	\inn{a_0} = \sum_M  \inn{a_0}  P[M|D]
\end{equation}
therefore a variance:
\begin{equation}
\begin{split}
\sigma_{a_0}^2 & = 	\inn{a_0^2} - 	\inn{a_0}^2  \\
& =  \sum_{i=1}^{N_M}  \inn{a_0^2}_i  P[M_i|D] - \Biggl( \sum_{i=1}^{N_M}  \inn{a_0}_i  P[M_i|D] \Biggr)^2 \\
& =  \sum_{i=1}^{N_M} \sigma_{a_0,i}^2 P[M_i|D] +  \sum_{i=1}^{N_M}  \inn{a_0^2}_i  P[M_i|D] - \Biggl( \sum_{i=1}^{N_M}  \inn{a_0}_i  P[M_i|D] \Biggr)^2 \\
\end{split}
\end{equation}
such that when the model weights are equal we obtain a variance over the space of models $\{M\}$,
\begin{equation}
	\begin{split}
	\sigma_{a_0}^2 =  \sum_{i=1}^{N_M}  \inn{a_0^2} w - \Biggl( \sum_{i=1}^{N_M} \inn{a_0^2} \Biggr)^2 w^2
	\end{split}
\end{equation}
where the weight is given as $w = {1}/{N_M}$. In the special case of a two models $M_1$ and $M_2$:
\begin{equation}
	\begin{split}
	P[M_1|D]  & = 1 - p \\
	P[M_2|D]  & =  p \\
	\end{split}
\end{equation}
Now where the parameter estimation over $\mathcal{D}$ strongly suggests a preference for $M_2$, such that $p$ is large:
\begin{equation}
	\begin{split}
		\inn{a_0} & = \inn{a_0}_2 + ( \inn{a_0}_1 - \inn{a_0}_2) (1- p), \\
		\sigma^2_{a_0} & \approx \sigma^2_{a_0,2} +[ (\sigma^2_{a_0,1} - \sigma^2_{a_0,2})   + ( \inn{a_0}_1 - \inn{a_0}_2)^2 ](1- p),
	\end{split}
\end{equation}
Therefore with large but non-zero $p$, mean and variance corrections to $a_0$ from $M_1$ due to including $M_2$, however limited, will be due to the differences between $M_2$ and $M_1$. In the limit of $p \rightarrow 1$, the statistical results will be expectedly that of $M_2$.
\vspace{0.25in}
\subsubsection*{Step 1: You Need a Model.}


Contrary to popular belief, access to more computing power is not a research panacea. Manifold increases in computing power can never prove that model selection has yielded \emph{good predictive performance}; in fact evidence may only be provided that model selection \st{to achieve} has achieved over fitting. Never set your model selection criteria solely based on computing power. Model selection must be supported by understanding the underlying  \% pattern recognition \% algorithms, not the number of available processor cores or threads. However your pattern recognition algorithm  must be general enough to exhibit \emph{model robustness}, even when trained against outliers. Model robustness is the degree that a model’s performance remains stable when applied to new data versus performance on data that was used to train the model.  Note that the application of models as tools, requires performance to be consistent.  Note: ones faith in a pattern recognition model as a tool will deteriorate when its performance is deemed unstable, particularly where the cause is difficult to appreciate. Additionally, any minor deviations from the forecasted performance may point to more significant issues that require attention - these may include outlier data, a biased estimator or omitted regressors. Ideally, model performance should never deviate significantly.     

The existence of miniscule particles in molecular dynamics was predicted using models developed for high performance computing more than a decade before the first miniscule particle was observed  by theoretical chemists (Reference 2013 Noble Prize in Chemistry). Therefore this continued development of high performance computing architectures will hold great promise to the scientific community. This book contains some of the  heterogeneous parallel computing  tools you need to develop your own AI pattern recognition algorithms.

\vspace{0.25in}

\subsubsection*{Heterogeneous Parallel Computing Helps Discover Theories.}

	%%Lesson 2: Heterogeneous Parallel Computing Helps Discover Theories 
Consider the following approach to develop efficient pattern recognition algorithms. First, you apply your \st{heterogeneous} parallel computing toolkit to compute one optimization per cluster node, in our complex phenomenon. Then compute one final optimization across \st{all} the sets of cluster nodes; noting for instance that optimization for intra-cluster and inter-cluster weights are sources of instability that assigning the problem to a heterogeneous parallel computing platform  must incorporate in order to reduce estimation errors. The heterogeneous parallel computing tools have generalized  sources of instability; however, they do not directly inform you about the main source that propagates the instability. Second, we formulate a model that connects our optimizations to  \st{these sources through} a \emph{theory}. This theory is essentially a system of equations that hypothesizes a particular cause?effect mechanism. Third, now our model has a wide range of testable implications that go beyond the observations predicted by the heterogeneous parallel computing tools in the first step. A successful model will predict events out-of-sample. Moreover, it will explain not only predictive performance ($x$ is an independent variable of a target $y$) but also poor predictive performance (failed tracking of a target y is due to significant noise). 
	
In the above discovery process, heterogeneous parallel computing allows us to decouple model optimization across different architecture programming models and  in \st{the} solving of the actual problems. Pattern recognition and deep learning are often criticized for being  ?black boxes, even to their creators? (castelvecchi2016can, bathaee2017artificial), ?having a built in bias? in their assumptions and having inherent "implementation bottlenecks" (bathaee2017artificial). Considering the complexity of efficient pattern recognition and deep learning applications, it is unlikely that a researcher will be able to uncover the sources of bottlenecks of a programming model by a \underline{simple} visual inspection of the data structure  or by running \st{a more simulation} more simulations. Classical parallel programming does not allow this complete decoupling of multiple architectural programming models from  solving the actual problems. However once \st{the} a model has been trained, it stands on its own feet. In this way, we focus our efforts on developing the model, not the  heterogeneous parallel computing approach, to improve our  predictions. 
	
In the above anecdote, the model, not an approximation error produced by  heterogeneous parallel computing \st{approach} architecture, was the source of our problems The approximation error may be low, and it \st{was} may not be based on some untestable theory. It is true that a model may not have been validated without the help of parallel computing techniques, but once the model was validated, the heterogeneous parallel computing approach  played no role in the decision to use the model in a given application. The most effective platform for pattern recognition is  heterogeneous parallel computing. These, massively multiprocessing graphics units with general-purpose programming capabilities present the most likely architectures long-term for low-cost high-performance processing. However the biggest challenge with the efficient implementation of pattern recognition and deep learning algorithms to heterogeneous parallel computing architectures remains the overhaul of existing application design methodologies to thereby achieve  efficient implementation. You may apply a serial pattern recognition algorithm successfully with  heterogeneous parallel computing to improve predictions, although it may not \st{necessarily} computationally be the most efficient implementation  of the algorithm (particularly if your dealing with very large datasets). 

\newpage
\begin{center}
	\subsection*{How Scientists Use Heterogeneous Parallel Computing.}
\end{center}
Heterogeneous parallel computing  focuses on achieving execution throughput using massively large amounts of threads; this is, commonly referred to as throughput-oriented design. That development of  the underlying  parallelized program  need not necessarily require a \st{overly specified}  the researcher to have parallel  computing  expertise, has led many to erroneously conclude that heterogeneous parallel computing will perform well on all tasks. In that view, heterogeneous parallel computing  is merely a ?moniker,? a computing platform from which no particular understanding is required. It is fuelled by popular industrial applications of parallel computing, where the search for low-latency high-throughput outweighs the added complexity required to understand the model. This universal high-throughput view of heterogeneous parallel computing is a misconception.  For applications that require only a few threads, execution of a program on modern multicore microprocessors will have much lower latency than many-thread computing, achieving much higher throughput.  
	
A review of recent scientific breakthroughs reveals radically different uses of heterogeneous parallel computing in science, including the following:
	
\begin{itemize}
	\item Existence: Heterogeneous parallel computing  has been deployed to evaluate the plausibility of a theory across all scientific fields, even beyond the empirical sciences. Notably, ML algorithms have helped make heterogeneous discoveries. ML algorithms cannot prove a theorem, however they can point to the existence of an undiscovered theorem, which can then be conjectured and eventually proved. In other words, if something can be predicted, there is hope that a mechanism can be uncovered (Gryak et al., forthcoming).
	
	\item  \emph{Importance}: Using heterogeneous parallel computing resources and pattern recognition algorithms, researchers  determined the relative informational content of features for explanatory or predictive purposes in  deoxyribonucleic acid  (DNA) sequence data (cowell2011probabilistic,cowell2007gamma). Heterogeneous parallel computing returns the entire sequence of a locus and can combine many more loci in multiplexes; this is considerably more information  compared to the old classic capillary gel method. Models based on heterogeneous parallel computing allow for much higher discriminating power. For example, the  quantitative \emph{EuroForMix} and \emph{DNAxs/DNAStatistX}  models which both use maximum likelihood estimation (MLE) using a $\gamma$ distribution for modeling probabilistic genotyping follows these steps:
	\begin{enumerate} 
		\item  With a database of $N$ individuals, each is considered as a  reference profile, there genotypes are compared to the crime scene swab $O$.; 
		\item Consequently, a likelihood ratio can be generated for every individual in the database, where the alternatives propositions are:  (a) $H1$: Candidate n is a contributor to the evidence profile $O$, (b) $H2$: An unknown person is a contributor to the evidence profile $O$;  
		\item All contributors to the profile that are not  considered as the reference profile, are then designated as unknown and deemed unrelated to the reference profile, 
		\item  Compute a \emph{likelihood ratio} (LR) to sum the weight of relevant genotype contributors. Consequently, for a well-represented DNA profile, the majority of candidates will return a low LR<1, which means that they will be eliminated from the investigation; one or more may return LR>1, and they are forwarded to the prosecuting authorities for further investigation; note it is assumed that autosomal markers are independent and in Hardy?Weinberg equilibrium. 
\end{enumerate}

Thus, a quantitative model for probabilistic genotypics does not uncover the underlying genotypes, it does discovers multiple contributors that should be considered for further investigation.
 These {quantitative} models [12?18] are the most complete DNA profiling models because they take into account all  the peak height information of genotype contributors in order to assign numerical values to the weights. The DNAxs/DNAStatistX model in particular supports parallel computing, allowing operations to be delegated to a heterogeneous parallel computing cluster, by parallelizing over independent function optimizations. 
	%%\item 
	\item \emph{Causation}: Heterogeneous parallel computing are often utilized to evaluate risk metrics following these steps: (1) Fit a ML algorithm on historical data to predict outcomes, absent of an effect. This model is nontheoretical, and it is purely driven by data (like an oracle); (2) collect observations of outcomes under the presence of the effect; (3) use the ML algorithm fit in (1) to predict the observation collected in (2). The prediction error can be largely attributed to the effect, and a theory of causation can be proposed (Varian 2014; Athey 2015).
	\item \emph{Reductionist}: parallel programming tools  are essential for handling  massive amounts of, high-dimensional, complex data sets. For example execution of parallel code requires managing parallel threads and resources.  And with this huge quantity of data, much of the computation can be done on different parts fof the data in parallel, although they have to be reconciled at some point. 
	%%\item  
	\item \emph{Retriever}: Heterogeneous parallel computing  is used in banking datacentres to search for areas of financial risk at a level of accuracy and speed conventional microprocessors can not achieve. For instance, every night heterogeneous parallel computing clusters are fed millions of trades and derivative  in search of risky positions. Once they find one position in a portfolio with a high probability of containing a concealed, risk, cross-asset platforms  again powered by heterogeneous clusters, can be pointed to particular positions in the portfolio, where traders  and risk managers will use tools to scrutinize the positions. A second example are debt tranches. Calculating default risk is a prediction problem rather than a qualitative problem. A powerful heterogeneous parallel computing clusters can detect an anomalous observation in a tranche based on a complex relationships it has identified in the data, even if that relationship is not readily explainable to us (Hodge and Austin 2004).
	Rather than replacing an entire model, heterogeneous parallel computing plays a critical role helping researchers develop models based on increasing the accuracy and highlighting relationships between variables based on strong empirical evidence, Furthermore, heterogeneous parallel computing provides an opportunity for researchers to apply model-driven techniques to develop high-tailored models for specific scenarios., rapid prototyping and assessing a range of competing models. 
%%	\end{itemize}
	
\newpage
\subsection*{Two Limitations of Parallel Computing}
		
		The dark side of heterogeneous parallel computing flexibility is that, in inexperienced hands, these algorithms can easily yield no accuracy or speed-up gains. The primary limitation in parallelizing an application is a divergence between the simplicity \st{complexity} of sequential programs and the added programming complexity of parallelizing a program for heterogeneous parallel computing  (known as the \emph{parallel complexity theory}). We can distinguish between two types of limitations: the limit in the level of speed-up due to the parralelizable function $\alpha$ of application and the added latency in the execution time of a task in the  absence of parallel computing . Figure 1.1 summarizes how heterogeneous parallel computing deals with both kinds of limitations.
		
	\newpage
	\subsection*{Amdahl's Law}
	%% \st{Train Set Overfitting} 
		
	The problem with confounding parallel execution with the sequential portion of the application  is called \emph{Amdah's Law}, that is: the larger the parallelizable fraction $\alpha$ of an application, the larger the speed-up. Parallel execution limitations that are due to a limit in the parallelizable portion of an application result from the accumulated lag in execution time of operations that can not be parallelized or are not worth parallelizing.  Severe limitations in throughput will be caused by large sequential fractions of the program or limited use of fractions of the program that have  parallel segments of the program. More generally,if a fraction $r$ of our serial program remains unparallelized, then Amdahl's law says we can't get a speedup better than $1/r$. For example, $75\%$ of our serial program has been parallelelized therefore $r = 1 - 0.75 = 0.25$; so we can not obtain a speed-up better than $4$ times. Therefore if a fraction $r$ of our serial program is ?inherently serial,? that is, it cannot possibly be parallelized, then we can't possibly get a speed-up better than $1/r$ Thus even if $r$ is quite small, we still can't possibly get a speed-up better than 100; unless virtually all of a serial program is parallelized, the possible speed-up is going to be very limited  which in turn will lead to limits in achievable speed-up when parallelizing a given sequential program. Parallel computing researchers are keenly aware of this problem, which they address in three complementary ways. 
	
	The first approach to correct for train set over-fitting is evaluating the generalization error, through resampling techniques (such as cross-validation) and Monte Carlo methods. Appendix A describes these techniques and methods in greater detail. The second approach to reduce train set over-fitting is regularization methods, which prevent model complexity unless it can be justified in terms of greater explanatory power. Model parsimony can be enforced by limiting the number of parameters (e.g., LASSO) or restricting the model?s structure (e.g., early stopping). The third approach to address train set over-fitting is ensemble techniques, which reduce the variance of the error by combining the forecasts of a collection of estimators. For example, we can control the risk of over-fitting a random forest on a train set in at least three ways: (1) cross-validating the forecasts; (2) limiting the depth of each tree; and (3) adding more trees.
		In summary, a backtest may hint at the occurrence of train set over-fitting, which can be remedied using the above approaches. Unfortunately, backtests are powerless against the second type of over-fitting, as explained next.
		
		Amdahl's law often motivates task-level parallelization. Although some of these smaller activities do not warrant fine-grained massive parallel execution, it may be desirable to execute some of these activities in parallel with each other when the data set is large enough. 	
		
		\subsubsection*{Severe Limitations in Throughput}
		
		Imagine, a friend claims to have developed a strategy to win at blackjack in casino gambling. His strategy uses a complex model which increases the odds of a win, but requires he sit at one poker table for long hours. As a consequence, if he moves tables the odds of winning drop markedly. Another group of friends, who also consider themselves tp be seasoned gamblers, have developed another strategy for beating the house at blackjack. Their approach does not require one player sit at one poker table, but it does require a large number of players coordinate their betting strategy against the casino. This second coordinated approach surprisingly appears to generate much larger returns than the single player strategy. To evaluate the legitimacy of both approaches, you decide to adjust winnings for the fact that the second technique requires coordinating teams of players who on aggregate place much larger bets. After adjusting for the bet size, your friends single gambler technique occasionally proves to be the more profitable strategy. Likewise, researchers running many-threads on the same models are more likely to make a false conclusion. By using heterogeneous parallel computing  with a model, without due consideration for the architecture and input data structures it is almost guaranteed that most researchers will arrive at a false conclusion; performance of a parallel program may occasionally run slower than a sequential program or have erratic performance. This performance fallacy comes from failing to account for the non-intuitive ways of approaching  a problem, that is required for parallel programming. 
		
		Another example of \st{severe  limitations in the throughput achievable} throughput limitations in heterogeneous parallel computing clusters, occurs when a researcher develops a model and fails to tweak a program for the differences between the  instruction sets of master-worker \emph{heterogeneous} system.. That rapidly porting a sequential program, can be a futile exercise that will inevitably end with severe limitations in throughput (poor portability). Instead, the researcher should have spent her time investigating how the research process misled her into achieving poor model performance. In other words, a poorly performing parallel program is an opportunity to fix the research process, not an opportunity to fix a particular model. Most published advances in pattern recognition are likely faulty, due to severe limitations in throughput. Heterogeneous parallel computing did not cause the current crisis in the search for efficient pattern recognition algorithms (). That crisis was caused by the widespread misuse of many-threaded systems in pattern recognition, and poor porting in particular. Heterogeneous parallel computing can help deal with the problem of limited execution speeds in three ways. 
		  
		  First, we can keep track of how many independent tests a researcher has run, to evaluate the probability that at least one of the outcomes is a false discovery (known as family wise error rate, or FWER). 
		  
		  %The deflated Sharpe ratio (Bailey and L�pez de Prado 2014) follows a similar approach in the context of %backtesting, as explained in Section 8. It is the equivalent to controlling for the number of lottery tickets %that your friend bought. 
		  
		  
		  Second, while it is possible to have long-latency or throughput limitations for a few threads completing an operation , it is unlikely that there will be latency or throughput limitations across a majority of threads. An important observation, has been that it is more difficult to reduce latency due to arithmetic operations or memory access, than it is to improve throughput by increasing the chip area and power (reference orangebook). The design goals are often therefore to optimize throughput across a massive number of threads. This is the approach followed by designers, introducing sub-optimality by allowing for long-latency within individual threads by reducing the area of pipelined memory and the power of the arithmetic units; the reduced area of the memory and arithmetic units allows designers to achieve increased chip density thereby increasing execution throughput. 
		  
		  Third, we can use historical series to estimate the underlying data-generating process, and sample synthetic data sets that match the statistical properties observed in history. Monte Carlo methods are particularly powerful at producing synthetic data sets that match the statistical properties of a historical series. The conclusions from these tests are conditional to the representativeness of the estimated data-generating process (AFML, chapter 13). The main advantage of this approach is that those conclusions are not connected to a particular (observed) realization of the data-generating process but to an entire distribution of random realizations. Following with our example, this is equivalent to replicating the lottery game and repeating it many times, so that we can rule luck out.
		In summary, there are multiple practical solutions to the problem of train set and test set over-fitting. These solutions are neither infallible nor incompatible, and my advice is that you apply all of them. At the same time, I must insist that no backtest can replace a theory, for at least two reasons: (1) backtests cannot simulate black swans ? only theories have the breadth and depth needed to consider the never- before-seen occurrences; (2) backtests may insinuate that a strategy is profitable, but they do not tell us why. They are not a controlled experiment. Only a theory can state the cause?effect mechanism, and formulate a wide range of predictions and implications that can be independently tested for facts and counter-facts. Some of these implications may even be testable outside the realm of investing. For example, the VPIN theory predicted that market makers would suffer stop-outs under persistent order flow imbalance. Beyond testing whether order flow imbalance causes a reduction in liquidity, researchers can also test whether market makers suffered losses during the flash crash (hint: they did). This latter test can be conducted by reviewing financial statements, independently from the evidence contained in exchange records of prices and quotes.
		
		\section*{Outline}
		
		This book offers users an unhurried guide to building efficient pattern recognition applications with the help of heterogeneous parallel computing. To that objective, each chapter uses what we have developed in the previous ones. Each chapter (except for this introduction) contains \st{an} code examples where the methods explained are put to the test\st{ in code examples}.
		
		The first step in building pattern recognition algorithms for heterogeneous parallel computing is \st{data} \emph{parallelism}, that is to detail how the problem might be broken down into smaller independent computations which can be run  \st{parallelised} across cooperating workers, thereby executing a program faster.  In heterogeneous parallel computing settings, programs are run on threads with distributed memory. We use a large number of threads to enhance the execution throughput by running typically computationally intensive portions of the program as parallel computations.  However, pattern recognition in heterogeneous parallel computers have a  high cost of communication, as processors are required to communicate by exchanging messages. Intuitively multiple processors will generate many-threads to collaborate on training one model, will reduce the overall training time, but the communication cost between processors generally limits the system scalability. A relatively small proportion of the time (i.e., computation-to-communication ratio) is spent in parallel computations when training a deep learning model on a heterogeneous systems with low-speed interconnect; i.e., the throughput across groups of processors could fall well short of a single processor. Section 2 explains how to develop communication-efficient parallel algorithms without giving up any  convergence guarantees. Most of the discussion centres on  the \emph{accelerator model}, but at the core of the solution sits a heterogeneous parallel computing technique: the graphical processing unit (GPU).
		
		%https://arxiv.org/pdf/2003.06307.pdf
	
		Many pattern recognition research questions consider the notion of similarity measure or distance function. For example, we may be interested in understanding how closely related two variables are. Different similarity measures are used for different \st{intensity distortions} attributes \st{between distributions vectors} within the data. Measures are broadly applied to single modality data and multi modality data \st{measures}.  Traditional \st{machine learning} pattern recognition algorithms have focused on single modality (e.g., audio  or text ). We denote a single-modality domain set as $\chi =  \chi^{(1)} $. Given the data $x$ consisting of a single modality, where $x \in \chi^{(1)}$, which is the domain set of the single modality.  Therefore a single-modality measures will consider only a single type of feature to calculate distance or similarity.  For example, Sum of absolute difference is a single-modal measure that is used for block comparison in image processing; it calculates the difference between two corresponding vectors in space. Single modality measures may be calculated by independent computations assessed at each spatial point and multimodal measures determine the mutual information (statistical) or correlation dependence (functional) of distributions, where the  distribution is assumed to be a realization of an underlying discrete random variable; the aim is to minimize distances between similar pairs whilst separating dissimilar pairs that fall with a certain margin.. When parallelizing an algorithm, single modality measures are readily adaptable to single instruction multiple data (SIMD) instruction sets and therefore architectures such as GPUs.  
		
		However the human body generates information from various data forms including audio, image and spectrograms. More robust modelling is required than single-modality, with researchers developing algorithms that integrate the modalities of data including images, text, and spectrograms. The main idea in multi-modal machine learning is that different modalities provide complementary information on the human body (e.g., sound, identifying objects in an image, the frequency of words in text or sensory function).  By contrast a multi-modal measures will incorporate multiple  features to determine similarity measures or distance functions. Deep learning algorithms made integrating different signal sources significantly easier and therefore has lead to the increase use of multi-modal data.  For example, the Jaccard similarity coefficient as a multi-modal measure will assess similarity between two sets by considering the presence or absence of  values. Lets \st{We} denote a \st{the} multi-modality domain set as $\chi =  \chi^{(1)} \times ... \times \chi^{(K)} $. Given the data $x := [x^{(1)}, ... , x^{(K)}]$ consisting of $K$ modalities, where $x^{(k)}  \in \chi^{(k)}$ is in the domain set of a k-th modality. Multi-modality measures \st{however} may require the estimation and combining of joint and marginal probability mass functions for the underlying discrete random variables from data. Methods for computing probability mass functions  are parallelizable but with varying degrees of difficulty. Note single-modality measures may also be combined with multi-modality measures to generate more comprehensive analysis. For instance, the sum-of-squared-difference may be used for comparing numerical data and the Jacard similarity coefficient used to compare the accuracy of transmitted words. In summary, given single-modal and multi-modal measures, te ideal approach will be dependent on the feature or number of features that must be analysed.
		
		
		Chapter 3 provides an information-theoretic frameworks for estimation of complex probability mass functions from noisy data. In particular, this will allows us to define similarity and distance measures with minimal assumptions regarding the underlying variables that characterize a distribution. One of the applications of distance matrices is to study whether some variables are more closely related among themselves than to the rest, hence forming clusters. Clustering has a wide range of applications across finance, like in asset class taxonomy, portfolio construction, dimensionality reduction, or modelling networks of agents. A general problem in clustering is finding the optimal number of clusters. Chapter 4 introduces the ONC algorithm, which provides a general solution to this problem. Various use cases for this algorithm are presented throughout this book. Clustering is an unsupervised learning problem. Before we can delve into supervised learning problems, we need to assess ways of labelling  data. The effectiveness of a supervised ML algorithm greatly depends on the kind of problem we attempt to solve. For example, it may be harder to forecast tomorrows S\&P 500 return than the sign of its next 5\% move. Different features are appropriate for different types of labels. Researchers should consider carefully what labelling method they apply on their data. Section 5 discusses the merits of various alternatives.
		
		
		%%http://users.cecs.anu.edu.au/~ramtin/papers/2010/SPM_2010.pdf
	   \begin{landscape}
		\begin{table}
		\begin{tabular}{|c|c|c|c|}
				\hline 
			\underline{Formula} 								&  \underline{Description} & \underline{Type} & \underline{Modality}\\
			\hline
			$D_{MD} [I,J] = {\sum_{\chi \in \Omega}  \biggl|I[x] - J[x]\biggl|^p}$		 & Minkowski distance (MD) & Distance & Single-modality  \\
			\hline
			$D_{TMD} [I,J] = {\sum_{\chi \in \Omega} \text{abs} \biggl[I[x] - J[x]\biggl]}$		 & The Manhattan distance (TMD) & Distance & Single-modality \\
		\hline 
		$D_{ED} [I,J] = \sqrt{\sum_{\chi \in \Omega} [I[x] - J[x]]^2}$		 & Euclidean distance (ED) & Distance & Single-modality \\
		\hline
		$D_{SSD} [I,J] = \sum_{\chi \in \Omega} [I[x] - J[x]]^2$		 & Sum of squared difference (SSD) & Distance & Single-modality \\
		\hline
		$D_{SAD} [I,J] = \sum_{\chi \in \Omega} | I[x] - J[x] |$		 & Sum of absolute difference (SAD) & Distance & Single-modality \\
			\hline
		$S_{JSC} [I,J] = \dfrac{\sum_{\chi \in \Omega} I[x] J[x]}{  \sum_{\chi \in \Omega} I[x] + \sum_{\chi \in \Omega} J[x]  - \sum_{\chi \in \Omega} [ I[x] J[x] ]}$		 &  Jacard Similarity Coefficient (JSC) & Similarity & Single-modality  \\
		\hline
		$S_{CC} [I,J] = \dfrac{(I[x] -\mathbb{E}  I[x] ) (J[x] - \mathbb{E} J[x])}{{\sigma[I]} \sigma[J]}$		 &  Cross correlation (CC) & Similarity & Single-modality  \\
		\hline
		$S_{NCC} [I,J] = \dfrac{I[x] J[x]}{\sqrt{\mathbb{E} [I[x]^2] \mathbb{E} [J[x]^2]}}$		 &  Normalized  correlation (NCC) & Similarity & Single-modality \\
		\hline
		$S_{GC} [I,J] = \dfrac{1}{d} \sum_{i=1}^d S_{CC}\Bigl( \dfrac{\delta I}{\delta X_i}, \dfrac{\delta I}{\delta X_j} \Bigl)$		 & Gradient  correlation (GC) & Similarity & Single-modality \\
		\hline
		$S_{MI} [I,J] =  \sum_{i} \sum_{j} p_{I,J} [i,j] \log \dfrac{p_{I,J} [i,j] }{p_{I} [i]  p_{J} [j] }$		 & Mutual  information (MI) & Similarity & Multi-modality \\
		\hline
		$S_{NMI} [I,J] = \dfrac{2 S_{MI} [I,J] }{H[I] + H[J]}$		 & Normalized mutual  information (NMI) & Similarity & Multi-modality \\
		\hline
		$S_{CR} [I,J] = \dfrac{\sigma^2[\mathbb{E}[J|I]]}{\sigma^2[I]}$		 & Correlation ratio (CR) & Similarity & Multi-modality \\
		\hline
			$S_{JSD} [I,J] = \dfrac{\sigma^2[\mathbb{E}[J|I]]}{\sigma^2[I]}$		 & Jensen-Shannon (JS) & Similarity & Multi-modality \\
		\hline
		\end{tabular}
	   \end{table}
  \end{landscape}
	
		
		
		
		\st{De-noised covariance matrices can be very useful for deriving distance metrics from linear relationships. Modelling non-linear relationships requires more advanced concepts. }
		
		
		
		
		
		----------->
		
	
		
	
		AFML warned readers that back-testing is not a research tool. Feature importance is. A backtest cannot help us develop an economic or financial theory.
		
		
		\item 1.6 Audience
		\item 1.7 Five Fallacy's of High Performance Computing
		\item -1.7.1 ML Is the Holy Grail versus ML Is Useless
	\item 	- 1.7.2 ML Is a Black Box
		\item -1.7.3 Finance Has Insufficient Data for ML
		\item -1.7.4 The Signal-to-Noise Ratio Is Too Low in Finance
	\item 	- 1.7.5 The Risk of Overfitting Is Too High in Finance
		\item 1.8 The Future of Financial Research
	\item 	1.9 FAQ's
	\item 	- In Simple Terms, What Is ML?
	\item 	-  How Is ML Different from Econometric Regressions?
	\item 	- How Is ML Different from Big Data?
	\item 	- How Is the Asset Management Industry Using ML?
	\item 	- And Quantitative Investors Specifically?
	\item 	-  What Are Some of the Ways That ML Can Be Applied to Investor Portfolios?
	\item 	- What Are the Risks? Is There Anything That Investors Should Be Aware of or Look Out For?
	\item 	- How Do You Expect ML to Impact the Asset Management Industry in the Next Decade?
	\item 	- How Do You Expect ML to Impact Financial Academia in the Next Decade?
	\item 	- Isn?t Financial ML All about Price Prediction?
	\item 	- Why Don?t You Discuss a Specific Investment Strategy, Like Many Other Books Do?
	\item 	1.10 Conclusions \\
	\item OpenMP \\
	\item Topology 
	\item Parallel Sorting \\
	\item Parallel linear algebra \\
	\item Mapreduce Paradigm \\
	\end{itemize}

\bibliographystyle{references}

\end{document}



12 Puch-Solis, R.; Rodgers, L.; Mazumder, A.; Pope, S.; Evett, I.; Curran, J.; Balding, D. Evaluating forensic DNA profiles using peak heights, allowing for multiple donors, allelic dropout and stutters. Forensic Sci. Int. Genet. 2013, 7, 555?563. [CrossRef]
13. Puch-Solis, R. A dropin peak height model. Forensic Sci. Int. Genet. 2014, 11, 80?84. [CrossRef] [PubMed]
14. Perlin, M.W.; Legler, M.W.; Spencer, C.E.; Smith, J.L.; Allan, W.P.; Belrose, J.L.; Duceman, B.W. Validating TrueAllele�DNA mixture interpretation. J. Forensic Sci. 2011, 56, 1430?1447. [CrossRef] [PubMed]
15. Taylor, D.; Bright, J.-A.; Buckleton, J. The interpretation of single source and mixed DNA profiles. Forensic Sci. Int. Genet. 2013, 7, 516?528. [CrossRef] [PubMed]
16. Robert, G.C. Validation of an STR peak area model. Forensic Sci. Int. Genet. 2009, 3, 193?199.
17. Bleka, �.; Storvik, G.O.; Gill, P. EuroForMix: An open source software based on a continuous model to evaluate STR DNA profilesfrom a mixture of contributors with artefacts. Forensic Sci. Int. Genet. 2016, 21, 35?44. [CrossRef]
@article{cowell2011probabilistic,
	title={Probabilistic expert systems for handling artifacts in complex DNA mixtures},
	author={Cowell, Robert G and Lauritzen, SL and Mortera, Julia},
	journal={Forensic Science International: Genetics},
	volume={5},
	number={3},
	pages={202--209},
	year={2011},
	publisher={Elsevier}
}
@article{cowell2007gamma,
	title={A gamma model for $\{$DNA$\}$ mixture analyses},
	author={Cowell, Robert G and Lauritzen, Steffen Lilholt and Mortera, Julia},
	journal={Bayesian Analysis},
	volume={2},
	number={2},
	pages={333--348},
	year={2007},
	publisher={International Society for Bayesian Analysis}
}

\item  Importance: heterogeneous parallal computing tools can determine the relative informational con- tent of explanatory variables (features, in ML parlance) for explanatory and/ or predictive purposes (Liu 2004). For example, the mean-decrease accuracy (MDA) method follows these steps: (1) Fit a ML algorithm on a particular data set; (2) derive the out-of-sample cross-validated accuracy; (3) repeat step (2) after shuffling the time series of individual features or combinations of features; (4) compute the decay in accuracy between (2) and (3). Shuffling the time series of an important feature will cause a significant decay in accuracy. Thus, although MDA does not uncover the underlying mechanism, it discovers the variables that should be part of the theory.

//  DNA REFERENCES
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8535381/#B124-genes-12-01559
https://www.eurecom.fr/sites/default/files/jobs/DS_RA_ING_OLIGOARCHIVE_062022_US.pdf
https://publications.aston.ac.uk/id/eprint/43499/1/etls_2020_0340c.pdf
https://thesai.org/Downloads/Volume12No11/Paper_15-DNA_Profiling_An_Investigation_of_Six_Machine_Learning_Algorithms.pdf
%%https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjdg8GI0437AhWM57sIHYrSAmkQtwJ6BAhAEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3ygPF-Qshkc&usg=AOvVaw2_-Sd-mCmcaVtyZ4xlIY_A

Clusters entered the market in the late 1980s and replaced MPPs for many applications. A cluster is a parallel computer comprised of numerous commercial computers linked together by a commercial network. Clusters are the workhorses of scientific computing today and dominate the data centers that drive the modern information era. Based on multi-core processors, parallel computing is becoming increasingly popular.


In a parallel programming context, \emph{unintended nondeterminism} is typically defined as an abnormal program behaviour caused by dependence of a result on the relative interleaving \st{timing} of events executed within a program. I once heard stated that by a presenter at a conference "we should expect no significant performances improvements from heterogeneous parallel computing". Curious, I enquired as to why. She replied, “Because any future performance improvements, particularly as new technologies are introduced into heterogeneous systems, will be plagued by unintended nondeterministic behaviour and heterogeneous parallel computers designs cannot control them.” I retorted that synchronization in heterogeneous CPU-GPU systems, in most cases, had continuously evolved to cater for unintended nondeterminism.

Synchronization remains an open ended engineering problem with many design considerations. In an ideal synchronization, there are  limited constraints to scheduling and therefore high resource utilization. At the same time there are costs to rigorous  global scheduling control. Therefore a trade-off often exists between increasing
synchronization costs and decreased resource idleness. The problems of synchronization do not exist exclusively in parallel systems. Synchronization comes with concurrency; parallelism is a special case of concurrency. There are many different ways to achieve synchronization.  Providing synchronization mechanisms to eliminate race conditions is one of the main purposes of a parallel programming model. In multithreading, synchronization is achieved with explicit synchronization primitives.  In multithreading, synchronization is achieved with explicit synchronization primitives. In hardware-based shared memory, these mechanisms are built on atomic processor instructions. 

