\documentclass[10pt]{article}[draft]
%% \usepackage{natlib}
\usepackage[T1]{fontenc}
\usepackage{physics}
\usepackage{amssymb}
\newlength{\rulethickness}
\setlength{\rulethickness}{1.2pt}
\newlength{\rulelength}
\setlength{\rulelength}{15cm}
%\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amscd}
\usepackage[colorlinks]{hyperref}
\usepackage{setspace}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{soul}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{blindtext}
\usepackage{lscape}
\usepackage{charter}
\usepackage[scaled=0.92]{helvet}
\usepackage[margin=2.5cm]{geometry} % set width of textblock appropriately
\usepackage[english]{babel}
\usepackage{graphicx,tabularx,ragged2e,booktabs}
\newcolumntype{L}{>{\RaggedRight\setlength\parskip{0.2\baselineskip}\arraybackslash}X}
\setlength\extrarowheight{1pt}
\usepackage{caption}
\newlength\mylen
\settowidth{\mylen}{Objective} % measure width of longest word in 1st col.

\usepackage[linesnumbered,ruled]{algorithm2e}

%%\bibliography{bibtex.bib}

%% 'mass' instead of 'measure'
%% 'approaches with probability one' instead of 'tends' 'in the limit'
%% Please check - the probability mass function is used for discrete vaeriables and the probability density function is used for continous variables

%%denoising chapter
%% should we use the phrase Quantum Mechanics (QM)


\linespread{1.55}
\NewDocumentCommand{\inn}{mo}{%
	\langle #1\rangle
	\IfValueT{#2}{^{}_{\mspace{-3mu}#2}}%
}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\begin{document}
	\bibliographystyle{plain}
%%	\begin{itemize}
%\title{Introduction:  An Overview of High Performance Computing.}
\title{Overview.}
\tableofcontents
\date{}
\maketitle


\begin{center}
%%\section{Application Matters.}
\section{Motivation}
\end{center}
The goals in pure engineering research differs from that of scientific research; an engineers intentions may be distinguished from \st{the intentions} that of a mathematician who undertakes a study on a fundamental problem that remains yet unsolved or the intentions of a scientist, whose goal may be to validate a conjecture \st{is to validate a standing hypotheses} or otherwise therefore partake in \st{the} an intellectual pursuit of knowledge. Although \st{the} a classic engineering method can often be difficult to separately distinguish from science - to an engineer, pushing the limits to create what is seemingly possible still remains the ultimate goal \st{gaining practical applications from ones research {is} usually remains the ultimate goal}\cite{vincenti1990}; i.e. in engineering research, there is typically less of a concern on intellectual pursuits - although engineering should never be discounted as \st{being wholly} trivial or less intellectual. In Theodore von Kármán's words \cite{vonKarman1967},\\
\begin{quote}
	"Scientists study the world as it is, engineers create the world that never has been."
\end{quote}

\subsubsection*{Heterogeneous Parallel Computing: Bridging High-Performance Architectures with AI Pattern Recognition}

The existence of miniscule particles in molecular dynamics were first predicted using models developed using high performance computing, more than a decade before the first \st{miniscule} particle was to be observed  by theoretical chemists \cite{nobelchemistry2013}; absent the support of high performance computers (HPC) as  tools, the research field would in large-part have remained stagnant. Therefore, this continued the trend towards the development of new HPC architectures and execution models such as heterogeneous parallel computers, which allow \st{collaboration by} combining multicore CPUs,  high-throughput many-core GPUs and TPUs \st{of heterogeneous processing units} to accelerate computational flows; this holds great promise to the scientific community at-large. However, it is noted that  these systems with increasingly smaller dimensions are able to be integrated onto single chips and then \st{onto} multi-core/many-core architectures without  performance requirements and energy constraints being relaxed \cite{borkar2007thousand}. Although processors have tended to be homogeneous, there is an increasing use of for example multicore CPUs with many-core GPUs in heterogeneous collaboration; i.e. this introduces variations, across cores of architecture including cache memory, communication management with low-complexity cores for low-power applications, multicore CPUs, many-core GPUs, DSPs, ASICs and embedded FPGAs particularly as chip sizes in terms of surface-area have become even smaller.  This book examines tools and techniques in the fields of heterogeneous parallel computing and AI pattern-recognition algorithm applications; both individually and  in combination with each other.
\vspace{0.25in}


\subsubsection*{Step 1, You Need a Model.}
In pattern recognition minimizes an \emph{error probability}, the \emph{Neyman-Pearson criterion} or the \emph{empirical risk} across \emph{M} classes is equivalent to the partitioning of a \emph{multidimensional feature space} into \emph{M} regions \st{which is consistent with a task with {M} classes}; these are problem dependent tasks. Therefore in the context of contiguous regions \(R_i\) , \(R_j\), these regions may then be separated by a decision surface within a feature space, by a \emph{decision function}. The \emph{decision test} assigns \(x\)  into a class  \(w_i\) when
\begin{equation}
g_i(x) > g_j(x) \forall j \neq i
\end{equation}
where given the \emph{discriminant function} \(g_i(x)\) the decision surface is \(g_{ij}(x) = g_i(x) - g_j(x) = 0, i,j=1,2,....,M, i\neq j\). More generally, in learning algorithms the \emph{classifier} or \emph{regressor} is responsible for the categorizing a function or a group of functions \(g(x)\) to a corresponding class or taxonomy. In the context of pattern recognition, our functions denoted by \(g( \cdot )\) are estimated utilizing a finite training dataset represented as \(D \{ (y_i,x_i),i \in \{1,2,\ldots,N\} \}\), based on an appropriate formulaic approach. To underscore this reliance on \(D\), it is often represented as \(g(x; D)\). 

Consider an approximation of a Mean Squared Error (\emph{MSE}) optimal classifier \(E[y|x]\) of the function $g^*$, and investigate the influence of the dataset's finite size, denoted by \(N\), on this estimate. The quality of this approximation  \(g(x;D)\), depends on the training dataset \(D\), which tends to be specific to a particular dataset; with the quality of the estimator then evaluated by computing its mean square deviation from an optimal value. This is represented as, 
\begin{equation}
	\begin{split}
		\frac{1}{|\mathcal{D}|} \sum_{\mathcal{D}} (g(x;\mathcal{D}) - g^*)^2  & = E_{\mathcal{D}} \Bigl[ (g(x;\mathcal{D}) - E[y|x] )^2 \Bigl] \\
		& = \bigr( E_{\mathcal{D}} \bigl[ g(x;\mathcal{D}) \bigl] - E[y|x]  \bigl)^2  + E_{\mathcal{D}} \bigl[ (g(x;\mathcal{D}) \bigl) - E_{\mathcal{D} }[g(x;\mathcal{D})  ])^2 \bigr] \\
	\end{split}
\end{equation}
where the first element $\bigr( E_{\mathcal{D}} \bigl[ g(x;\mathcal{D}) \bigl] - E[y|x]  \bigl)^2 $ is  termed the \emph{bias} of the estimator, and the second element, the \emph{variance}, is represented by  $E_{\mathcal{D}} \bigl[ (g(x;\mathcal{D}) \bigl) - E_{\mathcal{D} }[g(x;\mathcal{D})  ])^2 \bigr]$. This is the \emph{general classifier}.	Note that where an estimator is unbiased, it may still result in a large mean square error if the variance is large; this is termed the \emph{bias-variance trade-off}.  Special case is where there is \emph{a priori} information is available at the decoder, relative to the {general classifier} this knowledge can be harnessed in \st{the classifier or regressor} resulting in both a lower bias and a lower variance.
%%%%%%%%
Contrary to  prevailing opinion, access to more computing power has never been \st{is not} a research panacea. Manifold increases in computing power can never prove that model selection has yielded good predictive performance; in fact, the evidence \st{suggests may only be provided}  tends to suggest that for model selection \st{to achieve} , substantial or excessive increases in computing power often lead to over-fitting or \emph{model complexity} \cite{bengio2012practical, goodfellow2016, srivastava2014dropout, mukherjee2006learning}. Model complexity refers to the multifaceted structure and capacity of a model to capture intricate relationships and subtle features within the data and involves model trade-offs; it is often characterized by several attributes, such as the number of parameters or degrees of freedom, it's functional form, and the level of intricacy  required in the final representation; model complexity is governed by a {bias-variance trade-off}.  Mitigating this underlying \st{bias-variance} trade-off is crucial; minimizing bias and increasing complexity limits under-fitting at the risk through of increasing the variance (i.e. degree of over-fitting). 
  \begin{equation}
  	\text{Total Error} = \text{Bias}^2 + \text{Variance} +  \text{Irreducible Error}
  \end{equation}

  ((add a graphic here ))
 Never set your model selection criteria solely based on the available computing power. Choosing an appropriate regularization constant to minimize the prediction error, through optimizing model generalization and mitigating overfitting, balances this trade-off.  Note that the application of models as \emph{tools} used in prediction requires performance to be \emph{consistent}. Your \st{ones} faith in a pattern recognition model as a useful tool will quickly diminish \st{deteriorate} when its performance is deemed  to be non-stable or non-consistent, particularly where the root cause of this instability or inconsistency is then difficult to appreciate. Additionally, any minor deviations from the forecasted performance may point to more significant issues that require attention - these may include outlier data, biased estimators or omitted regressors. Ideally, model performance should never deviate significantly. Effective model selection must be supported by a thorough understanding of the underlying   pattern recognition algorithm, rather than the number of  processor cores or thread count \cite{hutter2015automated, hastie2009elements, xu2010robustness}. However your algorithm must be general enough to exhibit \emph{model robustness} in the \emph{minimizer}, even when trained against outliers particularly in very large datasets, where there is a lack of regularization in the data or with a \emph{complex hypothesis space}; model robustness is the degree that a model’s performance remains \emph{stable}, versus it's performance on data that was used to train the model.  
  
%% Algorithm development essentially aims to improve data structures and also throughput - where one objective design approach is to minimize resource consumption, termed asymptotic growth or “big-O” notation, a comparison of different algorithm approaches. For instance, a procedure that processes in linear time \(O(n)\) is more efficient than a procedure that takes quadratic time \(O(n^2)\) or log-linear time \(O(n \log n)\). A well-designed procedure in the context of heterogeneous parallel computing resource considers the problem space, the underlying hardware behaviour, computational flow requirements and the available processing power.}


\subsubsection*{And A Good Model Matters.}
%%https://arxiv.org/pdf/2008.01069.pdf

One of the central problems in \st{machine learning} pattern recognition is that of \emph{model selection} and \emph{model calibration}. Model selection considers narrow data questions in the research  problem, such as whether the data has been generated by a normal distribution or from otherwise non-normal distributions. Model selection allows researchers to minimize a certain risk function in a statistically valid way and thereby most account for the underlying mechanism generating the data; this may also give rise to \st{give require} due consideration of other model properties such as parsimony, efficiency, consistency and the intended use of a model.  Post model selection, conditioned on your choice of model, the process of fitting the various coefficients is termed \emph{model calibration}. From the stand point of pattern recognition, model calibration refers to a process of  rescaling predicted probabilities to ensure for each model the most accurate representation of the likelihood of occurrences for different classes in the training data (http://proceedings.mlr.press/v70/guo17a/guo17a.pdf). For example, given 200 forecasts, a each with confidence of 0.8, in a calibrated model we will expect that 160 objects should be correctly classified.  

Let \(\hat{P} : \mathbb{R}_d \rightarrow [0, 1]\) be the predicted probability distribution or the \emph{probabilistic classifier} - further,  consider a dataset with input features \(X\)and corresponding class labels \(Y \in \{0, ... , K-1 \}\); i.e. a model that outputs the predicted probability distribution over our labels, given a set of input features. Consider a model \st{which} that predicts a class \(Y\) with probability \(\hat{P}\). The model is called \emph{calibrated} if \(\hat{P}\) is always the true probability. Clearly, if a model were to be  \emph{perfectly calibrated} therefore,
\begin{equation}
	P \Bigr[ \hat{Y} = Y | \hat{P}  = p \Bigl] = p, \hspace{0.5in} \forall p \in [0,1]
	\label{basic}
\end{equation}
%%\begin{equation}
%%	P \Bigr[ \hat{Y} = y | \hat{P}  = p \Bigl] = p, \hspace{0.5in} \forall p \in [0,1]
%%	\label{basic}
%%\end{equation}
where \(\hat{Y}\) is the model that predicts the class, \(\hat{P}\) is our predicted probability (or confidence prediction), ${Y}$ is the actual class (or actual confidence) and $p$ is the actual accuracy. \st{that is the actual probability of $\hat{Y} = Y$ conditioned on $\hat{Y}$ predicting $p$ is exactly $p$}.  According to Bayesian methodology, when estimating a model, all unknown parameters must be treated as random variables and we should consider therefore all possible values when considering predictions. 

We note however the probability in equation \ref{basic}) cannot be computed using a finite number of samples, since our predicted probability $\hat{P}$ is a continuous random variable; i.e. this  yields a \emph{calibration error}.  To derive the calibration error, we start with the definition of conditional expectation and the  law of total probability:
\begin{align*}
	\begin{split}
	\mathbb{E}[Y|\hat{P}[X]] &= \int_{-\infty}^{\infty} y P(Y=y|\hat{P}[X])dy \\
	&= \int_{-\infty}^{\infty}  y \frac{P(\hat{P}[X]|Y=y)P(Y=y)}{P(\hat{P}[X])}dy \\
	&= \frac{\int_{-\infty}^{\infty}  y P(\hat{P}[X]|Y=y)P(Y=y)dy}{\int_{-\infty}^{\infty}  P(\hat{P}[X]|Y=y)P(Y=y)dy}
	\end{split}
\end{align*}
Hence the probability of error $P  \Biggl( \Bigl| \hat{P}[X] - \mathbb{E}\Bigr[ Y \Bigr| \hat{P}[X]\Bigl] \Bigr| \Biggr)$ is given as,
\begin{align*}
	\begin{split}
P  \Biggl( \Bigl| \hat{P}[X] - \mathbb{E}\Bigr[ Y | \hat{P}[X]\Bigl] \Bigr| \Biggr) &=  P  \Biggl( \Bigl| \hat{P}[X] - \frac{\int_{-\infty}^{\infty}  y P(\hat{P}[X]|Y=y)P(Y=y)dy}{\int_{-\infty}^{\infty}  P(\hat{P}[X]|Y=y)P(Y=y)dy} \Bigr| \Biggr) \\
	&= \mathbb{E}_{(X,Y)\sim P } \Biggl( \Bigl| \frac{\int_{-\infty}^{\infty}  \hat{P}[X] P(\hat{P}[X]|Y=y)P(Y=y)dy}{\int_{-\infty}^{\infty}  P(\hat{P}[X]|Y=y)P(Y=y)dy} - \frac{\int_{-\infty}^{\infty}  y P(\hat{P}[X]|Y=y)P(Y=y)dy}{\int_{-\infty}^{\infty}  P(\hat{P}[X]|Y=y)P(Y=y)dy} \Bigr| \Biggr)
	\end{split}
\end{align*}
Therefore, given the true label  $P(x,y)$,
\begin{equation}
	\begin{split}
& \int_{x,y}  \Bigr|  \hat{P} (y|x) - \mathbb{E}  \Bigl[ Y | \hat{P}[x] \Bigr] \Bigl| P(x,y) dx dy \\
& =  \int_{x} \Biggr[  \int_{y}  \Bigr|  \hat{P} (y|x) - \mathbb{E}  \Bigl[ Y | \hat{P}[x] \Bigr] \Bigl| P(x,y) dx \Biggr] dy \\
&=  \int_{x} \Biggr[  \int_{y}  \Bigr|  \hat{P} (y|x) - \mathbb{E}  \Bigl[ Y | \hat{P}[x] \Bigr] \Bigl| P(y | x	) dy \Biggr] P(x)dx \\
&=  \int_{x} \mathbb{E}_{y|x}\Biggr[ \Bigr|  \hat{P}[y|x] - \mathbb{E}\Bigr[ Y | \hat{P}[x]\Bigl] \Bigl| \Biggl] P[x] dx \\
&=  \int_{x,y}  \Bigr|  \hat{P}[y|x] - \mathbb{E}\Bigr[ Y | \hat{P}[x]\Bigl] \Bigl| P[x,y] dx dy 
\end{split}
\end{equation}
This is the standard calibration metric, that is the \emph{calibration error} (CE), the difference between the	prediction $\hat{P}[y|x]$ and the conditional mean of $Y$ \st{given the prediction (Guo et al., 2017)},
\begin{equation}
	\begin{split}
    	& \int_{x,y}  \Bigr|  \hat{P}[y|x] - \mathbb{E}\Bigr[ Y | \hat{P}[x]\Bigl] \Bigl| P[x,y] dx dy = \\
		& 	\mathbb{E}_{(X,Y)\sim P } \Biggl( \Bigl|  \hat{P}[X] - \mathbb{E}\Bigr[ Y | \hat{P}[X]\Bigl] \Bigr| \Biggr)
	\end{split}
\end{equation}
we must therefore obtain $\hat{P}$ during calibration and search \st{rather} for mechanisms of measuring the \emph{calibration error}. Practically, this means models can be either over-confident or under-confident in their predictions, necessitating model calibration - although it is impossible to achieve perfect calibration.  A model is \emph{over-confident} when it's confidence is higher than the  accuracy. For example, our model predicts $\hat{P}(x) = 0.5$, but the conditional mean is $ \mathbb{E}\Bigr[ Y | \hat{P}[X] = 0.5 \Bigl] = 0.55 $ therefore  $\hat{P}(x)$ is positive at level $p = 0.5$.  Therefore our model is over-confident given the calibration error at level $0.5$ is $\Delta_{0.5}^{cal} \hat{P} = 0.05 > 0$. Had our conditional mean been $ \mathbb{E}\Bigr[ Y | \hat{P}[X] = 0.5 \Bigl] = 0.45$  therefore  $\hat{P}(x)$ is negative at level $p = 0.5$;  our model would be under-confident given the calibration error at level $0.5$ is $\Delta_{0.5}^{cal} \hat{P} = 0.05 < 0$.


Making calibration \st{analysis} of pattern recognition \st{machine learning} algorithms even more challenging, models require an ever-increasing number of parameters as \st{simulation} forecasting needs become more precise.  For example, deep learning \st{is an effective algorithm which will} essentially \st{ breaks down} divides a neural network into single layers, with two or more hidden layers, which must all be parametrized; although \st{it may contain} in principle this single layer may then contain an infinite number of hidden layers each with their own parameters. Model calibration is therefore usually a process of trial and error\st{, where the most appropriate values for coefficients must be found} - functional forms must be used in model calibration over a large number of outputs, often involving a fair degree of extra tedium due to non-linear dependencies on \st{a} large numbers of parameters and in some cases dealing with models which are inherently numerically unstable. These concerns are too often dealt with by clipping models and  their datasets. Unfortunately, this  leads to subjectivity and systematic error \st{in the results;} due to the choices in model's range, the datasets and distributions of parameters.
Although, the impact of model clipping and data reductions  can be estimated through techniques such as sensitivity analysis, these approaches are often imprecise. Lastly, the model calibration approach routinely discounts the impact of \emph{model uncertainty}; i.e. it ignores the possibility that our model may not be the most appropriate model. 




\subsubsection*{For Example, Model Averaging.}
%\st{In addition,  to  an a priori probability for every good model, an a priori probability %has to must also be specified for the distribution of each model’s parameters}.  

One technique termed  \emph{Bayesian model averaging} or model averaging, avoids the selection of \st{one} a specific model altogether and instead uses a probability weighted average of the posterior distributions over all models in a particular class of models, weighting \st{each model with} by their marginal posterior probability. This aggregation approach considers  $Y = y_1, y_2, … y_n$ in the observed data $\mathcal{D}$, the state space $M  = M_0, M_1, M_2, ..M_k$; then the posterior distribution of the combined result, $\hat{Y}$  is $P(y | Y)$ and the mathematical formulation is shown in equation (\ref{eqn_13}). Each model is weighed by it’s parameter probability distribution $P(y | M_k, Y)$, and model posterior probability distribution, $P(M_k | Y )$ as exhibited in equation (\ref{eqn_14}). The model posterior probability distribution $P(M_k | Y )$ depends on the marginal likelihood of each model, $P(Y |M_k)$ which is illustrated in equation (\ref{eqn_15}).
\begin{equation}
P(y | Y) = \sum_{k=1}^K P(y | M_k, Y) P(M_k|Y)
\label{eqn_13}
\end{equation}
\begin{equation}
P(M_k | Y) = \dfrac{P(Y | M_k) P(M_k)}{ \sum_{k=1}^K P(Y | M_k) P(M_k)}
\label{eqn_14}
\end{equation}
\begin{equation}
P(Y |M_k) = \int P(Y|\theta_k , M_k)  P(\theta_k | M_k) d \theta_k
\label{eqn_15}
\end{equation}
therefore, the posterior distribution for a parameter Bayesian model averaging is simply the weighted mixture of the probabilities under each model. In the context of heterogeneous parallel computing, model averaging aids by training multiple models on the same dataset using different parallel computing resources or algorithms, and then combines their predictions to obtain a final prediction. Furthermore, model averaging can help address issues with model calibration by reducing the impact of any individual model's weaknesses. For example, a model trained on a smaller subset of the data may be overconfident in its predictions, but this effect can be mitigated by combining its predictions with those of other models that are less overconfident. Model averaging is often preferred over other techniques such as data parallelism as  it allows the leveraging of different parallel computing resources or algorithms without having to split the data. For small or medium-sized datasets where data parallelism may lead to over-fitting or poor model generalization, model averaging tends to be preferred. \st{Model averaging is a technique that can be used to improve model calibration by combining the predictions of multiple models. }

%Model averaging is tricky for two reasons. First, we must assign prior distributions for %each model in the model space we are examining. An obvious choice of prior may be one %that assigns equal prior weight to all models, but this approach may ultimately favor %models that are very similar to one another, giving them high posterior weight. Second, %model averaging is computationally intensive. For example, in a normal linear regression %model with J covariates there are 2J possible models that must be examined. To date, %two primary approaches to dealing with this problem have been advanced. One is to %exclude models if they have low posterior probability relative to another well-fitting %model using a decision rule called Occam's window. Under this approach, we can also %exclude models that are supersets of better-fitting smaller models. The second approach %is to sample from the posterior for all models using MCMC methods. The benefit of the %first method is that it typically reduces the model space to fewer than 100 models, %making Eq. (24) easier to compute; the downside is that it only provides an %approximation to the true posterior probability for a parameter because numerous %models are excluded. The benefit of the MCMC method is that it provides the correct %posterior probability; the downside is that it is difficult to obtain convergence and that it %is incredibly time intensive.

In general, although the model weighting process in model averaging \st{are} considers complicated integrals,  there are approximate formulae which may be used \st{when there are} with sufficiently large sample sizes; combining statistical results for an arbitrary function across a state space ${M}$ of $N_M$ models, relies on the estimation of \st{the} model weight $P[M|\mathcal{D}]$:
\begin{equation}
	\inn{f(a_c)} = \sum_M 	\inn{f(a_c)}_M P[M|\mathcal{D}]
\end{equation}
where given a base model $M_0$  we denote $\{a_c\}$ as the set of one or more common parameters used to describe a dataset $\mathcal{D}$. Note that therefore  all models in $\{M\}$ will contain $M_0$, in the sense that marginalizing over additional parameters $\{a_m\}$ reduces them to $M_0$; the set of $a_m$ will therefore depend on our choice of model $M$.  The set of all parameters for $\{a\}$ will therefore be the union of $\{a_c\}$ and $\{a_m\}$, where this formula assumes the parameters of $a$ are dimensionless. Should we therefore wish to estimate the parameters $a_0$ over the set $\{M\}$ with $N_M$ models, we determine the mean $\inn{a_0}$ \cite{kass1995bayes}:
\begin{equation}
	\inn{a_0} = \sum_M  \inn{a_0}  P[M|D]
\end{equation}
therefore a variance:
\begin{equation}
\begin{split}
\sigma_{a_0}^2 & = 	\inn{a_0^2} - 	\inn{a_0}^2  \\
& =  \sum_{i=1}^{N_M}  \inn{a_0^2}_i  P[M_i|D] - \Biggl( \sum_{i=1}^{N_M}  \inn{a_0}_i  P[M_i|D] \Biggr)^2 \\
& =  \sum_{i=1}^{N_M} \sigma_{a_0,i}^2 P[M_i|D] +  \sum_{i=1}^{N_M}  \inn{a_0}_i^2  P[M_i|D] - \Biggl( \sum_{i=1}^{N_M}  \inn{a_0}_i  P[M_i|D] \Biggr)^2 \\
\end{split}
\end{equation}
such that when \st{the model weights are} equal we obtain a variance over the space of models $\{M\}$,
\begin{equation}
	\begin{split}
	\sigma_{a_0}^2 =  \sum_{i=1}^{N_M}  \inn{a_0^2} w - \Biggl( \sum_{i=1}^{N_M} \inn{a_0^2} \Biggr)^2 w^2
	\end{split}
\label{general_form}
\end{equation}
where the weight is given as $P[M_i|D]  {1/N_M} = w$. For this our general case, the variance computed is not equivalent to the variance of the set of weighted estimates, i.e. $w_i \equiv \inn{a_0}_i P[M_i | D]$ as such a weighted variance would contain an extraneous factor of $P[Mi|D]$ in the $\inn{a_0}$ term. We also note that taking the full width of the distribution of results $\inn{a_0}_i$ results in an estimated systematic error strictly greater than $\sigma_{a_0,syst}$. Now consider the special case of just two models $M_1$ and $M_2$ with the corresponding weights,
\begin{equation}
	\begin{split}
	P[M_1|D]  & = 1 - w \\
	P[M_2|D]  & =  w \\
	\end{split}
\label{two_models}
\end{equation}
From equation (\ref{two_models}), we consider a parameter estimation over $\mathcal{D}$ that strongly suggests a preference for $M_2$, such that $w$ is large:
\begin{equation}
	\begin{split}
		\inn{a_0} & = \inn{a_0}_2 + ( \inn{a_0}_1 - \inn{a_0}_2) (1- w), \\
		\sigma^2_{a_0} & \approx \sigma^2_{a_0,2} +[ (\sigma^2_{a_0,1} - \sigma^2_{a_0,2})   + ( \inn{a_0}_1 - \inn{a_0}_2)^2 ](1- w),
	\end{split}
\end{equation}
Therefore with large but non-zero $w$, mean and variance corrections to $a_0$ from $M_1$ due to including $M_2$, however limited, must be due to the differences between $M_2$ and $M_1$. In the limit of $w \rightarrow 1$, the statistical results will expectedly be just that of $M_2$. 
\vspace{0.25in}

%%% reference: https://www.sciencedirect.com/topics/mathematics/bayesian-model-averaging

\subsection*{Step 2, Heterogeneous Parallel Computing Helps Discover Theories.}
	%%Lesson 2: Heterogeneous Parallel Computing Helps Discover Theories 
	
Programming heterogeneous parallel computers requires code to run on  different architectures often concurrently; i.e., a host system with one or more multicore CPUs and one or more many-core GPU devices. The device require a distinctly different design methodology from that of the host system,  understand these differences is essential if the heterogeneous parallel computer is to run effectively. However modern flexible multicore CPUs and high-throughput many-core GPUs may be combined  to achieve accelerated computational flows. 

Consider the following approach to develop efficient pattern recognition algorithms in heterogeneous processing units (i.e. general purpose GPUs and general purpose CPUs). While GPUs provide a mechanism to achieve parallel arithmetic throughput, general purpose CPU architectures provide the flexibility to orchestrate computational flow whilst also providing multicore parallelism. First, you apply a \st{your} \st{heterogeneous} parallel computing toolkit to compute one optimization per cluster node of our complex phenomenon. Then compute one final optimization across \st{all} the sets of cluster nodes; noting for instance that optimization for intra-cluster and inter-cluster weights are sources of instability that when assigning the problem to a heterogeneous processing unit \st{parallel computing platform} must \st{incorporate} be accounted for in order to reduce estimation errors. The heterogeneous parallel computing tools have generalized  sources of instability; however, they do not directly inform you about the main source that propagates the instability. Second, we formulate a model that connects our optimizations to  \st{these sources through} a \emph{theory}. This \emph{theory}, is essentially a system of equations that hypothesizes a particular cause-effect mechanism. Third, now our model has a wide range of testable implications that go beyond the observations predicted by the heterogeneous parallel computing tools in the first step. A successful model will predict events out-of-sample. Moreover, it will explain not only predictive performance ($x$ is an independent variable of a target $y$) but also poor predictive performance (failed tracking of a target y is due to significant noise). 
	
In the above discovery process, heterogeneous parallel computing allows us to decouple model optimization across different architecture programming models and  in \st{the} solving of the actual problems. Pattern recognition and deep learning are often criticized for being  ?black boxes, even to their creators? (castelvecchi2016can, bathaee2017artificial), ?having a built in bias? in their assumptions and having inherent "implementation bottlenecks" (bathaee2017artificial). Considering the complexity of efficient pattern recognition and deep learning applications, it is unlikely that a researcher will be able to uncover the sources of bottlenecks of a programming model by a \underline{simple} visual inspection of the data structure  or by running \st{a more simulation} more simulations. Classical parallel programming does not allow this complete decoupling of multiple architectural programming models from  solving the actual problems. Although, it is true that a model may not have been validated without the help of parallel computing techniques, but once the model was validated, the heterogeneous parallel computing approach  played no role in the decision to use the model in a given application. Therefore, the most effective platform for pattern recognition is  heterogeneous parallel computing. On this basis, we must focus our efforts initially on developing a model and not the heterogeneous parallel computing approach in order to improve our  predictions; the model not the approximation error  serves as \st{the source of our problems } our design constraint. The approximation errors with heterogeneous parallel computing can be assumed typically to be low, and \st{it was may} are not be based on some untestable theory. These, massively multiprocessing graphics units with general-purpose programming capabilities present the most likely architectures long-term for low-cost high-performance processing. 

However, the biggest challenge to an \emph{efficient} implementation of pattern recognition and deep learning algorithms with \st{to} heterogeneous parallel \st{computing} architectures and execution models remains, 1) the overhaul of existing application design methodologies to thereby achieve  efficient implementation and 2) programming a heterogeneous parallel computing unit to allow the flexible multicore CPU to communicate with high-throughput many-core GPU . That is, you may apply a single-thread \st{serial} pattern recognition algorithm successfully with  heterogeneous parallel computing to improve your predictions, although it may not {necessarily} computationally be the most efficient implementation  of the algorithm particularly when your dealing with very large datasets. 




\newpage
\begin{center}
	\section{How Scientists Use Heterogeneous Parallel Computing.}
\end{center}
Heterogeneous parallel computing  focuses on the use of collaborative heterogeneous processing units and high-speed interconnections to achieve execution acceleration at different stages of a computing operation. This often includes using massively large amounts of lightweight threads \st{this is,} in what is commonly referred to as \emph{throughput-oriented design}. The use of heterogeneous architectures allows \st{the system} us to exploit the inherent capabilities of the wide range of computing cores, to solve a variety of problems. These problems are divided-up and assigned in parallel to separate processing units to handle specific aspects of each problem.  

Although developing \st{of the underlying  parallelized programs}  models for heterogeneous parallel computers does  not necessarily require low-level architecture or parallel programming expertise, \st{overly specified the researcher to have parallel computing  expertise, this has led many to} it has been erroneously concluded that heterogeneous parallel computing \st{units will} can achieve high-throughput across all tasks. In that view, heterogeneous parallel computing  then serves as merely a 'moniker' for a computing platform \st{from which} where no particular expert knowledge is required. This universally achievable gains in throughput through use of heterogeneous parallel computing is a entirely misconception.  For applications that require only a few threads, in fact execution of a program on modern multicore microprocessors will have typically have much lower latency than many-thread computing, achieving much higher throughput. This notion of a panacea has been fuelled by the many {varied} and popular industrial applications of parallel computing, where this search for low-latency high-throughput has generally outweighed the added cost and complexity required to develop new \st{understand the} models. 

A review of recent scientific breakthroughs, reveals radically different uses of heterogeneous parallel computing in the sciences, including the following:

\begin{itemize}
	%\item	\emph{Optimization}
	
\item \emph{Testing conjectures and developing proofs}: HPC has been used to rapidly evaluate the plausibility of new theories across scientific fields, particularly in mathematics, through the use of large-scale simulations. Many such applications have significant time and computationally demands, with problems that are complex and irregular behaviour. Through simulation, HPC allows scientists  to gain greater insights into the dynamics of underlying models; 1) scientists may use HPC to discover underlying patterns and on this basis generate a series of conjectures then subsequently test their validity before arriving at proofs, 2) in mathematics, HPC may also be used to provide verify complex proofs through the  application of parallel algorithms to support computer-assisted proof systems or check large-scale proofs generated through automated proofing. 
	
	%%1. "Parallel Methods in Computational Mathematics and Optimization" by Yuri Nesterov and Arkadi Nemirovski, published in Mathematical Programming in 2005, provides an overview of the use of parallel computing in optimization and mathematical programming.
	
	%% "Parallel Computing and Theorem Proving" by Larry Wos and Ross Overbeek, published in the Journal of Automated Reasoning in 1993, describes the use of parallel computing in automated theorem proving, which is a method for generating proofs using computer algorithms.
	
\item \emph{Pattern and trend discovery}: HPC is used to discover patterns in big data by disassembling the data into blocks and then processing blocks in parallel on different computing units. The use of heterogeneous parallel computing which combines the flexibility of CPUs with high-throughput general purpose GPU architecture, \st{will allow} allowed patterns to be discovered with greater rapidity then standard computing architectures (reference). Heterogeneous parallel computing is utilized to search for linear and non-linear patterns in data using the following these steps: 1) preparation of data for processing in parallel computing; the data is converted into a format that is readily partitioned and distributable across a number of computing units. 2) data is  partitioned into smaller blocks of data using partitioning strategies; blocks of data are then ready for the various computing units.  3) blocks of data are then multitask processed in parallel using a variety of heterogeneous computing units each with different execution models across CPUs, GPUs and FPGAs. 4) results of the parallel processing are then combined and subsequently analysed for explanatory power using statistical techniques to identify patterns. 5) further iterations allow any conclusions on patterns in the data to be further strengthened and then also allow for data predictions. Parallel programming tools  are essential for handling  massive amounts of, high-dimensional, complex data sets. With this huge quantity of data, much of the computation can be done on different parts fof the data in parallel, although they have to be reconciled at some point. For example execution of parallel code requires managing parallel threads and resources.  

	
	%% "Parallel Machine Learning for Big Data: A Review" by Zhengdong Lu et al., published in the IEEE Transactions on Neural Networks and Learning Systems in 2017, provides a review of the use of parallel computing, including heterogeneous parallel computing, for machine learning on big data.
	
	%% "Heterogeneous Parallel Computing for Big Data: A Review" by Wei Tan et al., published in the IEEE Transactions on Parallel and Distributed Systems in 2016, provides a comprehensive review of the current state of heterogeneous parallel computing for big data analytics.
	
   %% "GPU Acceleration of Machine Learning Algorithms for Large Data" by Wenbin Fang et al., published in the International Journal of Parallel Programming in 2015, describes the use of heterogeneous parallel computing with GPUs to accelerate the processing of large amounts of data in machine learning algorithms.
   
   \item \emph{Learning algorithms}: Heterogeneous parallel computing  \st{can} has been used determine the relative informational content of pattern recognition features and train application logic obtained from data sets much faster and with improved accuracy than \st{sequential processing} single-threaded or homogenous computing methods.
   
   \item \emph{Visualization}: Heterogeneous parallel computing has better enabled the visualisation of large complex scientific data, such as three-dimensional models of the evolution of galaxies or for example the fast simulations of the behaviour of human enzymes. By using parallel processing, scientists can generate and interact with the data generating various visualizations in real-time and analyse the data in a meaningful way.

	\item  \emph{Importance}: Using heterogeneous parallel units  and pattern recognition algorithms, researchers have determined the relative informational content of features for explanatory or predictive purposes in  deoxyribonucleic acid  (DNA) sequence data (cowell2011probabilistic,cowell2007gamma). Heterogeneous parallel computing returns the entire sequence of a locus and can combine many more loci in multiplexes; this \st{is} provided considerably more information  compared to \st{the old} previous classic capillary gel methods. Models based on heterogeneous parallel computing achieved  much higher discriminating power. For example, the  quantitative \emph{EuroForMix} and \emph{DNAxs/DNAStatistX}  models \st{which} both \st{used} employed maximum likelihood estimation (MLE) using a $\gamma$ distribution for modelling probabilistic genotyping following these steps:
	\begin{enumerate} 
		\item  With a database of $N$ individuals, each is considered as a  reference profile, there genotypes are compared to the crime scene swab $O$.; \\
		\item Consequently, a likelihood ratio can be generated for every individual in the database, where the alternatives propositions are:  (a) $H1$: Candidate n is a contributor to the evidence profile $O$, (b) $H2$: An unknown person is a contributor to the evidence profile $O$;  \\
		\item All contributors to the profile that are not  considered as the reference profile, are then designated as unknown and deemed unrelated to the reference profile, \\
		\item  Compute a \emph{likelihood ratio} (LR) to sum the weight of relevant genotype contributors. Consequently, for a well-represented DNA profile, the majority of candidates will return a low LR<1, which means that they will be eliminated from the investigation; one or more may return LR>1, and they are forwarded to the prosecuting authorities for further investigation; note it is assumed that autosomal markers are independent and in Hardy?Weinberg equilibrium.  Thus, although a quantitative model for probabilistic genotypics \st{does not uncover the underlying genotypes} fails to isolate the underlying genotypes, it does discover multiple contributors that are then considered \st{should be considered} for further investigation. These subsequent {quantitative} models [12?18] are the most complete DNA profiling models, as they take into account \st{all} the peak height information of genotype contributors in order to assign numerical values to the weights. The DNAxs/DNAStatistX model in particular supports parallel computing, allowing operations to be delegated to a heterogeneous parallel computing cluster, by parallelizing over independent function optimizations. 
	\end{enumerate}

	\item \emph{Retriever}: Heterogeneous parallel computing  \st{is} has been used in banking datacentres to search for areas of financial risk at a level of accuracy and speed conventional processors \st{can} could not achieve. For instance, every night heterogeneous parallel computing clusters, fed millions of trades and derivatives,  \st{in} search for risky positions. Once found \st{they find one position} in a portfolio with a high probability of containing concealed risks, cross-asset risk models  again powered by heterogeneous clusters, \st{can be} are pointed to particular positions in the portfolio, where traders  and risk managers \st{will use tools to} then scrutinize the positions. A second example are debt tranches and calculating default risk is a prediction problem rather than a qualitative problem. A powerful heterogeneous parallel computing cluster \st{can} detects an anomalous observation in a tranche based on an analysis of complex relationships \st{it has} identified in the data, even \st{if that} where such relationship \st{is are} would not be readily explainable discernible to the naked eye \st{us} (Hodge and Austin 2004). Rather than replacing an entire model, heterogeneous parallel computing plays a critical role helping researchers develop models based on improving the accuracy and highlighting relationships between variables based on strong empirical evidence. Furthermore, heterogeneous parallel computing \st{provides} provided an opportunity for researchers to apply model-driven techniques to develop high-tailored models for specific scenarios, rapidly prototyping and assessing a range of competing models.  Rather than replacing theories, \st{ML} pattern recognition powered by heterogeneous parallel  units play a critical role in helping scientists form theories based off of rich empirical evidence. Likewise, \st{ML} pattern recognition has opened up opportunities for economists to apply powerful data science tools towards the development of sound theories.
\end{itemize}



	
\newpage
%%\subsection{\st{Parallel} Heterogeneous Computing Has Limitations}
\subsection{Heterogeneous Computing Has Limitations}
The main goal of \st{parallelization} heterogeneous computing \st{has been} is to achieve throughput gains; i.e. an investigation of mechanisms by which \st{system} architecture enhancements may be introduced to achieve this goal, through a collaboration of multicore CPUs, high-throughput many-core GPUs and increasing the number of processing cores. The apparent drawback of heterogeneous \st{parallel} computing flexibility is that in inexperienced hands, these execution models {and algorithms despite the considerable resources on offer}, even \st{after} with parallelization can \st{easily} yield no accuracy gains or speed-up gains. Therefore \st{the} one primary limitation in parallelizing an application particularly for heterogeneous parallel computing, is the divergence between the simplicity \st{complexity} of sequential programs and the added \st{programming} complexity of parallelizing a program, this is (known as the \emph{parallel complexity theory}). Note that computation flow speed-up \st{is not only achieved through the introduction of improvement techniques,} need not necessarily be limited to just a consideration of \st{improving} how to increase the number of cores; for example, Amdahl’s Law may be applied in this context \st{is used} to model the limit achievable [with a single core], provided a portion of a \emph{job} is designated as potentially being parallelizable. 
		
%%	\newpage
\subsubsection{Strong Scaling, Amdahl's Law}
	%% \st{Train Set Overfitting} 
In general, let us consider the time taken to complete a computation, the {job} as being denoted by $T_J$;
\begin{equation}
T_J = T_P + T_S
\label{eqnTJTPTS}
\end{equation}
where $T_P$ for a single core is the proportion of time execution occurs in the parallelized region of the job and $T_S$ is the proportion of time execution occurs in the sequential region of a job again for a single core. Wherein the throughput gain achievable whilst running this job, depends entirely on the degree to which it can be parallelized. Let us assume the time taken to run a program on one processor is given as $T_1$ and the time taken to run a program on $N$ processors of equal capability to the one processor, is defined as $T_N$ and therefore the maximal throughput gain whilst running a job with $N$ cores  will occur  where $T_J = T_P$. Thus any job that is parallelized to a limited degree, will only achieve very limited throughput gains, i.e. $T_J \approx T_P$. \st{Let us assume that the time taken to run a program on one processor is given as $T_1$}. \st{Given} However consider where the  parallelizable work may be distributed perfectly onto $N$ processors of equal capability, we may denote the time taken run a job \st{program} on $N$ processors will be $T_N$.  With $N$ available cores we may therefore express (\ref{eqnTJTPTS}), the time taken to run the entire workload is given by,
\begin{equation}
	T_J = \dfrac{\alpha T_1}{ N} + (1 - \alpha) . T_1 
\end{equation}
where the component $(\alpha T_1 )/ N$, represents the time it takes to execute the parallel part of the job \st{program} across $N$ processors and  the second component, $(1 - \alpha). T_1$, is the time  taken to execute the serial (non-parallelizable) part of the job \st{program} on a single processor.  Thus, for  $N$ multiprocessors with a parallelization overhead of $O_N$ the total throughput gain $(	T_1 / T_N)$, is expressed as,
\begin{equation}
	\begin{split}
 S_N	& \leq 	\dfrac{T_1}{T_N} \\
	& = \dfrac{T_1}{(1 - \alpha) . T_1 + \dfrac{\alpha T_1}{ N}}- O_N \\
		& = \dfrac{1}{(1 - \alpha) + \dfrac{\alpha}{ N}} - O_N \\
		& = \dfrac{1}{\dfrac{N(1 - \alpha)}{N} + \dfrac{\alpha}{ N}} - O_N \\
		& = \dfrac{1}{\dfrac{N(1 - \alpha) + \alpha}{ N}} - O_N \\
	 & = \dfrac{N}{{N(1 - \alpha) + \alpha}}
	\end{split}
\label{Amdahl2}
\end{equation}
Note, this problem  confounding parallel execution with the sequential portion of a fixed-sized \st{application} job  is termed \emph{Amdah's Law}; i.e. the larger the parallelizable fraction $\alpha$ of a job  of fixed size  (or "workload" or  "program"), the larger the potential acceleration-gains \st{speed-up} \st{of} achievable with $N$ processors. Parallel execution limitations \st{that} are due to a limit in the parallelizable portion of an application result from the accumulated lag in execution time of operations that can not be parallelized or are not worth parallelizing.  In fact, severe limitations in throughput \st{will be} are caused where there are large sequential fractions in a job \st{of the program} or there is limited use of the fractions of the  \st{program} job that have been parallelized. \st{that have  parallel segments of the program.}  

We note that \st{one} another standard Amdahl-type interpretation, \st{uncouples} decouples the Law from multi-cores and parallelization entirely. It rather frames throughput-gains on the basis of a general architecture-based acceleration. Under this framework, conditioned on an architecture-based acceleration (what does this mean?), a job may be divided into an infinitely enhanced part  and a non-enhanced part. \st{Therefore} The parameter $N$ in (\ref{Amdahl2}) may then describe a function (or a number), representing the degree of improvement due to \st{an} architecture-based acceleration. In this framework, \st{more generally} where a fraction $(1 - \alpha)$ of our \st{serial program} job remains non-accelerated, then Amdahl's Law states we are unable to obtain a speed-up of better than $1/(1 - \alpha)$. For example, $75\%$ of a \st{serial program} job has been \st{parallelized} accelerated therefore the serial portion of a job is $(1 - 0.75) = 0.25$; therefore, we can not obtain a speed-up of better than $4$ times. That is if a fraction \st{of our serial program is ?inherently serial,? that is, it} cannot possibly be accelerated \st{parallelized}, then we can't realistically obtain \st{possibly get} a speed-up better than $1/(1 - \alpha)$; i.e. there will always exists a \emph{sequential bottleneck}. Thus even if $(1 - \alpha)$ were quite small, say $1\%$,  even then any achievable acceleration can not possibly exceed 100; unless virtually all of \st{a serial} \st{the} a single-threaded program job may be infinitely accelerated \st{speed-up}, the wholesale acceleration of a job \st{speed-up} is going to be very limited. In summary, notice that with Amdahl's Law, 
\st{the} acceleration gains do not scale with $N$ \st{the number of processors} but rather saturate, even when $N \rightarrow \infty$. Unsurprisingly, Amdahl's contention at the time was that there would be no advantages in employing any machine that employed massively parallel-processing. Anecdotally, it is noted that Amdahl never developed a formal proof for his principle; it was later formulated as a Law based on his verbal descriptions.
%%we still couldn't possibly \st{get} achieve a speed-up better than 100; 
%%which in turn will lead to limits in achievable speed-up when parallelizing a given sequential program. 



% parallel computing researchers are keenly aware of this problem, which they address in three complementary %ways.  


%Through resampling techniques (such as cross-validation) and Monte Carlo methods. %Appendix A describes these techniques and methods in greater detail. The second %approach to reduce train set over-fitting is regularization methods, which prevent model %complexity unless it can be justified in terms of greater explanatory power. Model %parsimony can be enforced by limiting the number of parameters (e.g., LASSO) or %restricting the model?s structure (e.g., early stopping). The third approach to address %train set over-fitting is ensemble techniques, which reduce the variance of the error by %combining the forecasts of a collection of estimators. For example, we can control the %risk of over-fitting a random forest on a train set in at least three ways: (1) %cross-validating the forecasts; (2) limiting the depth of each tree; and (3) adding more %trees. In summary, a backtest may hint at the occurrence of train set over-fitting, which %can be remedied using the above approaches. Unfortunately, backtests are powerless %against the second type of over-fitting, as explained next. 




\subsubsection{Weak Scaling, Gustafson Model.}
[1988, John Gustafson, working with E. Barsis], modified  Amdahl's model  \st{is refined} by adjusting the underlying assumptions to more accurately reflect \st{the} observations in their experimental findings. Amdahl's Law had failed to account for the fact that, they had noticed that 1) as the available computing power $N$ increased the job size also tended increased and 2) the trend was that the non-parallelizable portion of jobs were also becoming a much smaller \st{as a} proportion of the overall job sizes. \st{Amdahl Law's limitations, according to the precept that t} In fact, one substantial \st{assumption} modification to Amdahl's Law, \st{is} was that the size of a job remains fixed irrespective of the number of processors; that is the proportion of job that is serial (non-parallelizable) \st{program} will always \st{inherently} limit the throughput gains achievable through parallelization, thus the gains from massively-parallel  processing \st{irrespective of the number of processors $N$} will always be capped \st{are limited.}  \st{This assumption, that irrespective of the number of processors $N$, practically seems unrealistically}. 

However, Gustafson argued that a machine with $N$ processors would continue to achieve scalable throughput gains; \st{as} he predicted job size could continue \st{tend} to scale linearly provided there was an \st{with the} availability of processing power. Gustafoson's model therefore proposed that \st{the assumption that} 1) overall, job sizes \st{could} might increase proportionally with the number of processor cores $N$ and 2) \st{whilst} the \st{size of the} serial portion of jobs \st{should} would remain constant as $N$ increases. Essentially, Gustafson had forecasted \st{concluded} that scalable machines with more processing power would be developed and \st{should be} then applied to solve larger problems. ((expand))To cite one practical contemporary example: as the availability of mainstream computational resources has improved, pattern recognition as applications have become far more sophisticated, both in terms of the user-interfaces and in terms of the underlying algorithms they employ.  

We consider the maximum throughput gain $J'$, \st{is then} the ratio between the proportion of a job that can be executed in the parallelizable region (i.e. a massively-parallel system)  and the proportion that can be executed sequentially (i.e. non-parallelizable region), provided both are given the same amount of time, that is,
\begin{equation}
 J' =  \alpha N J + (1 - \alpha) J 
\end{equation}
therefore the throughput gain given  $N$ multiprocessors (or cores) is expressed as,
\begin{equation}
	\begin{split}
		S_N	& \leq 	\dfrac{T_{N=1}}{T_{N>1}} \\
		S_N	& \leq  \dfrac{ \alpha N J + (1 - \alpha) J }{\dfrac{\alpha N J}{N} + \dfrac{(1 - \alpha) J}{1}} - O_N\\
	\end{split}
\end{equation}
this reduces to the expression,
\begin{equation}
	\begin{split}
		S_N	& \leq  { \alpha N  + (1 - \alpha)  } = \alpha (N -1) + 1
	\end{split}
	\label{Amdahl2}
\end{equation}

Note, that this expression does not consider thread management overheads, which tend to  increase significantly with the number of cores (or multiprocessors). Gustafson's model reconsiders the assumption in \st{takes into account the fact that} Amdahl's Law that the serial region is constant and therefore only a fixed \st{considers the} fraction of a system's workload \st{that} can be parallelized; with Gustafson's Law we inherently require that \st{but does not consider the increase in} the size of our parallelizable workload to increase \st{as} with the number of processors. However, even in this model \st{details  that the achievable} our throughput gains will eventually tail-off, because as the number of processors increases, the gains from parallelizing a job will increase but the presence of any \st{amount of} \st{non-parallelizable} serial regions will eventually bound these gains. Centrally, Gustafson's model \st{details} concludes throughput gains \st{is being} as being constrained by the number of cores not the proportion of the job that is parallelized.  In summary, Gustafson's model implies that massively-parallel processing can only be achieved when these processors are applied  to parallelizable yet large jobs. \st{a large parallel job is considered}. 

\begin{table}
	\begin{tabular}{|c|l|c|l|l|c|l|}
		\hline
		\# Cores & Computation  & Amdahl  &  Eff. &Computation & Gustafson  & Eff.   \\
			 &   &  acceleration &    (acc./cores)   &  &  acceleration& (acc./cores)  \\
		\hline
		2&2/(2 $\times$ 0.9 + .1) & 1.05$\times$ & 0.525 & 0.9 $\times$2 + .1 &  1.9$\times$& 0.950\\
		\hline
4&4/(4 $\times$ 0.9 + .1)& 1.08$\times$ & 0.270 & 0.9 $\times$4 + .1 &  3.7$\times$&0.925\\
		\hline
	8&8/(8 $\times$ 0.9 + .1) & 1.10$\times$ & 0.138 & 0.9 $\times$8 + .1 &  7.3$\times$&0.913\\
		\hline
		256&256/(256 $\times$ 0.9 + .1) &  1.11 $\times$ &0.004 & 0.9 $\times$256 + .1 &  230.5$\times$&0.900\\
		\hline
	\end{tabular}
\caption{The table details Amdahl's Law and Gustafson's Model acceleration to varying systems with varying cores.}
\end{table}

\subsubsection{Three Limitations of Parallel Computing.}
Both Amdahl's law and Gustafson's model present  theoretical acceleration gains without considering any other  factors \st{that are} that limit scalability, such as $O_N$. In practice as the number of processors $N$ increase, so will the number of threads and the extent of \emph{overheads}. i.e. any impediments in the capability of a system to realize it's maximum model \st{theoretical} throughput. We  therefore must distinguish between two types of limitations: 1) a limit in the level of acceleration achievable due to the limit in the size of a parallelizable region $\alpha$ of our job, 2) any additional overheard required to achieve parallelization and \st{any} added latency in the execution time of a job even in the  absence of parallel computing (what): 
\begin{enumerate}
	\item Thread creation/destruction: for example, thread pooling, supporting the run-time redistribution of partially executed threads across cores through such techniques as task (thread) migration and creating outer loops for individual threads. 
	\item Synchronization and lock management: this includes overheads that result from synchronizing, joining and handshaking of multiple processes with data. In addition, the locking of critical sections of code to
	prevent data from being overwritten by other threads. Note that to reduce overheads, thread synchronisation is not guaranteed on a global scope. 
	\item Load imbalance: this may be caused by factors including inherent model issues, algorithm concerns and other  infrastructure issues such as cache misses, non-uniform memory access times or interrupts %%\cite{https://edoc.unibas.ch/59514/1/20180128130357_5a6dbc2d9750f.pdf, %%https://arxiv.org/pdf/1909.07168.pdf}. 
\end{enumerate}

%% IET-Amdahl-Review.pdf @ page 3
Although we may introduce \st{a distinction between the fully parallelizable region and non-parallelizable region, may allow account for the} a single degradation factor $\alpha$ into both models to account for the various {causes of} thread overheads, i.e. \st{caused by} non-processing activities such as inter-core communication calls and memory access, where the \st{inherent relationship} behaviour of these effects are often difficult to model and   \st{not is} typically non-linear. Therefore, the parameter $\alpha$ may be \st{wholly} insufficient to capture  more complex dynamics of threads overheads for example thread behaviour due to different parts of a job communicating with each other \st{various communication effects},  lags caused by memory accesses during  joining-handshaking protocols of parallel processes  and any overheads inherent within shared memory architectures where  processors must wait on memory requests to continue computation. 

\subsubsection{Sun-Ni Model.}
Much later an extension to Amdahl's  and Gustafson's models [sun-li-paper],
to account for the overhead factors of memory, communications and other services [2, 57–59] through introduction $g(N)$ which is a function of $N$. The Sun-Ni model details throughput gains as  being bounded by constraints in memory, \cite{sun-ni} unlike in Gustafson's Model where throughput gains  were bounded by the number of processors $N$ and in Amdahl's Law were gains are bounded the size of the serial region. The Sun-Ni model details that even with an infinite number of cores, the largest size of a problems that may be considered is fundamentally constrained by the amount of available memory. This additional factor $g(N)$ allows for the introduction of a parameter that governs memory, communications and other service interactions [2, 57–59] \st{Therefore, as computing power increases, the corresponding increase of problem size will be constrained by a system's available memory size,},
\begin{equation}
	\begin{split}
	S_N	& \leq \dfrac{T_1}{T_N} \\
	S_N	&  \leq \dfrac{(1 - \alpha) + \alpha.g(N)}{(1 - \alpha)+ \dfrac{\alpha.g(N)}{N}}  
	\end{split}
\end{equation}
where $g(N)$ \st{represents  the memory bound, i.e.} models the relationship between total required memory and the number of cores $N$ \st{, the amount of required memory is assumed to depend on the number of cores $N$}. The function $g(N)$ can account for the \st{effects}  penalty of both thread management, memory constraints, communication overheads, synchronization and load imbalances [2, 35].  For example, consider the memory bound function problem in the context of  matrix multiplication. The memory requirement in an  two $N \times N$ matrices is proportional to $N^2$ , and the amount of computation is proportional to $N^3$, which gives $g(N) = N^{3/2}$, therefore the memory-bounded speed-up is
\begin{equation}
\label{sunni_model}
S_N \leq \dfrac{(1 - \alpha) + \alpha.N^{3/2}}{(1 - \alpha)+ \alpha.N^{1/2}}
\end{equation}
note that with $g(N) = 1$ this model reduces to Amdahl’s Law. In other words when is fixed $g(N)$ \st{indicates that} our workload does not increase with the computing power. Notable is that at $g(N) = N$, the required memory size is proportional to \st{the same as} the number of cores - i.e. Sun-Ni's model reduces to Gustafson’s model. 

%Given that a form of Amdahl’s Law has been derived in equation %(\ref{sunni_model}) that generalizes speedup modelling to cover wide %heterogeneity in workload, hardware and workload to hardware mapping, a %major challenge for researchers and engineers who want to reason about %speedup and performance bounds in the HeMCP era is to find the most %user-friendly reduced-scope models to target their specific needs. In this %context, the general model of equation (\ref{sunni_model})  may serve as %the foundation for deriving appropriate special-purpose models for practical %use. 
In equation (\ref{sunni_model}), a generalized form of Amdahl's Law has been articulated that extends speed-up modelling to encompass the broad diversity in workload characteristics, hardware configurations, and workload-to-hardware mappings. A paramount issue in the heterogeneous multicore (HeMCP) context, is the identification of streamlined, user-centric models that allow engineers and researchers to effectively assess speed-up potentials and set performance boundaries tailored to particular scenarios.

%While a generalized Amdahl's Law model (equation (\ref{sunni_model})) has %been derived to account for wide heterogeneity in workload, hardware, and %workload-to-hardware mapping, researchers and engineers still face a major %challenge in reasoning about speedup and performance bounds in the %heterogeneous multicore (HeMCP) era. finding the most user-friendly %reduced-scope models that target their specific needs.


\section{Heterogeneous Processing Units}
		
Your friend claims to have developed a strategy to win at blackjack in casino gambling. His strategy uses an \st{complex} innovative approach \st{model} \st{which increases the odds of a win}, but requires that he sit at one poker table for long hours to increase the odds in favour. A major caveat \st{: a consequence of his strategy}, is that if he moves  between blackjack tables his odds of winning \st{drop} vary markedly. Yet, you have another group of friends who also consider themselves to be seasoned gamblers, they have developed another strategy for beating the house in blackjack. Though, their approach does not necessitate one player sit in front of the same dealer for long hours \st{at one poker table}, but it does require a large number of players with varying skill levels to coordinate a betting strategy against the casino, to maximize earnings subsequently. This second coordinated approach  appears to generate much larger returns than the earlier single player strategy. Incredulous, you evaluate the legitimacy of both approaches, {you decide to} adjust the winnings for the fact that the second technique requires coordinating teams of players who on aggregate place much larger bets. After accounting for  bet size, to your surprise you find your friends single-player gambling strategy on \st{occasionally on some} most days proves to be the more profitable strategy. 

%(Gpt)Likewise, a utopian researchers running many-threads on the same models are more likely to make \st{a} false %conclusions. By using heterogeneous parallel computing  with an underlying model, without due consideration for %the architecture and input data structures, it is almost guaranteed that most researchers will arrive at a false %conclusion; in fact performance of a parallelized program may occasionally run slower than a sequential program %or have erratic performance. This performance fallacy comes from failing to account for the many non-intuitive %ways of approaching  a problem, that is required for parallel programming. 

To this end, it's crucial for those in our field to approach parallel computing with a critical lens. Naively, we could run numerous threads on identical models, the probability of reaching \st{flawed} the right conclusions decreases \st{increases}. When leveraging heterogeneous parallel computing on a base model, without proper consideration for the underlying architecture and the nuances of input data structures, is a pathway to misleading outcomes. It's imperative to understand that, occasionally, the performance of a parallelized program might lag behind its sequential counterpart or display unpredictable behaviour. Such deviations in performance can be attributed to a lack of understanding of the intricate and, quite often, non-obvious strategies essential for efficient parallel programming.
%%%% -----------------------------> good to here		
Another example of \st{severe  limitations in the throughput achievable} throughput limitations in heterogeneous parallel computing clusters, occurs when a researcher develops a model and fails to tweak a program for the differences between the  instruction sets of master-worker \emph{heterogeneous} system.  That rapid porting of a sequential program to a many-core/multi-core processor, can be a futile exercise that will inevitably end with severe limitations in throughput (poor portability). Instead, the researcher should have spent her time investigating how the research process misled her into achieving poor model performance. In other words, a poorly performing parallel program presents us an opportunity to fix the research process, not only an opportunity to fix a particular model. Most published advances in pattern recognition \st{are} may likely be intuitive and faulty, due to severe limitations in the actual advancements in throughput gain. Heterogeneous parallel computing did not cause the current crisis, of an endless  search for efficient pattern recognition algorithm yield. That crisis was caused by the widespread misuse in the application of many-threaded systems to pattern recognition, and poor porting in particular. However, going forward heterogeneous parallel computing can help deal with the problem of limited execution throughput gains in three ways:
		  
First, we can keep track of how many independent tests a researcher has run, to evaluate the probability that at least one of the outcomes is a false discovery (known as family wise error rate, or FWER). Second, while it is possible to have long-latency or throughput limitations for a few threads completing an operation , it is unlikely that there will be latency or throughput limitations across a majority of threads. An important observation, has been that it is more difficult to reduce latency due to arithmetic operations or memory access, than it is to improve throughput by increasing the chip area and power (reference orangebook). The design goals are often therefore to optimize throughput across a massive number of threads. This is the approach followed by designers, introducing sub-optimality by allowing for long-latency within individual threads by reducing the area of pipelined memory and the power of the arithmetic units; the reduced area of the memory and arithmetic units allows designers to achieve increased chip density thereby increasing execution throughput. 
		  
		  		  %The deflated Sharpe ratio (Bailey and L�pez de Prado 2014) follows a similar approach in the context of %backtesting, as explained in Section 8. It is the equivalent to controlling for the number of lottery tickets %that your friend bought. 
		  
Third, we can use historical series to estimate the underlying data-generating process, and sample synthetic data sets that match the statistical properties observed in history. Monte Carlo methods are particularly powerful at producing synthetic data sets that match the statistical properties of a historical series. The conclusions from these tests are conditional to the representativeness of the estimated data-generating process (AFML, chapter 13). The main advantage of this approach is that those conclusions are not connected to a particular (observed) realization of the data-generating process but to an entire distribution of random realizations. Following with our example, this is equivalent to replicating the lottery game and repeating it many times, so that we can rule luck out.

In summary, there are multiple practical solutions to the problem of train set and test set over-fitting. These solutions are neither infallible nor incompatible, and my advice is that you apply all of them. At the same time, I must insist that no backtest can replace a theory, for at least two reasons: (1) backtests cannot simulate black swans ? only theories have the breadth and depth needed to consider the never- before-seen occurrences; (2) backtests may insinuate that a strategy is profitable, but they do not tell us why. They are not a controlled experiment. Only a theory can state the cause?effect mechanism, and formulate a wide range of predictions and implications that can be independently tested for facts and counter-facts. Some of these implications may even be testable outside the realm of investing. For example, the VPIN theory predicted that market makers would suffer stop-outs under persistent order flow imbalance. Beyond testing whether order flow imbalance causes a reduction in liquidity, researchers can also test whether market makers suffered losses during the flash crash (hint: they did). This latter test can be conducted by reviewing financial statements, independently from the evidence contained in exchange records of prices and quotes.
		
\newpage
\section{Outline}
		
This book offers users an unhurried guide to building efficient pattern recognition applications with the help of heterogeneous parallel computing. To that objective, each chapter uses what we have developed in the previous ones. Each chapter (except for this introduction) contains \st{an} code examples where the methods explained are put to the test\st{ in code examples}.
		
The first step in building pattern recognition algorithms for heterogeneous parallel computing is \st{data} \emph{parallelism}, that is to detail how the problem might be broken down into smaller independent computations which can be run  \st{parallelised} across cooperating workers, thereby executing a program faster.  In heterogeneous parallel computing settings, programs are run on threads with distributed memory. We use a large number of threads to enhance the execution throughput by running typically computationally intensive portions of the program as parallel computations.  However, pattern recognition in heterogeneous parallel computers have a  high cost of communication, as processors are required to communicate by exchanging messages. Intuitively multiple processors will generate many-threads to collaborate on training one model, will reduce the overall training time, but the communication cost between processors generally limits the system scalability. A relatively small proportion of the time (i.e., computation-to-communication ratio) is spent in parallel computations when training a deep learning model on a heterogeneous systems with low-speed interconnect; i.e., the throughput across groups of processors could fall well short of a single processor. Section 2 explains how to develop communication-efficient parallel algorithms without giving up any  convergence guarantees. Most of the discussion centres on  the \emph{accelerator model}, but at the core of the solution sits a heterogeneous parallel computing technique: the graphical processing unit (GPU).
		
%https://arxiv.org/pdf/2003.06307.pdf
	
Many pattern recognition research questions consider the notion of similarity measure or distance function. For example, we may be interested in understanding how closely related two variables are. Different similarity measures are used for different \st{intensity distortions} attributes \st{between distributions vectors} within the data. Measures are broadly applied to single modality data and multi modality data \st{measures}.  Traditional \st{machine learning} pattern recognition algorithms have focused on single modality (e.g., audio  or text ). We denote a single-modality domain set as $\chi =  \chi^{(1)} $. Given the data $x$ consisting of a single modality, where $x \in \chi^{(1)}$, which is the domain set of the single modality.  Therefore a single-modality measures will consider only a single type of feature to calculate distance or similarity.  For example, Sum of absolute difference is a single-modal measure that is used for block comparison in image processing; it calculates the difference between two corresponding vectors in space. Single modality measures may be calculated by independent computations assessed at each spatial point and multimodal measures determine the mutual information (statistical) or correlation dependence (functional) of distributions, where the  distribution is assumed to be a realization of an underlying discrete random variable; the aim is to minimize distances between similar pairs whilst separating dissimilar pairs that fall with a certain margin.. When parallelizing an algorithm, single modality measures are readily adaptable to single instruction multiple data (SIMD) architectures and therefore instruction sets such as that of many-core GPUs.  The literature highlights GPUs as being primarily suited for single modality measures, i.e. arithmetic-intensive but simple computational flows (provide reference).
		
However the human body generates information from various data forms including audio, image and spectrograms.  More robust modelling is required than single-modality, with researchers developing algorithms that integrate the modalities of data including images, text, and spectrograms. The main idea in multi-modal machine learning is that different modalities provide complementary information on the human body (e.g., sound, identifying objects in an image, the frequency of words in text or sensory function).  By contrast a multi-modal measures will incorporate multiple  features to determine similarity measures or distance functions. Modern deep learning algorithms made integrating different signal sources significantly easier and therefore has lead to the increase use of multi-modal data.  For example, the Jaccard similarity coefficient as a multi-modal measure will assess similarity between two sets by considering the presence or absence of  values. Lets \st{We} denote a \st{the} multi-modality domain set as $\chi =  \chi^{(1)} \times ... \times \chi^{(K)} $. Given the data $x := [x^{(1)}, ... , x^{(K)}]$ consisting of $K$ modalities, where $x^{(k)}  \in \chi^{(k)}$ is in the domain set of a k-th modality. Multi-modality measures \st{however} may require the estimation and combining of joint and marginal probability mass functions for the underlying discrete random variables from data. Methods for computing probability mass functions  are parallelizable but with varying degrees of difficulty. Note single-modality measures may also be combined with multi-modality measures to generate more comprehensive analysis. For instance, the sum-of-squared-difference may be used for comparing numerical data and the Jacard similarity coefficient used to compare the accuracy of transmitted words. In summary, given single-modal and multi-modal measures, the ideal approach will be dependent on the feature or number of features that must be analysed. In summary, more realistic control-centric operations are often complex, requiring a lot of computational flow more suited to the flexibility of general purpose CPUs.
		
Chapter 3 provides an information-theoretic framework for estimation of complex probability mass functions from noisy data. In particular, this will allows us to define similarity and distance measures with minimal assumptions regarding the underlying variables that characterize a distribution. One of the applications of distance matrices is to study whether some variables are more closely related among themselves than to the rest, hence forming clusters. Clustering has a wide range of applications across finance, like in asset class taxonomy, portfolio construction, dimensionality reduction, or modelling networks of agents. A general problem in clustering is finding the optimal number of clusters. Chapter 4 introduces the ONC algorithm, which provides a general solution to this problem. Various use cases for this algorithm are presented throughout this book. Clustering is an unsupervised learning problem. Before we can delve into supervised learning problems, we need to assess ways of labelling  data. The effectiveness of a supervised ML algorithm greatly depends on the kind of problem we attempt to solve. For example, it may be harder to forecast tomorrows S\&P 500 return than the sign of its next 5\% move. Different features are appropriate for different types of labels. Researchers should consider carefully what labelling method they apply on their data. Section 5 discusses the merits of various alternatives.
		
		
		%%http://users.cecs.anu.edu.au/~ramtin/papers/2010/SPM_2010.pdf
	   \begin{landscape}
		\begin{table}
		\begin{tabular}{|c|c|c|c|}
				\hline 
			\underline{Formula} 								&  \underline{Description} & \underline{Type} & \underline{Modality}\\
			\hline
			$D_{MD} [I,J] = {\sum_{\chi \in \Omega}  \biggl|I[x] - J[x]\biggl|^p}$		 & Minkowski distance (MD) & Distance & Single-modality  \\
			\hline
			$D_{TMD} [I,J] = {\sum_{\chi \in \Omega} \text{abs} \biggl[I[x] - J[x]\biggl]}$		 & The Manhattan distance (TMD) & Distance & Single-modality \\
		\hline 
		$D_{ED} [I,J] = \sqrt{\sum_{\chi \in \Omega} [I[x] - J[x]]^2}$		 & Euclidean distance (ED) & Distance & Single-modality \\
		\hline
		$D_{SSD} [I,J] = \sum_{\chi \in \Omega} [I[x] - J[x]]^2$		 & Sum of squared difference (SSD) & Distance & Single-modality \\
		\hline
		$D_{SAD} [I,J] = \sum_{\chi \in \Omega} | I[x] - J[x] |$		 & Sum of absolute difference (SAD) & Distance & Single-modality \\
			\hline
		$S_{JSC} [I,J] = \dfrac{\sum_{\chi \in \Omega} I[x] J[x]}{  \sum_{\chi \in \Omega} I[x] + \sum_{\chi \in \Omega} J[x]  - \sum_{\chi \in \Omega} [ I[x] J[x] ]}$		 &  Jacard Similarity Coefficient (JSC) & Similarity & Single-modality  \\
		\hline
		$S_{CC} [I,J] = \dfrac{(I[x] -\mathbb{E}  I[x] ) (J[x] - \mathbb{E} J[x])}{{\sigma[I]} \sigma[J]}$		 &  Cross correlation (CC) & Similarity & Single-modality  \\
		\hline
		$S_{NCC} [I,J] = \dfrac{I[x] J[x]}{\sqrt{\mathbb{E} [I[x]^2] \mathbb{E} [J[x]^2]}}$		 &  Normalized  correlation (NCC) & Similarity & Single-modality \\
		\hline
		$S_{GC} [I,J] = \dfrac{1}{d} \sum_{i=1}^d S_{CC}\Bigl( \dfrac{\delta I}{\delta X_i}, \dfrac{\delta I}{\delta X_j} \Bigl)$		 & Gradient  correlation (GC) & Similarity & Single-modality \\
		\hline
		$S_{MI} [I,J] =  \sum_{i} \sum_{j} p_{I,J} [i,j] \log \dfrac{p_{I,J} [i,j] }{p_{I} [i]  p_{J} [j] }$		 & Mutual  information (MI) & Similarity & Multi-modality \\
		\hline
		$S_{NMI} [I,J] = \dfrac{2 S_{MI} [I,J] }{H[I] + H[J]}$		 & Normalized mutual  information (NMI) & Similarity & Multi-modality \\
		\hline
		$S_{CR} [I,J] = \dfrac{\sigma^2[\mathbb{E}[J|I]]}{\sigma^2[I]}$		 & Correlation ratio (CR) & Similarity & Multi-modality \\
		\hline
			$S_{JSD} [I,J] = \dfrac{\sigma^2[\mathbb{E}[J|I]]}{\sigma^2[I]}$		 & Jensen-Shannon (JS) & Similarity & Multi-modality \\
		\hline
		\end{tabular}
	   \end{table}
  \end{landscape}
	
		
\subsection{Audience}

Should you be like most users and  regularly estimate covariance, use correlation matrices, search for low-rank models, build neural networks or apply the same test multiple times on a given data set, regular apply feature engineering or deep learning then you are reading the right book. In it, you will consider the high-level abstraction that is common with CPUs compared to the low-level considerations of GPUs, the impact of computational flow requirements on the underlying hardware choice, learn that in order to take advantage of memory bandwidth limitations  and use of warps a different algorithm implementation is required to use GPUs  (Section 2). The book also covers information-theoretic metrics as an alternative to  correlations measure (Section 3). The book then discusses the static declaration as a common programming paradigm for handling pattern recognition with GPUs (https://odr.chalmers.se/server/api/core/bitstreams/35801255-a257-436e-b836-dbfb114c5b55/content). \emph{You will learn intuitive ways of reducing the dimensionality of a space, which do not involve a change of basis. Unlike PCA, ML-based dimensionality reduction methods provide intuitive results (Section 4). Rather than aiming for implausible fixed-horizon predictions, we will consider alternative ways of developing pattern recognition models that can be solved by heterogeneous parallel computing. You will learn modern alternatives to the classical p-values (Section 6). You will learn how to address the instability problem that plagues mean-variance investment port- folios (Section 7). And you will learn how to evaluate the probability that your discovery is false as a result of multiple testing (Section 8). If you work in the asset management industry or in academic finance, this Element is for you.}GPU


		\st{De-noised covariance matrices can be very useful for deriving distance metrics from linear relationships. Modelling non-linear relationships requires more advanced concepts. }
		
\section{General Purpose GPUs}
Graphical Processing Units (GPUs) were developed to allow for the  rendering of images on computer screens, which were  coded using a three-dimensional code. GPUs enabled the parallel calculation of many computationally-intensive operations such as vector operations, matrix multiplications and activations thereby render graphics rapidly. Similar to image processing, modern scientific and engineering applications including neural-network implementations  also require large matrix-matrix multiplications.  The architecture of massively parallel accelerators based on GPUs are organized into multiple \emph{Streaming Multiprocessors} (SM) which consist of multiple threads executing the same kernel or instruction; within a SM for each block of threads they share the same instructions each cycle and this allows synchronous execution; this is called a \emph{wavefront} or \emph{warp}.  A GPU was thought to be well-suited to parallelizing these operations by harnessing its different cores each with multithreading [9783319944623.pdf/203]; throughput improvements are achieved where groups of threads running the same code are  executed synchronously. A technique known as \emph{Single Instruction Multiple Threading} (SIMT), which achieves a level of parallelism much higher than achievable in CPUs. With warps, thread blocks consist of 32 or 64 threads depending on the underlying architecture. For example, consider an image processing kernel where we compute the matrix-matrix multiplication:
\begin{equation*}
	\underbrace{\begin{bmatrix}
		a_{11} & a_{12} & \cdots & a_{1n}\\
		a_{21} & a_{22} & \cdots & a_{2n}\\ 
		\vdots & \vdots & \ddots & \vdots\\ 
		a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{bmatrix}}_{200 \times 40}
	\times
	\underbrace{\begin{bmatrix}
		b_{11} & b_{12} & \cdots & b_{1p}\\
		b_{21} & b_{22} & \cdots & b_{2p}\\ 
		\vdots & \vdots & \ddots & \vdots\\ 
		b_{n1} & b_{n2} & \cdots & b_{np} 
	\end{bmatrix}}_{40 \times 600}
	=
	\underbrace{\begin{bmatrix}
		c_{11} & c_{12} & \cdots & c_{1p}\\
		c_{21} & c_{22} & \cdots & c_{2p}\\ 
		\vdots & \vdots & \ddots & \vdots\\ 
		c_{m1} & c_{m2} & \cdots & c_{mp} 
	\end{bmatrix}}_{12,000 \text{ threads}}
\end{equation*}
where a thread is used to compute a single entry in the product will have $(200 \times 600)$ entries and  will therefore be $12,000$ threads or where multiple 32-thread warps are used there will be 375 warps required to compute the matrix [9783319944623.pdf/203].  Note although internal clock speeds of GPUs are  slower than that of general purpose CPUs, huge throughput gains are achieved by GPUs through using warps with high degrees of parallelization. However compared to general purpose CPUs, the synchronous thread execution in GPUs will require \emph{memory bandwidth}  be carefully managed; note memory bandwidth,the rate at which the cores can access memory locations, is often the bottleneck with such highly parallelized thread architecture. In heterogeneous parallel computing, memory transfer bottlenecks will result in idle general purpose CPU or GPU cores and thus considerable drops in throughput gains. GPUs however do present a host of trade-offs when compared to general purpose CPUs. Although great at repetitious operations such as large matrix-matrix multiplications, GPUs are poorly suited to conditional tasks, task which require considerable flow-control like \emph{if-then-else} statements. \st{However modern pattern recognition including deep learning involve repetitive matrix multiplications during training.}

\subsection{Please edit this section - Key concepts for Parallel Computation in GPUs}

Many-core GPU’s achieve high-throughput through massive parallelism. Modern GPUs contain hundreds and increasingly thousands of processing cores. With this,  GPU architecture has had to support multiple forms of parallelism:
\begin{enumerate}
\item  Instruction-level parallelism extracts sets of independent instructions from a stream of sequential instructions, these are then executed in parallel on multiple processing stages within each processing core - a single instruction stream is typically executed via a single thread on a multi-threaded CPU. 
\item TLP attempts to keep processors busy by rapidly switching between multiple instruction streams whenever the current thread stalls waiting on some other resource. TLP, also known as multi-threading, supports both task-level parallelism and data-level parallelism. For task-level parallelism, work is defined as an abstract unit of useful computation, functionality refers to useful functions, modules, sub-programs which can be grouped by common tasks such as graphics, audio, user-interface, etc.. Task-Level Parallelism divides work by functionality across multiple threads of execution with each subtask being mapped onto its own thread. Because of the heterogeneous nature of the subtasks, this form of parallelism works best on multi-core CPUs. \item For data-level parallelism (DLP), work is defined as by how many data elements (work items) are processed by each individual thread. DLP partitions a data array into smaller chunks with each thread being assigned its own individual chunk of data to process according to some data parallel function, kernel, or program. Each thread executes the same set of instructions but on different pieces of data. Because data- parallel code is largely homogenous across threads, DLP strongly favors GPUs with their massive number of simple cores arranged in SIMD layout. These different types of parallelism (ILP, TLP, and DLP) will be discussed in more detail in “Chapter 2 – Parallelism”.
\end{enumerate}

\subsection{Computation, caching and memory requirements}
GPUs architectures present a trade-off between computation, caching and memory access requirements when compared to GPUs. Therefore, pattern recognition algorithm implementation can often require additional design considerations when working with heterogeneous parallel computers. Consider contemporary flexible multicore CPU design which typically includes  large chip real-estate dedicated to a cache; general purpose CPUs are designed with much larger caches than GPUs, where the GPUs are used for storage of intermediate results from an arithmetic logic unit. For the general purpose CPU, repeatedly accessing an intermediate result from the cache has a much lower latency than accessing it from conventional memory or computing the result again. Ordinarily therefore a large \st{CPU} cache would provide considerable high-throughput gains when compared to a GPU, however for modern pattern recognition including deep learning often involves repetitive matrix multiplications during training, where the sizes of  matrices are often quite large. Although a CPU cache is considerably larger than that of a GPU, it is not large enough for most modern applications and therefore matrix manipulation in CPUs introduces considerable latency due to their use of conventional memory. GPUs are a more reliable option when dealing with large matrices,  because where intermediate values are not available in the cache, it is  often quicker to repeat  a matrix calculation instead of accessing it again from memory. Note therefore, GPU implementation of algorithms present a different set of constraints to a CPU implementation.  Furthermore GPUs provide considerable memory bandwidth when compared to general purpose CPUs. Furthermore, achieving high-throughput gains often necessitates design considerations when developing  neural network structures;  memory bandwidth constraints and multi-threading specifications differ between GPU architectures.

\subsection{Programming interface}

Initially, attempts to harness the GPU’s computational power as a General Purpose GPU (GPGPU), for various non-graphics applications for scientific and engineering applications still required  kernels to have low-level programming for graphic-specific drivers such as DirectX and OpenGL. However GPGPUs were to became the prominent scientific computing platform  after NVIDIA Corporation introduced it's CUDA platform [37,38]; Nvidia's CUDA provided a scalable programming interface within several standard programming libraries and abstractions that allow the technology to be used as general purpose hardware accelerators.  CUDA allows users to write instructions for massively parallel architectures that include GPUs using extensions to standard languages such as C. The CUDA programming model involves dividing a problem into smaller blocks that may be run as separate programs by a group of threads, i.e. the warp. As a result the changes required to convert code for a particular pattern recognition algorithm from a general purpose CPU version to a GPU version, are often limited, because the CUDA libraries resolve most of the low-level details of parallelization for GPUs. 

CUDA allows users to develop separate blocks which are mapped onto the GPU for execution by warps. Multiple warps are managed within the SM by a \emph{warp scheduler}. With each SM having an array of \emph{Streaming Processors} (SPs) termed \emph{cores}, that share cache and control logic. Note that the number of SMs and cores has increased steadily with each generation of GPUs. A \emph{warp divergence} occurs when one or more threads within a warp are forced to run an aberrant set of instructions, leading the SM to run all threads and data-blocks sequentially instead of in parallel thereby causing significantly reduced performance.  A significant requirement in developing  applications for GPU architecture is to limit minimizing the degree of instruction aberrations within warps. Aberrations and differences between threads in a warp may be limited by using warp-aware algorithms and managing data to allow for better flow-control ()SPM2010.pdf/33).


 CUDA manages the mapping onto the GPU and also manages the memory resources. CUDA \st{platform} considers kernel source code  as being either \emph{host code} or \emph{device code}. Host code is source code in the kernel that is compiled for the modern flexible multicore CPU of a heterogeneous parallel computer and typically executes serially. Device code on the other hand, is typically designed to exhibit parallelism and is compiled for the GPU of a heterogeneous parallel computer. CUDA was developed as an extension of the C programming language and therefore all host code is typically written in standard C where as device code which allows users to access the GPU kernel must be written in CUDA C and then compiled for the underlying GPU. The CUDA language utilises it's own compiler, the NVCC compiler.  When the compiler is called, host code in the kernel is compiled  by a standard C compiler for mapping to the CPU, whilst the device code in the kernel code is optimized and then compiled by NVCC compiler for mapping to the GPU. Within the GPU, the kernel code invokes blocks of threads.  The CUDA language reserves the terms \emph{Host}, \emph{Device} and \emph{Global} for the execution of code; Host is used to map a kernel to the CPU, Global invokes kernel the GPU and device is used to map a kernel to the GPU.

\subsection{Scalable Development Framework}

GPUs are effective in parallel-computing because they employ data parallelism to perform parallel computations on large datasets. Kernels enable parallel processing for GPUs, typically this involves computationally-intensive operations such as matrix manipulation, signal processing, computational simulations and deep learning. Kernel development for GPUs is specific to the underlying architecture and the threading model adopted by the kernel language. A developer may further optimize the kernel by minimizing data dependencies and maximizing parallelism  to thereby achieve high-throughput on the GPU. In data parallelism, a large dataset are divided into smaller segments, and each segment is  then processed in parallel by the GPUs parallel architecture \st{massively-parallel processing units} thereby achieving high-throughput gains. GPUs achieve high-throughput  performance by dedicating \st{more transistors}(more real-estate) to their arithmetic logic units (ALUs) for parallelism, whilst constraining real-estate for the purposes of flow control and data caching (SPM2010.pdf). The data parallelism goal is achieved in GPUs by employing single-instruction multiple data (SIMD) to simultaneously execute the same kernel on different segments of the data. SIMD refers to a technique where a single instruction is applied to multiple data elements simultaneously. In a GPU context, this means that multiple processing units in a GPU execute the same instruction on different pieces of data at the same time. SIMD is ideal where the same operation is required on blocks within a large dataset. For example, in image processing, an image may be sub-divided into smaller segments where each region is  processed by a different processing unit of the GPU. Or in pattern recognition applications for example, training examples consisting of large dataset of training examples can be subdivided into smaller segments, and each segment  processed simultaneously by different processing units. Modern GPUs extend SIMD to the single-instruction-multiple-threads (SIMT), a technique introduced by NVIDIA that achieves finely-grained parallelism by allowing a single-instruction to achieve thread-level parallelism, with each thread then achieving data-parallelism.  Under Nvidia's Single Instruction Multiple Thread (SIMT) framework, a single instruction executes multiple threads, i.e. all threads in  the warp  run in parallel and execute the same kernel, thereby achieving high-throughput and low latency. This extension of SIMT to thread-level parallelism is achieved by the introduction of two additional layers of parallelism: (1) warps and (2)groupings of warps of referred to as \emph{thread blocks}.

Load imbalance issues between GPUs is one of the major performance loss factors when developing parallel applications.  This load imbalance may be caused by any number of factors including inherent model issues, algorithm concerns and other systemic infrastructure issues such as cache misses, non-uniform memory access times or interrupts \cite{https://edoc.unibas.ch/59514/1/20180128130357_5a6dbc2d9750f.pdf, https://arxiv.org/pdf/1909.07168.pdf}. To address load imbalance issues, load balancing techniques have been developed around a central goal of re-partitioning a problem space into equal subdomains and then mapping onto GPUs so as to standardize access times. However with dynamic applications, the computational load evolves over time and therefore multiple read-writes are required by a load balance strategy during the course of model operation. Both data parallelism and model parallelism techniques have evolved to address the load balance question. Load balancing techniques come with a computational cost that is hard to predict; therefore with very large architectures involving thousands of GPUs, load balancing should only be applied when the net gain exceeds the losses from load imbalance:


%\subsection{Parallel and Distributed Computing Strategies}
\subsection{Strategies of Parallelism}
 
 Recent developments in pattern recognition has focused on models that are  \emph{effective zero-} or \emph{few-shot} learners with achievable high
 accuracy across many tasks and datasets. Such models  have lead to considerable research in downstream applications. As a result, the number of hyper-parameters in state-of-the-art models continues to grow at an exponential rate thereby leading to  challenges including a)  lengthy model training times due to the sheer number of compute operations or b) more often, instances were models are unable to fit onto a single GPU due to limited main memory capacity. This  lead to parallel scale-outs, parallelism techniques which seek to maximize training throughput of large models given a  batch size and maintaining the strict optimizer semantics, ensuring optimizers are synchronized across all devices.
 
 \subsubsection{Hyperparameter Parallelism.}
 
 \emph{Hyperparameter} tuning can lead to pattern recognition  and deep learning algorithms that achieve the difference between average and superior predictive performance. However model optimization in either a single-criteria case or a multi-criteria presents a trade-off between improved accuracy when achieving the \emph{pareto frontier} and a lower time to model inference \cite{https://developer.nvidia.com/blog/sigopt-deep-learning-hyperparameter-optimization/}.  Modern pattern recognition algorithms such as deep learning and machine learning often contain over half-a-dozen {hyper-parameters}, i.e. parameters which are chosen to control the learning process before an algorithm is applied to a dataset; where tuning for just one hyperparameter can often be a protracted process taking days or weeks. However the optimization of computationally  pattern recognition models can benefit from  massive parallelism, where an ensemble of models with different parameters may each be evaluated independently across the network of processors. Hyperparameter parallelism with runs across independent groups of processors requires no communication overhead and therefore there is no latency between runs. Users should typically adopt \emph{randomized searches} and \emph{grid searches} within the parallel setting to tune for the hyperparameters - both of these approaches may be further parallelized to achieve significant speed-up.  For example, recent work has shown how tensor intra-layer model parallelism, that splits matrix multiplications within each transformer layer over multiple GPUs and pipeline model parallelism where layers of large model are distributed over multiple GPUs. 
 
  \subsubsection{Data Parallelism.}
  
 Data parallelism achieves significant training throughput gains  when the training data is large but the underlying \st{pattern recognition} model still remain small enough to be contained on a single GPU \cite{(Krizhevsky, 2014)}. In these scenarios, the model parameters are shared across a number of  GPUs, each running the same model but where each processor considers a subset of the training data; the GPUs are then required to provide  updates periodically. Each GPU therefore computes  its own gradients;  gradients are then aggregated to a parameter server through summation.  Aggregated gradient weights are  rebroadcast, requiring each GPUs to update to the same  weights; the aggregation and rebroadcasting of weights is termed \emph{weight synchronization}, it ensures models remain consistent across all processors.  In \emph{synchronous data parallelism}, latency is however introduced as  perfect synchronization is required between updates, therefore processors may  have to wait for others processors in order to complete a synchronous  update. Synchronous data parallelism requires significant inter-GPU communication and further, the slowest processor also serves as a bottleneck;  overheads increase disproportionately with model size, thereby greatly limiting scalability \cite{(Seide et al., 2014; Strom, 2015; Alistarh et al., 2016; Zhou et al., 2016; Aji & Heafield, 2017; Lin et al., 2017}.  \emph{Asynchronous data parallelism}, a modification on the standard approach, allows GPUs to send and receive gradient weight updates independently as there are no lock mechanisms and no synchronization required between processors \cite{J. Dean et al. Large scale distributed deep networks. NIPS Conference, 2012}. This strategy approach eliminates bottleneck issues but it does not reduce inter-GPU communication overheads and additionally introduces model robustness concerns; i.e. the lack of synchronized gradient weights across GPUs and the lack of guards to prevent GPUs from overwriting the progress of other processors  both introduce model inefficiencies. The net throughput gains from an asynchronous data parallelism strategy are however considerable and it tends to be the preferred approach for {data parallelism}.
 
 \subsubsection{Model Parallelism.}
 
 Model parallelism  tends to be well adopted to scenarios  where  models requiring large domains can not be \emph{efficiently} computed using a single GPU or even on a multi-GPU server, as the memory capacity on GPUs remains limited. Therefore the compute operation times can be quite lengthy. For example, consider convolutional neural networks where in those instances the sequential model's layers are then partitioned across $N$ GPUs, where $N$ corresponds a multiple of the number of hidden layers \cite{[76] A. Coates and A. Ng. Learning feature representations with k-means. Neural networks: Tricks of the Trade} Each GPU is provided with exactly the same batch inputs, although each GPU solves for different hidden activations and gradient of weights. For a given GPU, it will hold only the weight matrix that is to be  multiplied with its  activations present. The additional overheard and therefore latency is introduced by inter-GPU communication for  model parallelism approach when:
 \begin{enumerate}
\item  During training  in pipelined model parallelism, GPUs are required to communicate to other GPUs after solving their activations. 
 \item The results of  activations are required from other GPUs  in order to solve for the gradient weights between it's activations and the hidden activations of  layers on another GPU. In a learning pipeline, as the first GPU updates it's gradient weights, GPUs further in the learning pipeline must initially adopt stale gradient weights, this  has been found to lead to network instability and a loss of model accuracy \cite{https://arxiv.org/pdf/1809.02839.pdf}. 
  \end{enumerate}
As each GPU is only responsible for  weight updates of it's assigned model's layers the amount of inter-GPU communication among  is significantly less than that of data parallelism. Furthermore, any number of tricks and approaches may be adopted to limit the degree of communication between specific layers, although the resulting approximations will impact model performance. Note that where the computation of a model is to be completed in sequential fashion, a naive model parallelism may introduce unnecessary latency as GPUs are required to sit idle. However in \cite{https://arxiv.org/pdf/1809.02839.pdf}, a pipelined model parallel execution method was proposed that achieved high GPU utilization whilst achieving robust training accuracy through a novel weight prediction technique. The model robustness issues were addressed in \cite{Pipedream: Fast and efficient pipeline parallel DNN
training.}, by ensuring forward and backward passes are set to the same weights, by storing multiple versions of weights. In general, model parallelism is not helpful in cases where the number of parameters in the neural network are small, and should only be used for large networks. Various forms of model parallelism have been proposed to  address issues of GPU utilization of high communication overheads and limits on the number of devices that can be used in training.

\subsubsection{Pipeline model parallelism}
In pipeline model parallelism,  training data is divided into a number sub-batches, with each sub-batch being processed by a different segment of the model. For GPU pipelining each segment can be loaded into the memory of a separate GPU, allowing the overall model to be processed in parallel across multiple GPUs. In a GPU pipeline, the output of one segment being fed as an input to another segment; each segment completes a different set of model operations on their input data. For example, when used on models with the same transformer block repeated, each device can be assigned an equal number of transformer layers. By dividing a model into segments, each segment can be executed on a separate GPU, allowing  different segments to operate in parallel with overlapping execution time. During a forward pass, each GPU passes an intermediate activation to a next stage. During a backward pass, each GPU passes a gradient update for the input tensor back to the previous pipeline stage. This allows GPU to compute simultaneously and increase training throughput.  A drawback to  pipeline model parallel training is that there will always be waiting time or \emph{bubble time}, i.e. the proportion of time the pipeline stage spends idle, waiting for data to arrive. In other words, it is the time during which a pipeline stage does not have any work to do because it is waiting for the results of a previous operation. Bubble time can be a significant factor in pipeline model parallelism performance, especially if there are many pipeline stages with long latencies. Also pipeline model parallelism introduces additional overhead due to the need to communicate data between different segments and synchronize their execution.  Note that there are more asymmetric model architectures, where assignments to pipeline stages are more difficult  [22, 29, 41]. 

\subsubsection{Tensor model parallelism}
Tensor model parallelism, individual layers of the model are partitioned over multiple devices based on partitioning scenarios. Tensor parallel training splits a model or tensor into $N$ blocks along a specific dimension and each smaller tensor therefore holds $1/N$th of the whole tensor.  Tensor model parallelism allows us to distribute the tensor over GPUs whilst maintaining  the computational flow remains correct; however additional communication overhead is required to ensure the results are correct. Taking a matrix-matrix multiplication example again, i.e. $A.B=C$, we divide $B$ into two blocks. We then multiply $A$ with each column in $B$ on the respective device, where we obtain $[A.B_0, A.B_1, A.B_2, ... A.B_n]$. Each GPU initially holds a partial set of the results, e.g. GPU rank 0 holds ${A.B_{[0]}}$. The final set of solutions is then obtained by concatenating along the dimensions.
	 %[https://cs.stanford.edu/~matei/papers/2021/sc_megatron_lm.pdf40].

\begin{equation*}
	\underbrace{\begin{bmatrix}
			a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
			a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\ 
			\vdots & \vdots & \ddots & \vdots\\ 
			a_{m,1} & a_{m,2} & \cdots & a_{m,n} 
	\end{bmatrix}}_{200 \times 40}
	\times
	\underbrace{\begin{bmatrix}
			b_{1,1} & b_{1,2} & | \square & \square \\
			b_{2,1} & b_{2,2} & | \square & \square \\ 
			\vdots & \vdots & | \square & \square \\ 
			b_{n,1} & b_{n,2} & | \square & \square 
	\end{bmatrix}}_{40 \times 300}
	=
	\underbrace{\begin{bmatrix}
			c_{1,1} & c_{1,2} & | \square & \square \\
			c_{2,1} & c_{2,2} & | \square & \square \\
			\vdots & \vdots & | \square & \square \\
			c_{m,1} & c_{m,2} & | \square & \square 
	\end{bmatrix}}_{200 \times 300}
\end{equation*}

\begin{equation*}
	\underbrace{\begin{bmatrix}
			a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
			a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\ 
			\vdots & \vdots & \ddots & \vdots\\ 
			a_{m,1} & a_{m,2} & \cdots & a_{m,n} 
	\end{bmatrix}}_{200 \times 40}
	\times
	\underbrace{\begin{bmatrix}
			\square & \square | & b_{1,n-1} & b_{1,n}  \\
			\square & \square | & b_{2,n-1} & b_{2,n}   \\ 
			\square & \square | & \vdots & \vdots   \\ 
			\square & \square |  & b_{n,n-1} & b_{n,n}  
	\end{bmatrix}}_{40 \times 300}
	=
	\underbrace{\begin{bmatrix}
			\square & \square|  &c_{1,n-1} & c_{1,n}\\
			\square & \square |&c_{2,n-1} & c_{2,n} \\ 
			\square & \square |&\vdots & \vdots \\ 
			\square & \square | & c_{m,n-1}& c_{m,n}
	\end{bmatrix}}_{200 \times 300}
\end{equation*}

Tensor model parallelism typically requires considerable hardware support, including  device interconnections and specialized memory architectures to enable efficient communication between the processing units.


\subsubsection{Hybrid Parallelism.}

Although model parallelism on GPUs are better suited to models which require a large domain and data parallelism is suited to models with a smaller domain, however in hybrid parallelism both types of parallelism  may be combined effectively by using different approaches across the network. For example, in a CNN the majority of computation takes place early in the network and the model parametrization is required at later stages of the network, therefore data parallelism may be implemented in the earlier layers and model parallelism implemented for later stages of the network [Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance / Neural Networks and Deep Learning, 254].

\subsubsection{Comparison of Parallelism Strategies.}

In Table \ref{parallelism_strategies}, the parallelism approaches of data parallelism, model parallelism and hybrid parallelism in GPUs are compared. 

\begin{table}[htbpp]
	\small
	\setlength\tabcolsep{3pt} % default: 6pt
	\captionsetup{font=small,skip=0.333\baselineskip}
	\caption{Comparison of parallelism strategies for deep neural networks.} \label{T2.6}

	\begin{center}
	\begin{tabularx}{\textwidth}{@{} p{\mylen} LLL @{}}
		\toprule
		& 
		Load Balance  & 
		Communication Overhead& 
		Training Quality \\ 
		\midrule
		Data Parallelism & 
		Load balancing under model parallelism is difficult to achieve. The different layers of a model have different levels of complexities, therefore  when partitioning across a number of GPUs it is inherently difficult to ensure all the computational loads remain balanced. In (Harlap et al., 2018)  GPUs were profiled offline, with load balancing  across the network   introduced  using dynamic programming.  Reinforcement learning with dynamic programming has  also been proposed as a means to  dynamically partition  model layers across GPUs (Mirhoseini et al., 2017). With massively parallel processor applications, per-GPU batch sizes are small and load-balance overheads can be considerable. & 
		The inter-GPU communication overhead involves mainly transmitting information on  gradients for weight synchronization. In \cite{https://arxiv.org/pdf/1809.02839.pdf}, it was found that for a DNN, data parallelism required up to 13X the inter-GPU communication overhead of model parallelism; this relatively high level of inter-GPU communication overhead for data parallelism
		leads to considerable processing latency. Massively parallel applications with low per-GPU batch sizes will tend to increase the communication overhead.
		
		& 
	 	For data parallelism, if each GPU performs the training process for a mini-batch, then the effective mini-batch size is the number of GPU times the mini-batch size. To avoid the adverse effect of a large mini-batch size, we
	 	could choose to partition a mini-batch among GPUs. However, it could under-utilize GPU resources. As the number of GPU increases, it becomes more and more challenging to select a data partition size that is good for DNN training efficiency. For data parallelism model accuracy increases as the training process evolves\\ 
		\midrule
		Model Parallelism & 
		 For model parallelism, achieving a
		load balance is  more challenging. 
		Since the complexity of different 
		DNN layers varies, it would introduce 
		significant  efforts for programmers to
		partition model layers to GPUs in a
		balanced way. A few prior works 
		havey addressed this issue(Harlap 
		et al., 2018; Mirhoseini   et al., 2017).
		PipeDream (Harlap et al., 2018) 
		proposes to profile the processing time 
		of each layer offline and  use dynamic 
		programming to partition the model.
		Mirhoseini et al. (Mirhoseini et al., 2017) 
		adopts  reinforcement learning to dynamically 
		partition the model at run-time.& 
		Inter-GPU communication for model synchronisation primarily involves the transfer of intermediate model data.
		& 
		In terms of model robustness, model parallelism introduces a staleness issue; before a weight is updated in by a GPU running an earlier layer, later layers are required to use stale gradient weights. This use of stale weights reduces model accuracy and introduces instability into the network. For model parallelism the model accuracy metric is unstable due to the stale gradient weight issue, as model training progresses. \\
		\midrule
		Hyper- parameter Parallelism & As each GPU runs a separate version of the model to obtain hyperparameters, models are inherently load balanced.  & In terms of model robustness, hyper parallelism introduces a staleness issue; before a weight is updated in by a GPU running an earlier layer, later layers are required to use stale gradient weights. This use of stale weights reduces model accuracy and introduces instability into the network. For model parallelism the model accuracy metric is unstable due to the stale gradient weight issue, as model training progresses.  & In terms of model robustness, model parallelism introduces a staleness issue; before a weight is updated in by a GPU running an earlier layer, later layers are required to use stale gradient weights. This use of stale weights reduces model accuracy and introduces instability into the network. For model parallelism the model accuracy metric is unstable due to the stale gradient weight issue, as model training progresses. \\
		\bottomrule
	\end{tabularx}
\end{center}
\label{parallelism_strategies}
\end{table}

\begin{table}[htbpp]
	\small
	\setlength\tabcolsep{3pt} % default: 6pt
	\captionsetup{font=small,skip=0.333\baselineskip}
	\caption{Comparison of parallelism strategies for deep neural networks.} \label{T2.6}
	
	\begin{center}
		\begin{tabularx}{\textwidth}{@{} p{\mylen} LLL @{}}
			\toprule
			& 
			Load Balance  & 
			Communication Overhead& 
			Training Quality \\ 
			\midrule
			Data Parallelism & 
			Load balancing under model parallelism is difficult to achieve. The different layers of a model have different levels of complexities, therefore  when partitioning across a number of GPUs it is inherently difficult to ensure all the computational loads remain balanced. In (Harlap et al., 2018)  GPUs were profiled offline, with load balancing  across the network   introduced  using dynamic programming.  Reinforcement learning with dynamic programming has  also been proposed as a means to  dynamically partition  model layers across GPUs (Mirhoseini et al., 2017). With massively parallel processor applications, per-GPU batch sizes are small and load-balance overheads can be considerable. & 
			The inter-GPU communication overhead involves mainly transmitting information on  gradients for weight synchronization. In \cite{https://arxiv.org/pdf/1809.02839.pdf}, it was found that for a DNN, data parallelism required up to 13X the inter-GPU communication overhead of model parallelism; this relatively high level of inter-GPU communication overhead for data parallelism
			leads to considerable processing latency. Massively parallel applications with low per-GPU batch sizes will tend to increase the communication overhead.
			
			& 
			For data parallelism, if each GPU performs the training process for a mini-batch, then the effective mini-batch size is the number of GPU times the mini-batch size. To avoid the adverse effect of a large mini-batch size, we
			could choose to partition a mini-batch among GPUs. However, it could under-utilize GPU resources. As the number of GPU increases, it becomes more and more challenging to select a data partition size that is good for DNN training efficiency. For data parallelism model accuracy increases as the training process evolves\\ 
			\midrule
			Tensor Model Parallelism & 
			Achieving load balancing is challenging; as the complexity of different 
			 layers varies, it would require 
			significant  effort for programmers to
			partition model layers across GPUs in a
			balanced way.
			& 
			Inter-GPU communication for model synchronisation primarily involves the transfer of intermediate model data.
			&
			In terms of model robustness, model parallelism introduces a staleness issue; before a weight is updated in by a GPU running an earlier layer, later layers are required to use stale gradient weights. This use of stale weights reduces model accuracy and introduces instability into the network. For model parallelism the model accuracy metric is unstable due to the stale gradient weight issue, as model training progresses. \\
			\midrule
			Pipeline Model Parallelism 
			& Bubble time can be a significant factor in pipeline model parallelism performance, especially if there are many pipeline stages with long latencies. Also pipeline model parallelism introduces additional overhead due to the need to communicate data between different segments and synchronize their execution.  Note that there are more asymmetric model architectures, where assignments to pipeline stages are more difficult  [22, 29, 41]. 
			& Inter-GPU communication is required for peer-to-peer communication to send data between pipeline stages. This requires scheduling multiple compute and communication tasks for micro-batches and stages.
			& Model accuracy is unstable due to the stale gradient weight issue, particularly as  training progresses. \\
			\bottomrule
		\end{tabularx}
	\end{center}
	\label{parallelism_strategies}
\end{table}

\bibliography{Manuscript_Hardware}

\end{document}

 %%The GPU, where instructed by the kernel to run a function, manages the parallel execution of threads may be %%generated by a kernel; a collection of threads together managed by the GPU are known as \emph{grids}, a grid is %%created for each invoked CUDA function. CUDA organizes these threads into logical blocks, where
%%each thread is synchronized and has its ID to determine which data to work on,
%%within the block that executes the single function. 


%%The GPU
%%processes instructions parallelly by allocating multiple threads to each task. The
%%threads generated by the kernel are known as grid;  grids are created for each
%%invoked CUDA function. CUDA organizes these threads into logical blocks, where
%%each thread is synchronized and has its ID to determine which data to work on,
%%within the block that executes the single function. Thousands of threads together
%%execute one function, referred to as kernel. However, data managed by each thread
%%is different.


		
		----------->
		
	
		
	
		AFML warned readers that back-testing is not a research tool. Feature importance is. A backtest cannot help us develop an economic or financial theory.
		
	\begin{itemize}
		\item 1.6 Audience
		\item 1.7 Five Fallacy's of High Performance Computing
		\item -1.7.1 ML Is the Holy Grail versus ML Is Useless
	\item 	- 1.7.2 ML Is a Black Box
		\item -1.7.3 Finance Has Insufficient Data for ML
		\item -1.7.4 The Signal-to-Noise Ratio Is Too Low in Finance
	\item 	- 1.7.5 The Risk of Overfitting Is Too High in Finance
		\item 1.8 The Future of Financial Research
	\item 	1.9 FAQ's
	\item 	- In Simple Terms, What Is ML?
	\item 	-  How Is ML Different from Econometric Regressions?
	\item 	- How Is ML Different from Big Data?
	\item 	- How Is the Asset Management Industry Using ML?
	\item 	- And Quantitative Investors Specifically?
	\item 	-  What Are Some of the Ways That ML Can Be Applied to Investor Portfolios?
	\item 	- What Are the Risks? Is There Anything That Investors Should Be Aware of or Look Out For?
	\item 	- How Do You Expect ML to Impact the Asset Management Industry in the Next Decade?
	\item 	- How Do You Expect ML to Impact Financial Academia in the Next Decade?
	\item 	- Isn?t Financial ML All about Price Prediction?
	\item 	- Why Don?t You Discuss a Specific Investment Strategy, Like Many Other Books Do?
	\item 	1.10 Conclusions \\
	\item OpenMP \\
	\item Topology 
	\item Parallel Sorting \\
	\item Parallel linear algebra \\
	\item Mapreduce Paradigm \\
	\end{itemize}

\bibliographystyle{references}





12 Puch-Solis, R.; Rodgers, L.; Mazumder, A.; Pope, S.; Evett, I.; Curran, J.; Balding, D. Evaluating forensic DNA profiles using peak heights, allowing for multiple donors, allelic dropout and stutters. Forensic Sci. Int. Genet. 2013, 7, 555?563. [CrossRef]
13. Puch-Solis, R. A dropin peak height model. Forensic Sci. Int. Genet. 2014, 11, 80?84. [CrossRef] [PubMed]
14. Perlin, M.W.; Legler, M.W.; Spencer, C.E.; Smith, J.L.; Allan, W.P.; Belrose, J.L.; Duceman, B.W. Validating TrueAllele�DNA mixture interpretation. J. Forensic Sci. 2011, 56, 1430?1447. [CrossRef] [PubMed]
15. Taylor, D.; Bright, J.-A.; Buckleton, J. The interpretation of single source and mixed DNA profiles. Forensic Sci. Int. Genet. 2013, 7, 516?528. [CrossRef] [PubMed]
16. Robert, G.C. Validation of an STR peak area model. Forensic Sci. Int. Genet. 2009, 3, 193?199.
17. Bleka, �.; Storvik, G.O.; Gill, P. EuroForMix: An open source software based on a continuous model to evaluate STR DNA profilesfrom a mixture of contributors with artefacts. Forensic Sci. Int. Genet. 2016, 21, 35?44. [CrossRef]
@article{cowell2011probabilistic,
	title={Probabilistic expert systems for handling artifacts in complex DNA mixtures},
	author={Cowell, Robert G and Lauritzen, SL and Mortera, Julia},
	journal={Forensic Science International: Genetics},
	volume={5},
	number={3},
	pages={202--209},
	year={2011},
	publisher={Elsevier}
}
@article{cowell2007gamma,
	title={A gamma model for $\{$DNA$\}$ mixture analyses},
	author={Cowell, Robert G and Lauritzen, Steffen Lilholt and Mortera, Julia},
	journal={Bayesian Analysis},
	volume={2},
	number={2},
	pages={333--348},
	year={2007},
	publisher={International Society for Bayesian Analysis}
}

\item  Importance: heterogeneous parallal computing tools can determine the relative informational con- tent of explanatory variables (features, in ML parlance) for explanatory and/ or predictive purposes (Liu 2004). For example, the mean-decrease accuracy (MDA) method follows these steps: (1) Fit a ML algorithm on a particular data set; (2) derive the out-of-sample cross-validated accuracy; (3) repeat step (2) after shuffling the time series of individual features or combinations of features; (4) compute the decay in accuracy between (2) and (3). Shuffling the time series of an important feature will cause a significant decay in accuracy. Thus, although MDA does not uncover the underlying mechanism, it discovers the variables that should be part of the theory.

//  DNA REFERENCES
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8535381/#B124-genes-12-01559
https://www.eurecom.fr/sites/default/files/jobs/DS_RA_ING_OLIGOARCHIVE_062022_US.pdf
https://publications.aston.ac.uk/id/eprint/43499/1/etls_2020_0340c.pdf
https://thesai.org/Downloads/Volume12No11/Paper_15-DNA_Profiling_An_Investigation_of_Six_Machine_Learning_Algorithms.pdf
%%https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjdg8GI0437AhWM57sIHYrSAmkQtwJ6BAhAEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3ygPF-Qshkc&usg=AOvVaw2_-Sd-mCmcaVtyZ4xlIY_A

Clusters entered the market in the late 1980s and replaced MPPs for many applications. A cluster is a parallel computer comprised of numerous commercial computers linked together by a commercial network. Clusters are the workhorses of scientific computing today and dominate the data centers that drive the modern information era. Based on multi-core processors, parallel computing is becoming increasingly popular.


In a parallel programming context, \emph{unintended nondeterminism} is typically defined as an abnormal program behaviour caused by dependence of a result on the relative interleaving \st{timing} of events executed within a program. I once heard stated that by a presenter at a conference "we should expect no significant performances improvements from heterogeneous parallel computing". Curious, I enquired as to why. She replied, “Because any future performance improvements, particularly as new technologies are introduced into heterogeneous systems, will be plagued by unintended nondeterministic behaviour and heterogeneous parallel computers designs cannot control them.” I retorted that synchronization in heterogeneous CPU-GPU systems, in most cases, had continuously evolved to cater for unintended nondeterminism.

Synchronization remains an open ended engineering problem with many design considerations. In an ideal synchronization, there are  limited constraints to scheduling and therefore high resource utilization. At the same time there are costs to rigorous  global scheduling control. Therefore a trade-off often exists between increasing
synchronization costs and decreased resource idleness. The problems of synchronization do not exist exclusively in parallel systems. Synchronization comes with concurrency; parallelism is a special case of concurrency. There are many different ways to achieve synchronization.  Providing synchronization mechanisms to eliminate race conditions is one of the main purposes of a parallel programming model. In multithreading, synchronization is achieved with explicit synchronization primitives.  In multithreading, synchronization is achieved with explicit synchronization primitives. In hardware-based shared memory, these mechanisms are built on atomic processor instructions. 

