
\documentclass[12pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\title{Introduction to Information Theory}
\author{Author's Name}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides an introduction to Information Theory, covering fundamental concepts such as entropy, mutual information, and the data processing inequality. Through definitions, theorems, and proofs, we explore the mathematical underpinnings of how information is quantified, transmitted, and encoded. Practical examples are provided to illustrate these concepts in action.
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}
Information theory is a mathematical framework for quantifying information, understanding how it is transmitted, and how it can be encoded efficiently. It was founded by Claude Shannon in the mid-20th century and has since become a cornerstone in communications and coding theory.

\section{Preliminaries}
\subsection{Probability Theory Basics}
A brief overview of probability theory, including probability spaces, random variables, and expected value, which are foundational to understanding information theory.

\subsection{Notations and Definitions}
Introduce notations used in information theory, such as \(H(X)\) for entropy, \(I(X;Y)\) for mutual information, and more.

\section{Entropy}
\subsection{Definition of Entropy}
Entropy, denoted as \(H(X)\), is a measure of the unpredictability or uncertainty of a random variable. The entropy of a discrete random variable \(X\) is defined as:
\[
H(X) = -\sum_{x \in X} p(x) \log p(x)
\]
where \(p(x)\) is the probability mass function of \(X\).

\subsection{Properties of Entropy}
Discuss the properties of entropy, including non-negativity, maximization, and the relation to uniform distribution.

\section{Mutual Information and Conditional Entropy}
\subsection{Mutual Information}
Define mutual information \(I(X;Y)\) and its significance in measuring the amount of information that two random variables share.

\subsection{Conditional Entropy}
Conditional entropy \(H(X|Y)\) quantifies the amount of information needed to describe the outcome of a random variable \(X\) given that the value of another random variable \(Y\) is known.

\section{The Data Processing Inequality}
The data processing inequality theorem states that no processing of transmitted data, without additional information, can increase the mutual information between the transmitted and received data.

\section{Applications of Information Theory}
\subsection{Coding Theory}
Discuss how information theory guides the development of error-correcting codes and efficient data compression algorithms.

\subsection{Communication Systems}
Explain the relevance of information theory in designing communication systems, including channel capacity and the Shannon limit.

\section{Conclusion}
Summarize the impact of information theory on modern communications and data processing, highlighting its continuing relevance and areas of active research.

\end{document}
