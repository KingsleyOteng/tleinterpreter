\documentclass[7pt]{article}
\usepackage[framemethod=TikZ]{mdframed}[2013/07/01]
\usepackage[T1]{fontenc}
\usepackage{framed}
\usepackage{physics}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{changepage}
\newlength{\rulethickness}
\setlength{\rulethickness}{1.2pt}
\newlength{\rulelength}
\setlength{\rulelength}{15cm}
\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amscd}
\usepackage[colorlinks]{hyperref}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{xparse}
\usepackage{soul}
\usepackage{caption}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{lipsum}% http://ctan.org/pkg/lipsum


%%%% watch this video
%%%%https://www.youtube.com/watch?v=gtRLmL70TH0

%%%%also watch this
%%%%https://www.youtube.com/watch?v=p7Lv9GxigYU

\begin{document}
\newpage


 \begin{titlepage}
	\vspace*{\fill}
	\begin{center}
			\center{{\huge CHAPTER TWO: HISTORICAL COMMENTS}}\\[0.5cm]
	\end{center}
	\vspace*{\fill}
\end{titlepage}

 \begin{titlepage}
	\vspace*{\fill}
	\begin{center}
		\center{{\small \textbf{Objectives}}}\\
	\end{center}
	By the end of this chapter you should be able to:\\
1. \\
2. \\
3. \\
4. \\
5. \\
6. \\
	\vspace*{\fill}
\end{titlepage}

\newpage

\vspace{15.0in}

In the words of F.W. Maitland,Simplicity is the outcome of technical subtlety; it is the goal not the starting point'


\newpage


\vspace{0.073in}

The problem of 'learning' and inferring from raw data is immensely necessary, with a formidable and sedulously crafted [academic] history. For instance, a court in 1867 allowed Benjamin Pierce to present a legal argument that given a binomial distribution $prior$, the degree of exactness across each of the 30 signatures in the will of a deceased Sylvia Ann Harlow suggested fraud; Pierce's argument was to serve as key case law in the development of the Common Law of 'Torts'. Sadly, the apparent difficulties when introducing in legal proceedings any evidence based solely off of data samples, led to considerable professional reticence amongst lawyers and there were almost no further legal examples over the next 100 years. It was under pressure in 1968 in the Collins case, that defence lawyers first applied models to make probabilistic inference around regularities in place of evidence; the first such documented case of its kind in American legal history. But lawyers whom were involved in the Collins case had a limited understanding for the scope or applicability of evidence based on their inferences: 
\begin{itemize}

\item underestimating the dependence in the evidence presented; i.e. that even an innocent bystander who vaguely matched a description could demonstrate other characteristics matching the evidence, \\
\item that an individual with a body of low weighted evidence is likely innocent. 

\end{itemize}

*It is important therefore that we have a thorough appreciation for the bottom-up application and any inherent dangers in using AI algorithms in a  complex modelling system. *

Data, particularly large data, tends to be heterogeneous, generated by models with different underlying distributions, therefore with differing [[characteristics]] and [[behaviours]]; actually, .this heterogeneity biases the data. 


[[To illustrate the impact of biasing in data consider a hypothetical prisoner study which measured how the outcome, proportion of violent crimes, changes relative to the proportion of unemployed (Figure 1). Regression analysis (solid red line) shows no positive relationship in the population between these variables; a more penurious prisoner background did not point to a greater inclination to commit violent crime. A lack of a positive trend suggests that increases in violent crime are not associated with lower unemployment. However, unbeknown to researchers, the study population was heterogeneous, composed of cohorts that vary in their employability level—geographic areas that did not exhibit a growth in per capita income, areas with low a police presence and being black.]]
 
 
 So far, we have discussed decision theory in the context of classification prob- lems to learning. We now turn to the case of regression problems, such as the curve fitting example discussed earlier; specifically where we proposed [four approaches]. The decision stage consists of choosing a specific esti- mate y(x) of the value of t for each input x. 


Let's consider another legal example - the use of commercial risk assessment software in the determination of recidivism, illustrated in Figure 1.1. 

We should think responsibly,  as these tools are actually making decisions which affect peoples’ lives (rewrite).

Thankfully we have options available that let us assess the fairness of models when applied to data: these are standard bias and fairness functions exist.


Each individual corresponds to a 28×28 pixel image and so may therefore be represented by a model $x$ comprising 784 real numbers. The goal is to develop a criminal risk assessment tool, an algorithm that will take such a vector $x$ as input, take in the details of a defendant’s profile and spit out a recidivism score; a digit of 1 to 4 "low risk"; 5 to 7 "medium risk"; 8 to 10 as "high risk" — this single number represents the likelihood that he or she will reoffend. This is a nontrivial problem due to the wide variability of considerations (refer to COMPAS). It could be tackled using handcrafted rules or heuristics for distinguishing the criminals based on their individualized case histories, but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on, and invariably gives poor out-of-sample results. A judge would factor each score into a myriad of decisions that can determine what type of rehabilitation services particular defendants should receive, whether they should be held in jail before trial, and how severe their sentences should be. A low score paves the way for a kinder fate. A high score does precisely the opposite. In theory, it also reduces any bias influencing the process, because judges are making decisions on the basis of data-driven recommendations and not their gut. You may have already spotted the problem. Modern-day risk assessment tools are often driven by algorithms trained on historical crime data.

-----

Far better results can be obtained by adopting a machine learning algorithm in which a large set of N digits {x1,...,xN} called a training set is used to tune the parameters of an adaptive model. The categories of the digits in the training set are trained on historical criminal data in advance; we can express the category of a digit using target vector t, which represents the degree of recidivism risk in a corresponding digit. Suitable techniques for representing categories in terms of vectors will be discussed later.  As we’ve covered before, machine-learning algorithms use statistics to find patterns in data. So if you feed it historical crime data, it will pick out the patterns associated with crime. But those patterns are statistical correlations—nowhere near the same as causations. If an algorithm found, for example, that low income was correlated with high recidivism, it would leave you none the wiser about whether low income actually caused crime. But this is precisely what risk assessment tools do: they turn correlative insights into causal scoring mechanisms.

-----
		 
The result of running the machine learning algorithm can be expressed as a function y(x) which takes a new digit image x as input and that generates an output vector y, encoded in the same way as the target vectors. The precise form of the function y(x) is determined during the training phase, also known as the learning phase, on the basis of the training data. Once the model is trained it can then determine the risk of recidivism in other cases, whom are said to comprise a test set. The ability to categorize correctly new cases that differ from those used for training is known as generalization. In practical applications, the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors, and so generalization is a central goal in pattern recognition.
		 
		 --------
		 
Now populations that have historically been disproportionately targeted by law enforcement—especially low-income and minority communities—are at risk of being slapped with high recidivism scores. As a result, the algorithm could amplify and perpetuate embedded biases and generate even more bias-tainted data to feed a vicious cycle. Because most risk assessment algorithms are proprietary, it’s also impossible to interrogate their decisions or hold them accountable.
		 
		 --------
		 
For most practical applications, the original input variables are typically preprocessed to transform them into some new space of variables where, it is hoped, the pattern recognition problem will be easier to solve. For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a fixed size as defined by the [model framework]. This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes of user inputs. This pre-processing stage is sometimes also called 'feature extraction', quite generally. Note that new test data must be pre-processed using the same steps as the training data. Pre-processing might also be performed in order to speed up computation. For example, if the goal is real-time face detection in a high-resolution video stream, the computer must handle huge numbers of pixels per second, and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible. Instead, the aim is to find useful features that are fast to compute, and yet that also preserve useful discriminatory information enabling faces to be distinguished from non-faces. These features are then used as the inputs to the pattern recognition algorithm. For instance, the average value of the image intensity over a rectangular subregion can be evaluated extremely efficiently (Viola and Jones, 2004), and a set of such features can prove very effective in fast face detection. Because the number of such features is smaller than the number of pixels, this kind of pre-processing represents a form of dimensionality reduction. Care must be taken during pre-processing because often information is discarded, and if this information is important to the solution of the problem then the overall accuracy of the system can suffer.
		 
		 -------
Our aim in writing this book was to provide a clear and yet comprehensive  framework by which to  appreciate and implement deep learning and pattern recognition techniques, evaluate models and construct deep learning frameworks optimized for even a lightweight GPU platform. It would be worth highlighting that this text is not intended to be an exhaustive literature review on deep learning or of the models that are of practical importance. However the  references section provides sources that may be better able to provide guidance for suitably interested readers. 

In our discussion, we have tended to focus more on a deep learning framework to analysing credit risk, supplemented by other methods we think are useful in particular applications where the structural framework falls short. As it turns out, there are many applications where we will recommend the reduced-form modelling approach or a data-driven econometric one. A well-trained analyst will be comfortable with a variety of models and frameworks. While our preferred framework is grounded in economic explanations of default, our discussions of other frameworks are generally motivated by the challenge of making use of existing data. While we often find structural models most appealing from an intuitive perspective, in a number of settings such models cannot be practically implemented and thus pragmatism, rather then dogma, guides us. When helpful in highlighting our recommended approach, we discuss some other popular implementations of the models for certain applications.”

So to therefore set the atmosphere and enhance a reader’s understanding of deep learning models from a practical standpoint, we attempt to provide an anecdotal history of a few key ideas and their instigators, that have contributed to the discipline over time. We hope that this contextualization of how deep learning has developed will improve a reader’s grasp of fundamental concepts. This chapter is by no means comprehensive in the descriptions (again we refer the reader to the texts cited in the References to expand on our exposition); but we have highlighted the key ideas and persons contributed to the development of machine learning as it is practised today.
\newpage
\textbf{Andrei Nikolaevich Kolmogorov (1903 - 1987)}: when we consider information theory and entropy we typically consider Claude Shannon, who last century contributed at a furious pace to this discipline. Andrei Kolmogorov, a Russian mathematician evolved his \textit{Theory of Algorithmic information Theory} in the mid-1950's a few decades after Shannon's less rigorous first efforts; in these early days Kolmogorv was often known to give frenetic lectures on the topic,  fascinating both Russian and non-Russian speakers, with the types of insights which at the time only he could understand. Andrei came from a small family - no siblings, a mother and a clergyman father Nikolai a clergyman who served as an agriculturalist. He knew neither growing up, his father was killed in World War I and his mother died on his day of birth. But Andrei grew up in the trappings of comfort, raised by a sister of his mother who passed on traits of responsibility, industriousness and and an independence of opinion- even in situations that lead others to consider him foolish. He knew therefore that technical problems could be solved through tenacity, diligence and through a pursuit of knowledge; his  contributions would there continue well into his 80's, after his information theory period wound down.    

 After an apprenticeship as a railway conductor, he enrolled in Moscow University initially to study metallurgy but his aptitude for mathematics would lead him to switch majors. In his second year as an undergraduate degree he undertook research - such that by the time of his graduation he had published 10 papers, in what he considered a particularly brutal undergraduate degree. Graduating by 1925 with a first degree in mathematics, by 1929 time he had completed a doctorate and was then elected to the Institute of Mathematics and Mechanics at Moscow State University; by 1931 aged 28 he was a Professor at the same institute.In 1965 Andrei presented \textit{algorithmic information theory}, a theory of randomness measured by it's complexity; not wholly distinct from Shannon's more fashionable classical information theory. 
 

In \textit{Three Approaches To The Quantitative Definition of Information}, Kolmogorov introduces algorithmic information theory, his attempt to reconstruct information theory based on algorithms; displaying a strong "utilitarian bent", Kolmogorov wrote: 

\vspace{0.5cm}
\leftskip1.5cm\relax
\rightskip1.5cm\relax

	"It would, for example, be of interest to estimate the entropy of Russian Texts that could be regarded as sufficiently accurate (in terms of content) translations of a given foreign-language text. It is only "residual entropy" that makes it \textit{possible} to translate poetry, where the "entropy cost" of adhering to a given meter and rhyme scheme can be calculated rather accurately...The probabilistic approach which is natural in the theory of information theory over communication channels carrying "bulk" information consisting of a large number of unrelated or weakly related messages obeying definite probabilistic laws. In this type of problem there is harmless and (in applied work) deep-rooted tendency to mix up probabilities and frequencies within a sufficiently long time sequence (which is rigorously justified if it is assumed that "mixing" is sufficiently rapid)...If something goes wrong here, the problem lies in the vagueness of our ideas of the relationship between mathematical probability theory and real random events in general...But what real meaning is there, for example, in asking how much information is contained in "War and Peace"? Is it reasonable to include  this novel in the set of "possible novels", or even to postulate some probability distribution for this set?"

\leftskip0cm\relax
\rightskip0cm\relax
\vspace{0.5cm}

 How, for instance, could we recount or otherwise portray literally the complexity of an measurable object by the length of it's shortest description, when it is seemingly just a random and long sequence? For example  for a string $y$: \\
\newcounter{theo}[section]\setcounter{theo}{0}
\renewcommand{\thetheo}{\arabic{section}.\arabic{theo}}
\begin{mdframed}[tikzsetting={draw=blue,dashed,line width=0.25pt,dash pattern = on 10pt off 3pt},linecolor=white,backgroundcolor=gray!18,outerlinewidth=1pt]
\centering{	"001001001001001001001001001001001001001001001001001001"}
\end{mdframed} it is quantified in its \textit{simplicty} by a short description as "18 repetitions of '001'", whereas:
\begin{mdframed}[tikzsetting={draw=blue,dashed,line width=0.25pt,dash pattern = on 10pt off 3pt},linecolor=white,backgroundcolor=gray!18,outerlinewidth=1pt]
\centering{	"001101001001111001101001001001001001001001011011001111"}
	\end{mdframed} would on cursory glance have no {simple} description (it actually has an intuitive description) and may be best described through \textit{complexity},  by writing-out the string itself. We will revisit string $y$ later. Begun by von Mises (reference), Wald and Alonzo Church (reference), \textit{axioms of  randomness as unpredictability}  analysed this issue in detail with various degrees of success; to formalize the notion of true randomness of strings, that one string could be more random than another string, in their mathematical philosophy. Von Mises consideration suggested that any absolute definition of probability depends on obtaining an absolute definition of a random sequence; this was  a ‘frequentist’ approach to randomness — von Mises was a developer of frequency theory (reference Paul Vitanyi, Randomness, 2001).  When von Mises’ considered randomness his objective was to develop applications to the tangible phenomena, Andrei  Kolmogorov’s (1903—1987) classic 1933 treatment developed his
 axiomatic theory of probability based purely of a set theoretic axiomatic basis; at the time this notion was revolutionary amongst mathematicians in Europe.

  Algorithmic information theory or 'Kolmogorov complexity', was borne out of von Mises notion of a random sequence; i.e. that any universal description of an individual finite object is based on a  description of its complexity, \emph{a priori} probabilities and the notion of randomness (or entropy). Kolmogorov's complexity measure is defined therefore by this notion of an absolute length of the shortest computer routine in bits that might objectively reconstruct an individual object\footnote{Although there are many optimal algorithms for reconstructing an object, their corresponding complexities differ typically by a constant; under such a notion a finite object is termed random if there is no less complex algorithm than  itself. Final  note that for the above scenario Shannon-Weaver presents a description for an information theoretic bounds of the ensemble; it does not account for an individual object }. In it's minimalism it describes objects and human communications across a channel, the outcome is therefore measured by the shortest description  to consummate knowledge.
\vspace{0.15in}

\textbf{\emph{Definition}}: The Kolmogorov complexity $K_U[x]$ defined on a string $y$ which with respect to a computer $U$ can be defined as,
\begin{center}
	\begin{equation*}
	K_U[y|x] := \min_{p : U[p,x]=y} \{ l[p] \}\footnote{Note that $K_U[x]$  is independent of the probability mass function. By 1936, Alan Turing was interested in whether a computer might actually be able to think. Hence the \emph{Turing machine} ($T$) as an idealized computer is defined by: a finite-state machine with a finite symbol set, a unidirectional read-only program tape where it obtains instructions, a bidirectional rewritable work-tape to store intermediate processes and a unidirectional writable output tape. With each real-time wrinkle the Turing machine ($T$) reads an input $p$, writes to an output tape $x$, updates the state machine and then reverts back to a program-tape such that: $\leftrightarrow T[p] = x$. Where a Turing machine reads the program tape once without restart, $\mathbb{P}$ its programs are 'prefix-free' and $p:T[p]=x$ is a 'prefix code'. Note, no program leading to a halting computation can be the prefix of another such program. The restriction leading to prefix-free programs allows for a theory of Kolmogorov complexity to be \underline{analogous} to classical information theory; i.e. Kolmogorov's complexity: an indiscriminate or non-specific objects was a shortest definition, cyclical patterns were of low Kolmogorov complexity as they might be defined by a template and a high-entropy event in the Shannon context would be equivalent to it's Kolmogorov complexity. Thus, the shortest computer routine acts as a universal code and is uniformly independent of a random variable. We might therefore consider the complexity of an object as $K[x]$ where $log 1/p[x]$ given a probability mass function $p[x]$ is the number of bits required to describe $x$ in Shannon's thereom. Kolmogorov Complexity is incomputable. \\
		It can be shown that the definition of $K_U[x]$ is independent of the choice of $U$ as $K[x]$ changes at most by an additive constant which is independent of $y$}
	\end{equation*}
\end{center}
where $l[p]$ is the minimum length of the shortest computer routine that outputs $y$ before termination on a universal prefix [Church-Turing] machine U with input $x$  (Li and Vitanyi 1997, reference Alonzo Church).


The complexity therefore of a finite object  was defined as the length of the smallest description or otherwise an algorithm that might be used to  reconstruct the object. Kolmogorov complexity axiomized the notion of simplicity and complexity of  strings; a string $y$ should be considered 'simple' if it could be described deterministically e.g. "a sequence of fifty-four ones". It would be considered 'complex' if it were say random and nondeterministic, thereby requiring a description for each bit independently. Kolmogorov defined $p$ as the shortest program that outputs $y$, where a program is executed on some fixed reference universal [Turing] computer $U$\footnote{The dawn of the digital computer age in the later part of the 1950's placed greater emphasis on computing algorithms and the notion of complexity first proposed by Kolmogorov. In 1960, R. Solomonoff of Cambridge Massachusetts research generated similar findings to that of Kolmogorov. Between 1965 - 1969, Gregory Chaitin also formulated a set of similar ideas on the notion of complexity which he published in where he formally defined the concept of Kolmogorov complexity and [where in 1969 he also extends Shannon's non-invariant notion of state-symbol measure for the complexity of Turing machines]. The term 'Kolmogorov' complexity appears to have stuck.}. 
\vspace{.1in}


\newpage
\vfill
\begin{center}    
	\includegraphics [width=5 in] {kolmogorov.eps}\\
	{Andrei Nikolaevich Kolmogorov}
\end{center}
\vfill
\newpage


%%[[In the post-war period]] Kolmogorov's research considered [turbulence]; his %%research from that era considered ergodic theory, function theory, information %%theory, the theory of algorithms and classical mechanics. [[ He managed to %%find links between totally unconnected fields, and published a small number of %%papers, but quite fundamental ones, on each topic. ]] 







\newpage

\textbf{John von Neumann (1903 - 1957)}: the pre-eminent twentieth-century polymath  and pioneering digital computer scientist was birthed of Hungarian provenance. Born into a wealthy non-practising Jewish community of early-20th century Budapest, John  von Neumann's father Miksa was a respected banker, was eligible for and subsequently acquired in 1913 the aristocratic appellation "von"; he was known for his contributions to the burgeoning Hungarian state and his vast economic resources. As an adult, John would assume the appellation "von" and thence became known as "von Neumann"; his name was often innovatory mispronounced with many errors.  


Identified young as having prodigious talent, he purportedly mastered integral calculus by the age of eight and could speak five languages with ease. He graduated school with his age peers, his teachers and parents nurtured his talents by providing additional tuition in the areas considered his strongest at the end of each day before going back to school. He studied at the University of Berlin in 1921, completing a two-year degree in chemistry and later completed another degree in chemical engineering; somewhere during this time he published two mathematical letters  By 1926, he had graduated from ETH Zurich and soon after received his PhD degree from Pázmány Péter Catholic University; he had achieved much early through a personal substantial investment. His dissertation was an axiomatization of Cantor's set theory having received considerable acclaim that year. In 1926, von Neumann commenced postdoctoral work under David Hilbert of the University of Göttingen. John von Neumann  would later relocate to Germany before eventually joining Princeton University, Princeton, U.S.A. It was during his time at Germany in 1931, that Kurt Godel's incompleteness theorem defeated the goal of axiomatizing mathematics; the significance of which was grasped immediately by John von Neumann and his supervisor David Hilbert. Initially he taught quantum theory but it was later as a member in the Institute of Advanced Study  that  he flourished in the numerous research areas he touched. He joined the Manhattan project whilst teaching at Princeton in the 1940's; his work on calculating implosion designs and flight path analysis lead him to first implement a universal machines; he would for the rest of his career focus on universal machines with intense focus. Work on the Manhattan Project in the 1940's for most of its scientists yielded an eclectic set of research developments, unique breakthroughs and new hobbies. Von Neumann during that period was also credited as pioneering the application of modelling and mathematical methods, with varied results across different fields, many of which he calculated using computers. By 1945, von Neumann had proposed his own computing architecture which presented the fundamentals of modern electronic digital computers, which came to be known as the von Neumann architecture.

Despite his reputation as an aggressive yet unremarkable driver, von Neumann developed a love for driving and occasionally did so whilst reading handbooks; understandably, he was involved in road accidents and more run-ins with the police than he might have wished. Von Neumann had a taste for high-priced clothes, hard-liquor and racy jokes. To von Neumann, Soiree's and raucous nightlife which contributed to a reputation of anomie, appealed to his good side. Whilst teaching in Konigsberg Germany around 1937, von Neumann became a habitué of Berlin Cabaret scene; later, in Princeton von Neumann and his second wife Klara were also to become famous for their active nightlife\footnote{In \textit{Theory of Parlor Games}, which was likely impelled by a peppy night-life, Von Neumann first considered what was to become Game Theory and proved the now-famous Minimax Theorem decision rule used to minimize worse-case losses, used by Artificial Intelligence researchers.}. These parties at the von Neumann’s house were regular, boisterous and extended; often disturbing their neighbours including a certain Prof Einstein.

Despite von Neumann developing Game Theory as analytic tools to observe social interactions within the field of economics, his interest were to be piqued by potential applications to politics and warfare: this at least in part stemmed from a favourite childhood pass-time \textit{Kriegspiel}, a chess-like military game. During the cold war, John Von Neumann used the axioms with Game Theory  using also the machines he had developed, to model cold war interactions between the U.S. and the USSR, viewing them as two players in a zero-sum game. Whilst examining the U.S. Army's defence computers during World War II, that Von Neumann first developed his ideas for a better computer based off of stored algorithms, using has considerable abilities to improve the computer's logic design. And so, exceedingly near the end of his career unsurprisingly John von Neumann's research was to drift greatly into military warfare where his many an early suppositions such as game theory were seen to be playing out in America's dealings with Russia during the cold war, across embarrassingly many tense and stupefyingly orchestrated scenarios.  By the end of the war, the U.S. Navy and other government agencies were to provided funds to develop  Von Neumann's machine, which allowed him to pursue many an involved research pursuit. These contributions were viewed against his body of earlier contributions as having limited scientific potential, received with critical opprobrium and viewed with mostly otherwise vague apprehension.

\newpage
\vfill
\begin{center}    
	\includegraphics [width=5 in] {vonneumann_4.eps}\\
	{John von Neumann}
\end{center}
\vfill
\newpage


\textbf{Kurt Friedrich Gödel (1906 - 1978)}:  known most notably for his \textit{Incompleteness Theorem}, considered amongst a handful of landmark theorems in twentieth century mathematics, his work did however touch every field of mathematical logic and in most cases instigated much of the fields future research. Kurt Gödel was born in the Czech Republic city of Brunn in what was then part of the Austro-Hungarian Empire, to Rudolf a businessman and Marianne. A major textile centre, Gödel had moved to Brünn from Austria to be part of its industry. His mother's family partook in the burgeoning textiles industry; from the German Rhineland they too took the road to Brunn.  He understood the importance of education; Gödel demonstrated considerable ability in learning languages and religious studies as a child.  Enrolling at the University of Vienna, he undertook a course in physics - despite a childhood plagued with illness he choose to study away from his family in Brunn. Physics was, of course, a logical area of study for a confident, naturally gifted child, but his was a merchant family in industrial town. Early in his university career, Kurt Gödel was to grow enamoured with mathematics after a course in number theory taught by Philip Furtwängler\textemdash he switched to mathematics; he graduated with a D.Phil in mathematics at the age of 23.


%%https://www.lesswrong.com/posts/GZjGtd35vhCnzSQKy/godel-s-completeness-and-incompleteness-theorems

Acknowledged for his two thereoms on the incompleteness of formal systems of mathematics, he helped the field of mathematical logic to advance past David Hilbert's and Wilhelm Ackermann's classical proof system; when his note in 1931 was newly published. It had been easy for Ausgustus De Morgan, David Hilbert  , Alfred North Whitehead and a sibylline Bertrand Russell \st{had by then established} to establish in some form or another (or at least they thought so) a set of axioms as a universal fulcrum of sorts upon which all mathematics could be derived, if not otherwise depended upon; any other notion of an axiom was deemed literally much more difficult by others. \footnote{The question Hilbert and Ackermann pose is whether a certain explicitly given axiom system for the first order predicate calculus “…is complete in the sense that from it all logical formulas that are correct for each domain of individuals can be derived…” (van Heijenoort 1967, p. 48); i.e. that every valid logical expression is provable. Equivalently, every logical expression is either satisfiable or refutable.}.  The "Second Problem" of Hilbert's detailed that what mathematical logic therefore requried was, "a direct consistency proof of analysis, i.e., one not based on reduction to another theory” (Zach (2015) ”Hilbert’s Program” §1.1 emphasis added); this served as the basis for much of Hilbert program. 

Kurt Gödel's \textit{First Incompleteness Theorem} accounts by \textit{argumentum ad absurdum} {the Completeness Theorem}, to prove that any arithmetic statement  is neither provable nor necessarily refutable in [Peano] arithmetic, although it would still hold true in the standard academic model . \textit{The Second Incompleteness Theorem} accounts that the very consistency, for example, of []Peano] arithmetic cannot be proved in [Peano] arithmetic in itself: on it's own an insurmountable truth. Kurt Gödel's theorems exhibited the infeasibility of the 'Hilbert program's' erstwhile desiderata, when conditioned on consistency and completeness. Gödel's proof demonstrated, that given a complete body of mathematical logic, much ambiguity would still exist! His entire proof, showed that for a consistent arithmetic system of whole numbers, there were no exhaustive set of axioms capable of enumerating the arithmetic as such and therefore all inquiry in arithmetic may not necessarily be stated or otherwise drawn-up using a finite subset of the axioms; i.e. not all verifiable statements within the arithmetic system, may be shown to be in fact verifiable (\textemdash in so many words don't depend on it!) John von Neumann, on reading Gödel's \textit{On Formally Undecidable Propositions of Principia Mathematica and Related Systems} thus wrote:\footnote{This  'Hilbert program', formulation by German mathematician David Hilbert in the early part of the 20th century, as  axiomatic a proposal as it was it did provide a solution to what was at the time a crisis of mathematics; early systems by mathematicians had been found to lack independence, completeness, and the consistency.}



\vspace{0.25cm}
\leftskip1.5cm\relax
\rightskip1.5cm\relax

"Thus today I am of the opinion that Gödel has \emph{shown} the \emph{unrealizability} of Hilbert's program...Therefore I consider the state of the foundational discussion in Königsberg to be outdated, for Gödel's fundamental discoveries have brought the question to a completely different level\footnote{von Neumann, John, 2005, John von Neumann: Selected Letters (History of Mathematics, Volume 27), foreword by P. Lax, introduction by Marina von Neumann Whitman, preface and introductory comments by Miklós Rédei (ed.), Providence, RI: American Mathematical Society.}...Thus, I think that [Gödel's] result has \emph{solved negatively the foundational question} \textemdash there is no [longer a] rigorous \emph{justification} for classical mathematics."\footnote{(Gödel 2003b, p. 339)}

\leftskip0cm\relax
\rightskip0cm\relax
\vspace{0.25cm}

The 1930's were as a period  considerably productive for Kurt Gödel. Based on the reception to his incompleteness theorom, he would be granted a \textit{Habilitation} in 1932 and later a \textit{Privatdozentur} (an appointment to lecture) at the University of Vienna in 1933. By the end of that decade the polymath had published papers on intuition free logic, arithmetic, predicate calculus, differential geometry and projective geometry; in it's wake much of what had been upheld as sacrosanct, had been undone. \\


\vfill
\begin{center}    
	\includegraphics [width=5 in] {godel_image.eps}\\
	{Kurt Friedrich Gödel}
\end{center}
\vfill
\newpage






\textbf{Claude Elwood Shannon (1916 - 2001)}: born  1916 in Petoskey, Michigan in the United States of America; Shannon's father was a businessman and his mother a teacher. Even as a child Shannon demonstrated an innovative light-hearted bent. Uniquely capable of  developing his ideas into innovations, among his inventive endeavors during his Gaylord Michigan childhood, he developed an extempore elevator system and a barbed-wire telegraph; both designs were of merit. Innovation ran through his family; his grandfather David D. Shannon was awarded on July 16th, 1889 - U.S. Patent 407,130: for, "new and useful improvements in Washing Machines." [reference, United States Patent Office] In a university application to the University of Michigan, Claude cites having earned pin-money during high-school:\\

\vspace{0.00in}
\centerline{"\emph{...peddeling [sic] papers and delivering [sic] telegrams.}"}
\vspace{0.100in} He graduated from the University of Michigan at the age of 20, but had already decided to return to university. By that time, Shannon had completed dual degrees from the University of Michigan in electrical engineering and mathematics\footnote{his choice of a dual-degree was apparently due to indecision between  mathematics or engineering}; eager enough have to relocate particularly after spending a summer at MIT.  Shortly after graduation he sought and obtained a research assistantship with Vannever Bush, developing an early computing device; effectively immediately returning to school. Mentored by Bush at the Massachusetts Institute of Technology, their efforts in developing  an analogue device capable of solving differential equations was considered considered essential as there were no well established theories in the field. Later, in his early 30's, during an exchange with Vannevar Bush he first touched upon a quandary that would develop into a masters' thesis which helped bridge the gap between complex theory and engineering implementation . In, \emph{A Symbolic Analysis Of Relay And Switching Circuits}, Claude Shannon developed one of the earliest abstractions of electrons in circuits of switches and relays to the algebra of decision logic of George Boole; thus one of the most fundamental aspects of modern computers, the binary logic of "0" and "1" was born in this study.  Undoubtedly his greatest service to mankinds' development, {information theory} which he created in 1945 at Bell Labs, Claude Shannon proved information as electrons could be measured and objectively controlled\footnote{However, he first published his findings in 1948 as two-part paper in the Bell Labs journal titled,  \emph{The Mathematical Theory of Communication.} [reference]. The paper would define a "bit". }; that, inspired much of the digital age and was his most lasting gift. Enraptured with his initial efforts, Claude 
Shannon would encounter and often work with Turing, von Neumann and Bush to push the bounds of modern communication; as was the camaraderie of the age\footnote{Turing,  considered  the father of  computer science and artificial intelligence (AI), first  
 first met Turing in 1942 when Alan was part of a British government tour of America's military efforts. Over  cups of tea, remembered Shannon: "we would talk about mathematical subjects." Both men thought deeply about thinking machines: "the notion of building computers that will think and what you can do with computers and all that."  In 1950, Alan  Turing published a paper on Artificial Intelligence: "Can Machines Think?"; that same year Shannon visited Turing in Manchester that year, and was impressed to see that Turing was still working on understanding what went on inside computers.}.
    
Over a 50-year career, Shannon developed a curious yet focused approach to his research helping him to generate many clear and insightful findings often discounting current research trends; for example he was purported to spend extended periods researching in white-washed rooms where he was was his most productive research wise. Later Shannon famously built a motorized mouse capable of maneuvering a maze puzzle to find  mechanical cheese; a ground braking innovation in theoritical computing. A mouse fought (a mechanical one at that) its way  around  controlled by [circuitry], moving down a maze until it reached a barrier, promptly then moving another direction in the maze or back up again. One of the earliest developments in artificial intelligence, was a computing device developed by Shannon that applied various strategies to play chess. His device lit-up the location of each subsequent move and in so doing would allow a human to play against a computer opponent. In \emph{Programming a Computer for Playing Chess} he first postulates about a world where AI software would be intelligent  enough to a beat human champion\footnote{Shannon surmised at the beginning of the paper: "This paper is concerned with the problem of constructing a computing routine or "program" for a modern general purpose computer which will enable it to play chess. Although perhaps of no practical importance, the question is of theoretical interest, and it is hoped that a satisfactory solution of this problem will act as a wedge in attacking other problems of a similar nature and of greater significance."}. Among so many of Claude Shannon's contributions as a novitiate \st{tinker} and polymath, he also developed rocket powered frisbees and a fleet of customized unicycles.



\vfill
\begin{center}    
	%%\includegraphics [width=5 in] {claude-shannon1.eps}\\
	{Claude Shannon}
\end{center}
\vfill

\newpage



SCRAP the below




[[If you say why not bomb them tomorrow, I say why not today? If you say today at 5 o'clock, I say why not one o'clock?"]]

[[In 1943, Von Neumann was invited to work on the Manhattan Project. Von Neumann did crucial calculations on the implosion design of the atomic bomb, allowing for a more efficient, and more deadly, weapon. Von Neumann's mathematical models were also used to plan out the path the bombers carrying the bombs would take to minimize their chances of being shot down. The mathematician helped select the location in Japan to bomb. Among the potential targets he examined was Kyoto, Yokohama, and Kokura."Of all of Von Neumann's postwar work, his development of the digital computer looms the largest today." (Poundstone 76) ]


[[ private, public and defense contractors ]]

[[ On his deathbed, he reportedly entertained his brother by reciting the first few lines of each page from Goethe’s Faust, word-for-word, by heart]]

[[An occasional heavy drinker, Von Neumann was an aggressive and reckless driver, supposedly totaling a car every year or so]]

[[From the outset, Von Neumann knew that game theory would prove invaluable to economists. He teamed up with Oskar Morgenstern, an Austrian economist at Princeton, to develop his theory.]]


[[Von Neumann came, looked at the schematics, walked around the dynamo, then took out a pencil. He marked a line on the outside casing and said, “If you’ll go in and cut the coil here, the dynamo will work fine.”

They cut the coil, and the dynamo did work fine. Ford then told von Neumann to send him a bill for the work. Von Neumann sent Ford a bill for \$5,000. Ford was astounded — \$5,000 was a lot in the 1940s — and asked von Neumann for an itemised account. Here’s what he submitted:

Drawing a line with the pencil: \$1

Knowing where to draw the line with the pencil: \$4,999]]


Von Neumann research style tended to be




[[Johnny, apparently in agreement with his father, decided initially to pursue a career in chemical engineering. As he didn’t have any knowledge of chemistry, it was arranged that he could take a two-year non-degree course in chemistry at the University of Berlin.]]



[[Like most aspiring mathematicians in Europe at the time, von Neumann worked on mathematical formalization problems [of Hilbert], [[and for example wrote papers aimed at finding a proof of consistency for the axioms of arithmetic]]].

[[on Hilbert spaces]]

[[showing that quantum mechanics was profoundly different from all previously known theories in physics.]]

[[Kolmogorov]]\\

At this time he also started to work on the theory of automata and the theory of algorithms. Together with his pupil Uspenskii he formulated the important notion of Kolmogorov-Uspenskii machine. He supported the up and coming field of cybernetics (theory of computation) against heavy initial antagonism (in the USSR). Many USSR computer scientists are K.'s pupils or pupils of K.'s pupils.
The second period from 1955-1959 consisted in applications from information theory to the ergodic theory of dynamical systems. He introduced the fruitful idea of informational (entropic) characteristics in the study of metric spaces and of dynamical systems.

It is seductive to define an random infinite sequence as one of which the growth of complexity if the initial segments with the length is sufficiently fast, thus relating to von Mises' earlier approach. Due to unavoidable oscillations of the complexity of prefixes as function of their length this did not work out. However, P. Martin-Loef, a Swedish mathematician visiting K. in Moscow in 1964-1965, was able to show that under appropriate axiomatic definitions of randomness, one can prove once and for all that the thus defined sequences satisfy all effective tests for randomness, and have measure one in the set of all such infinite sequences. This rigorously defined an appropriate class, intuitively satisfactory as well, to qualify as von Mises' Kollektivs. Later it was shown by L.A. Levin, P. Gacs and G.J. Chaitin that one can refine the notion of complexity by defining it relative to a set of admissible descriptions.


The day-to-day life, birth to death. The comings, the goings, the relatives, the friends, the interaction with the historic events of his day, where and how Gödel lived. His appointments, honours. The stuff of Who's Who.
Gödel's accomplishments in logics, set theory, and cosmology. Their background, their evolution, and their post-Gödelian sequels.
The Institute for Advanced Study during the first quarter century of its existence and the interaction between Gödel and Einstein and other luminaries in residence.
Gödel's philosophical and theological views.
Gödel as a case study for Oliver Sacks. The role of mental instability, paranoia, hypochondria, depression in creativity. The old idea (and indeed the frequent observation of the general public) of the close relationship between genius and mental instability.
The important role played by Adele Porkert Gödel in keeping her husband alive and on track.



[[Gödel showed that this was not the case. Ambiguity is possible. The views of Russell and Whitehead and of many others lay shattered. Initially Gödel's theorems were misunderstood and considered bewildering by such logicians and philosophers as Zermelo and Wittgenstein. Russell (Literature Prize Nobelist), who had early on turned from mathematics to social activism was "glad he was no longer working at mathematical logic."]]


[[Gödel's Incompleteness Theorem came in 1931, and the shock waves left in its wake can still be felt in some quarters. Mathematical logic had been developing since the mid-1800's. De Morgan, Boole, Frege are some of the early names. By the time of the publication of their Principia Mathematica, (1910-1913), Bertrand Russell and Alfred North Whitehead "saw logic as a universal system within which all mathematics could be derived." Mathematicians, joining their opinion to that of David Hilbert, had gone so far as to assert that all sensible mathematical claims could either be proved or disproved.]]

[[Less well known are his contributions to set theory, even among the mathematical community, which nonetheless may be ready to say that Gödel was the most original, most insightful mathematical mind of the 20th century, the greatest logician since Aristotle. His cosmological discoveries ("Gödel's rotating worlds" based on the Einstein field equations), I gather, are not much studied, and his philosophical and theological ideas, along with the facts of his personal life, have been diffused largely through community gossip.]]

[[The 1930s were a prodigious decade for Gödel. After publishing his 1929 dissertation in 1930, he published his groundbreaking incompleteness theorems in 1931, on the basis of which he was granted his Habilitation in 1932 and a Privatdozentur at the University of Vienna in 1933.]]

[[Among his mathematical achievements at the decade's close is the proof of the consistency of both the Axiom of Choice and Cantor's Continuum Hypothesis with the Zermelo-Fraenkel axioms for set theory, obtained in 1935 and 1937, respectively. Gödel also published a number of significant papers on modal and intuitionist logic and arithmetic during this period, principal among which is his “On intuitionistic arithmetic and number theory,” (Gödel 1933e), in which he showed that classical first order arithmetic is interpretable in Heyting arithmetic by a simple translation. Other publications of the 1930s include those on the decision problem for the predicate calculus, on the length of proofs, and on differential and projective geometry.]]


if it was not in most cases their original stimulus.


[[was one of the principal founders of the modern, metamathematical era in mathematical logic. He is widely known for his Incompleteness Theorems, which are among the handful of landmark theorems in twentieth century mathematics, but his work touched every field of mathematical logic, if it was not in most cases their original stimulus.]] Godel was born in the Czech Republic city of Brunn in what was then the Austro-Hungarian Empire, to Rudolf a businessman and Marianne.

 \end{document} 