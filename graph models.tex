	\documentclass[7pt]{article}
	\usepackage[framemethod=TikZ]{mdframed}[2013/07/01]
	\usepackage[T1]{fontenc}
	\usepackage{framed}
	\usepackage{physics}
	\usepackage{subcaption}
	\usepackage{amssymb}
	\usepackage{soul}
	\usepackage{changepage}
	\usepackage{pdflscape}
	\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
	\usepackage[utf8]{inputenc}
	\usepackage{setspace}
	\usepackage{amsmath,amsthm,amscd}
	\usepackage[colorlinks]{hyperref}
	\usepackage{setspace}
	\usepackage{xcolor}
	\usepackage{xparse}
	\usepackage{soul}
	\usepackage{caption}
	\usepackage{blindtext}
	\usepackage{enumitem}
	\usepackage{lscape}
	\usepackage{lipsum}
	\usepackage{tcolorbox}
	\usepackage{lipsum}% http://ctan.org/pkg/lipsum
	\usepackage{graphicx}
	\usepackage{wrapfig}
	\usepackage{lscape}
	\usepackage{rotating}
	\usepackage{epstopdf}
%	\usepackage{algorithm}
	\usepackage[noend]{algpseudocode}
	\usepackage{pdflscape}
	\usepackage[T1]{fontenc}
	\usepackage[english]{babel}
	\usepackage{physics}
	\usepackage{amssymb}
	\usepackage{lipsum}
	\usepackage{cancel}
	\newlength{\rulethickness}
	\setlength{\rulethickness}{1.2pt}
	\newlength{\rulelength}
	\setlength{\rulelength}{15cm}
	\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
	\usepackage[utf8]{inputenc}
	\usepackage{setspace}
	\usepackage{amsmath,amsthm,amscd}
	\usepackage[colorlinks]{hyperref}
	\usepackage{setspace}
	\usepackage{listings}
	\usepackage{xcolor}
	\usepackage{xparse}
	\usepackage{soul}
	\usepackage{amsfonts}
	\usepackage{booktabs}
	\usepackage{siunitx}
	
	

	\usepackage[linesnumbered,ruled]{algorithm2e}
	
	\linespread{1.25}
	\NewDocumentCommand{\inn}{mo}{%
		\langle #1\rangle
		\IfValueT{#2}{^{}_{\mspace{-3mu}#2}}%
	}
	\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
	\newcommand{\inperp}{\rotatebox[origin=c]{90}{$\models$}}
	
	\newenvironment{itquote}
	{\begin{quote}\itshape}
		{\end{quote}\ignorespacesafterend}
	\newenvironment{itpars}
	{\par\itshape}
	{\par}
	
\begin{document}
	
\tableofcontents
\newpage
\part{Equations}

\section{Introduction.}
\st{Probabilistic models in pattern recognition are used to conjecture on the values of one or more variables, given observations of  another variable.}
Our current chapter, \st{we initially} considers \st{provide} an overview of the branch of mathematics termed \emph{probability theory}, \st{which provides} providing \st{the} tools for the semantics of representation, \st{of estimation and} inference and \st{structured} learning which underpin \st{the} families of the most common graphical models\st{which are termed Markovian Networks and Bayesian Networks}. 
\\
 %%\textbf{Probabilistic Graphical Models - Chapter 1.2.2}

\section{Knowledge representation.}
The main focus of {probabilistic graphical modelling} is to develop a \emph{declarative representation} or \emph{model-based methodology} for high-dimensional network structure, wherein a system model is obtained through a \st{a based on rapid} a compact and principled rationalisation in our models. A {declarative probabilistic representation} provides  a \st{more} general-purpose, developmental algorithm based framework with a standard formalism to rationalize \st{a system} internal processes, this allows its application across varied modelling domains.  \st{independent of the underlying model domains; this allows it's application}. Conditioned on the \st{a corresponding} model domain \st{allows us to} we may query the system with any number of possible event scenarios or where model is misspecified we receive a null response on the other hand. For example for given a distribution \emph{P}, an algorithm may query whether "Will \emph{P} satisfy $(A,B \inperp  C | D)?$"; we receive $"1/0"$ as the feedback. 

\noindent\rule{7cm}{1.7pt}\\
\begin{itquote}
{We note therefore three properties for probabilistic declarative representations:}
\begin{enumerate}
	\item They provide us with a representation that is independent of the reasoning which we apply.  Once our system is modelled we may \st{therefore} modify a \st{the} modelling domain without a need to edit the underlying declarative rationalization.
	\item Ensure any errors we obtain during model development will always be independent of our semantic formalism - i.e. we may consider semantic inaccuracies and any modelling inaccuracies, separately.  
	\item Achieve a decoupling of knowledge and reasoning within a computer \st{our network structure}; for this \emph{model}, its final representation will allow therefore for a coherent semantic framework independent of a later specific implementation or algorithm, which we adopt. 
\end{enumerate}
\end{itquote}
\noindent\rule{7cm}{1.7pt}\\
Succinctly, declarative representations work by encapsulating the set of outcomes within the model domain which are most \emph{probable}. These model domains will require one or more  \emph{random variables} in a set of elements or  space $\chi$  to assess the posterior distribution based on a \emph{joint distributions} which encodes or factorizes the full the dynamics of any complex domain; our final model therefore limits our consideration to \st{framework here will consider primarily} the uncertainty of internal processes - uncertainty in the underlying state of the system and the uncertainty due to noise. In this calculus of uncertainty or \emph{probability theory}, provides us with a framework for modelling the possible scenarios of a modelling domain and the likelihoods functions between variables. For example  \st{when we consider} given the random variables $\chi = \{X,Y, Z\}$, we can consider the interaction of their respective observations $x_i$, $y_i$ and $z_i$ on the system dynamics.  
 
 
 %Within this \st{Within a} declarative representation framework, we achieve  separation between knowledge and reasoning (reference); %once modelled we may \st{therefore} modify a \st{the} modelling domain without a need to edit the underlying declarative %rationalization.

\subsection{Probability.}


%%this would 
%%well defined global average temperature measurement for the year 2025 could define a futures
%%market where the global warmers and global coolers could settle their differences (actually,
%%contracts of this kind but with near settlements are traded in the form of binary bets, which are
%%described below).

%For example, our \st{algorithm within a} graphical model representation might allow us to delve \st{might detail} into how specific %features interact probabilistically and thereby affect the final output. \\% Create a new 1st level heading
%%\subsection{Declarative representation.}
{Probabilistic models} in pattern recognition allows us to model likelihoods and thereby uncertainty, thereby making sense of their scenarios. These models assume that although internal \st{variables} processes may be modelled with relative accuracy, other properties must be considered in a model to account for \st{the} variableness or irregularity in our observations of these processes; this irregularity is represented therefore by a \emph{probability distribution} - be it a distribution that is parametrized (e.g. variance and mean), where the underlying distribution is discretely approximated or randomly sampled. Formally, we define events by assuming that there is a known space of outcomes, this outcome space which we define as $\chi$. For example, if we consider the suit of hearts in standard deck of playing cards, we would define $\chi = \{Ace, King, Queen, Jack, 10, 9, 8, 7, 6, 5, 4, 3, 2\}$. In the instance where an individual is playing a game of poker with all four suit, their initial hand or the space would consider all possible orders of card pairs, an expanded space, despite an individual being dealt face down just two cards. Further, we assume a set of measurable events $X$ from which probabilities are assigned.  Therefore, each event $x \in Val[X]$ is a subset of $\chi$. In the poker example, 
the event $\{SA, C2 \}$ represents Ace of Spades and 2 of Hearts for an initial hand, and $\{C8\}$ represents an 8 of Clubs where a subsequent card draw this is a possible outcome.  Therefore for a probability distribution $P$ over some $[\chi,S]$, we define this as the mapping from events in $S$ to real values such that: 1) $P[X = x] \geq 0$ for all values $x \in \text{Val}[X]$, 2) $\sum_{i=1}^N P[X = x_i ] = 1$ probability and, 3) if $x_1, x_2 \in X$ and $x_1 \cap x_2 = \varnothing$, then $P[x_1 \cup x_2] = P[x_1] + P[x_2] $.

%%https://www.math.uni-hamburg.de/home/erde/Entropy/Discrete%20Entropy.pdf
\subsubsection{Probabilistic models.}
The calculus of \st{uncertainty or} \emph{probability theory} allows us to identify an exhaustive set of events and measure what their likelihoods have been\st{, between '1' and '0', of these events.}. These events are assigned a probability between 1 and 0 which denotes \st{events respectively which are} probable or events otherwise uncertain. For example, if we let $\chi$ \st{therefore} be a finite set, \st{such that} we define $\chi^*$ formally as \st{denotes} the set of finite sequences \st{of} obtained by concatenating zero or more strings from $\chi$ \st{as} 
\begin{equation}
\bigr\{ \{0,1\}^* \bigl\} = \{\phi,0,1,00,01...,000,...\} 
\end{equation}
where the concatenation of one string is the string itself and $\phi$ denotes the concatenation of zero strings, i.e. an \emph{empty word}; or in a generalized form, %%https://www3.cs.stonybrook.edu/~cse303/lecture4.pdf
\begin{equation}
\begin{split}
\chi^* & = \bigr\{ X_1, X_2, ... , X_k : k \geq 0, X_i \in \chi, i = 1,2,...,k \bigl\}  \\
& = \large\bigcup_{k\geq0} \bigr\{ X_1, X_2, ... , X_k : X_1, X_2, ... , X_k \in \chi \bigl\} 
\end{split}
\end{equation}
Formally, we define $X_i$  \st{to refer to the}  as a random variable that takes values in $\chi$ with a corresponding probability distribution $P$, that is, $\chi = \{X_0,…,X_i, … X_{n-1}\}$. Therefore  each random variable $X_i$ is equally likely. Let $P$ be defined over a countable or finite set $\chi$. Categorically we can define the random variables as having a form $X_1 = x_1$ and $X_2 = x_2 ,...,$ and $X_n = x_n$  for a choice of message  $x_1, . . . , x_n$ for all variables. Thus \st{the} probability of an event $X_i$ occurs or in coding theory terms a particular message $x_i$,  is given in $P[X=x]= P[\{x\}]$. We find that, if we interpret this as given the event or words at the source $x_i \in \{0,1\}$ produced by observing random variable $X_i$ with fixed properties, then the probability of the event is given succinctly as $P[\{x\}]$. Note that for a random variable $X_i$ \st{a} its corresponding entropy is given as $\mathbf{H}[x] = \log |x|$, however when observing the instance value $x_i \subset \chi$ we obtain an \emph{information}  measure, i.e. $\mathbf{I}[X:=x] = \log |x|$ bits \st{from the original entropy measure} from the original entropy measure.  
\begin{figure*}
\centering
\begin{minipage}[t]{0.52\textwidth} 
\centering
\begin{tabular}{ c | c |c c c c||c} 
																		&				&  	\multicolumn{4}{c}{\emph{skew}} \\
P[\emph{level=high},slope,skew]						 &	 			  &  -ve  		&  	norm	  & +ve 		& & \\ 
\hline
																		& -ve	      & .020 	  & .081 & .025  &  & \\
\emph{slope}	 												& norm    	& .019  	   & .043  & .037  & &  \\
																		& +ve	   	 & .018  	 & .031   & .038  & & \\
\hline
\hline
																		& 			     &  		    & 					& 				&  & 0.312 \\
\end{tabular}
\caption{Canonical space for  $P[{level},slope,skew]$; for an options market for Soyabean oil we consider the latent factors that affect options returns namely slope, skew and with the price {level at high}.}
\label{canonical_space1}
\end{minipage}
\\
\centering
\begin{minipage}[t]{0.52\textwidth} 
\centering
\vspace{20 mm}
\begin{tabular}{ c | c |c c c c||c} 
																		&				&  	\multicolumn{4}{c}{\emph{skew}}\\
P[\emph{level=norm},slope,skew]						&	 			  &  -ve  		&  	norm	  & +ve 		& &\\ 
\hline
																		& -ve	      & .027 	  & .042  & .037  &  &\\
\emph{slope}	 												& norm    	& .231  	 & .107  & .025  & & \\
																		& +ve	   	 & .021   	  & .011   & .021  & &\\
\hline
\hline
																		& 			     &  		    & 					& 				&  & 0.522\\
\end{tabular}
\caption{Canonical space for  $P[{level},slope,skew]$; for an options market for Soyabean oil we consider the latent factors that affect options returns namely slope, skew and with the price {level at normal}.}
\label{canonical_space3}
\end{minipage}
\\
\begin{minipage}[t]{0.52\textwidth} 
\centering
\vspace{20 mm}
\begin{tabular}{ c | c |c c c c||c} 
																		&				&  	\multicolumn{4}{c}{\emph{skew}}\\
P[\emph{level=low},slope,skew]						 &	 			  &  -ve  		   &  	norm	  & +ve 		& &\\ 
\hline
																		& -ve	      & .012 	      & .004             & .012  	   	 &  &\\
\emph{slope}	 												& norm    	& .009  	    & .021             & .080 		  & & \\
																		& +ve	   	 & .011   	      & .006             & .011  		 & &\\
\hline
\hline
																		& 			     &  		       & 					& 				  &  & 0.166\\
\end{tabular}

\caption{Canonical space for  $P[{level},slope,skew]$; for an options market for Soyabean oil we consider the latent factors that affect options returns namely slope, skew and with the price {level at low}.}
\label{canonical_space}
\end{minipage}
\end{figure*}

More generally the joint distribution over \st{a set of} random variables $\chi = \{ X_0,…,X_i, …, X_{n-1} \}$ is denoted by $P [ X_1, ... , X_n ]$; this is the distribution that assigns probabilities to events of $X_i \in \chi$, formally speaking. We note that any event that therefore occurs in $\chi$, must be a union of events; we define this is our \emph{canonical space}, as it corresponds to a joint distribution over a random sample ${X_1,...X_n}$; 	we are not required to use a canonical space but we often do. We use $\varepsilon$ to denote the range of outcomes, in the original space or a canonical space that assigns a variable in $\chi$, that is, $\varepsilon \in Val[\chi]$. For example in Figure \ref{canonical_space1}-\ref{canonical_space}, we consider the market for Soyabean oil options in terms of three latent factors of the volatility surface $\chi = \{ level, slope, skew \}$; this is shown to generate joint distributions and also 27 possible atomic outcomes. 

Probabilistic models therefore conjecture on the values of one or more variables, given observations of  another variable. This simplicity in the representation of probabilistic models is however misleading, all but the most simple of probabilistic models are analytically or computationally insoluble. This resulting probability distribution in probabilistic models  expresses a belief about an external behaviour or a hypothesis of interest $P[Y]$ termed the \emph{prior distribution} \st{i.e. the probability of our current hypothesis}. Any new behaviour or beliefs $P[X]$ is then assumed subject to uncertainty expressed by a subsequent distribution termed the \emph{likelihood distribution}; the probability of observing  new behaviour given our current hypothesis, i.e. \underline{we maximize} $P[X | Y]$.  


%%\subsection{Graphical models.}
\subsection{Representation.}
A \emph{probabilistic graphical model} \st{in particular,} representation exploits the fact that the underlying random variables tend to interact with a limited number of other random variables; therefore any joint distribution that exhibits a set of features may be efficiently modelled using a decomposable graphical representation; so for every dependence structure representable by decomposable graphs we may develop \st{that there exists} a representative model within \st{in either of the families of the most common a} the common graph model \st{of} families \st{which are called} namely  \emph{Bayesian Networks}  (or \emph{Directed graphical models}) and \emph{Markovian Networks}  (or \emph{Undirected graphical models}). These models are \emph{hence} \st{therefore would attempt to capture} used to encode how \st{an algorithm} algorithms in a computer process information for a specific modelling domain; \st{This therefore will} this allows any number of algorithms within a given modelling domain to resolve specific probabilistic inference queries such as \emph{"What features are relevant in our model?", "How will limiting an input impact the performance of the model?"} and \emph{"Is the model a fair representation of our underlying network topology?"}.  For example a market for options contracts, such as \emph{Soybean Oil options}, could be defined by global average production of soybeans and the average production of castor oil for a given year; a graphical model \st{representation might} would allow us to delve \st{might detail} into how specific levels of production interact probabilistically and thereby affect a options market.

Let us first delve into probabilistic graphical models which is \st{fundamental in machine learning} integral to pattern recognition, at the very least. A Bayesian Network (BN) is a type of a probabilistic graphical model introduced by Pearl to support reasoning under uncertainty (Pearl, 1985; 1988). Probability theory provides a means to articulate and manipulate key relationships used in human reasoning: likelihood, conditionality, relevance, and causality (Pearl, 1988). Bayesian Networks combine probability theory with graph theory and causality to provide a mechanism to answer queries such as “what will be the likely effect of this intervention?” or “what factors are relevant to this effect?”. 


 A modelling formalism for Bayesian Networks follows; note that our model formalism does not consider parameters and is therefore somewhat limited.  Graph models can as a data structure consist of four components parts, 
\begin{itemize}
	\item \emph{random variables} $\chi$:  space $\chi$ with observations $\chi = \{X_0,…,X_i, … X_{n-1}\}$; $X_i$ is the subset of a random variables\footnote{We may determine the unknown structure between random variables using the Grow-Shrink Markov blanket algorithm or the test of conditional independence},
	\item \emph{nodes} or \emph{vertices} $\mathcal{V}$: given that $\mathcal{V}= \{v_0,…,v_i, … v_{n-1}\}$, we may consider the observation of the random variable and the node as interchangeable as {node} $v \in \mathcal{V}$ maps to a random variable  $\chi = \{X_0,…,X_i, … X_{n-1}\}$. From figure 1, note that when two edges converge on a hidden node, these parents are considered  \emph{marginally independent}; but where two edges pass through an observed node the parents become \emph{dependent}. Therefore for the Markov Networks child nodes would always be separate from  parents nodes; therefore in converting a Bayesian network graph to a Markov network, we must \emph{moralize} parents that share a child node or else we would infer incorrect independence statements.
	\item \emph{edges} or \emph{links} $\mathcal{E}$: given an edge $e \in \mathcal{E}$ details the dependence structure of observations in $\chi$ within the data and the exhaustive set of edge pairs $\xi$ is given by $\xi =\{X_i \rightarrow X_j, X_j \rightarrow X_i, X_i - X_j \} \hspace{0.15in} \forall X_i, X_j \in \chi$. Any denotation in the use of edges for graph models as we shall determine later differs for \emph{undirected graphs} and \emph{directed acyclic graphs} \cite{bang2008digraphs,diestel2017extremal}.  We note that,
	\begin{itemize}
		\item \st{For a set of $n$ nodes $\chi = \{X_0,…,X_i, … X_{n-1}\}$,}
		\item A directed edge $'\rightarrow'$ connects every parent node to a child node as $\{X_i \rightarrow X_j\}$; i.e. the direction denoting a parent "generating" a child. 
		\item An undirected edge $'-'$ connects two nodes, i.e. $\{X_i - X_j\}$,
		\item Two nodes connected by either undirected or directed edges  may be represented simply as the same, i.e.  $\{X_i , X_j \}$,
		\item Note therefore, we cannot have both $\{X_i \rightarrow X_j\}$ and $\{X_j \rightarrow X_i\}$ or $\{X_i \rightarrow X_j\}$ and $\{X_i - X_j^2\}$,
		
	\end{itemize}
	\item \emph{graph} $\mathcal{\kappa} = \{\mathcal{V},\mathcal{E} \}$: a  \emph{directed graph} or \emph{directed separation} (i.e. d-separation) is a graphical model where all edges which are $\{X_i \rightarrow X_j\}$ or $\{X_j \rightarrow X_j\}$ and an \emph{undirected graph} or \emph{undirected separation} (i.e. u-graph) is a graphical model that consists only of edges such as $\{X_i - X_j\}$. In the two most common types of probabilistic graphical models, Bayesian Networks utilise directed graphs whilst Markovian Networks utilise undirected graphs.  Graphical models are called \emph{simple} when there is a single edge between a pair of vertices and the model does not contain loops or edges. Where an edge is present, the vertices are called \emph{adjacent}. When all pairs of vertices in the graph are adjacent, the graph is called \emph{complete}; and a \emph{multi-graph} allows multiple edges between any pair of vertices. Note g raph models are assumed time-invariant.
	
	\item \emph{Markov blanket $\mathcal{M}$}: for a set of random variables \st{$\chi$ with} $X_i \in \chi$ and their joint probability distribution $P$, \st{then} the {Markov blanket} $\mathcal{M}$ of $X_i$ conditioned on $\mathcal{M}$ \st{are} is the set of variables that cause $X_i$ to be conditionally independent of all other variables (or nodes). This formalism \st{is defined as the} defines a statistical partitioning of a system into internal \st{states} and external states, where the blanket \st{itself consists of} is the \st{states} boundary separating the states. \st{that separate the two.}. \st{Therefore given} The {Markov blanket}  is considered as containing the set of nodes which includes all the information needed to complete an inference on $X_i$ - this inference includes prediction and hypothesis testing; all nodes outside of a Markov blanket are conditionally independent from $X_i$. Therefore in Markov Networks a {Markov blanket}  is the set of neighbours of $X_i$, (all the nodes that are connected to $X_i$ by an edge) whereas in Bayesian Networks the Markov blanket is the union of the children of $X_i$, and all their parents. Therefore any node which is absent from \st{a} the class attribute of Markov blankets has values or behaviour completely irrelevant \st{in} to a classification process. For example the Markov blanket of the {Directed graphical models} and {Undirected graphical models} are depicted in Figure \ref{markov-blankets}.
\end{itemize}

A graph $\kappa$ is termed a \emph{dependence mapping} of the distribution $P$ \st{of $\chi$} when there is a one-to-one mapping from the random variables in $\chi$ with the nodes $\mathcal{V}$ if $[\hspace{0.05in} \mathbf{A \inperp_P B \hspace{0.05in} \vert \hspace{0.05in} C \hspace{0.05in}}] \Rightarrow [\hspace{0.05in} \mathbf{A \inperp_\kappa B \hspace{0.05in} \vert \hspace{0.05in} C \hspace{0.05in}}]$. A \emph{perfect mapping} of a distribution $P$ is obtained when isomorphism exists with $\kappa$ i.e. $[\hspace{0.05in} \mathbf{A \inperp_P B \hspace{0.05in} \vert \hspace{0.05in} C \hspace{0.05in}}] \Leftrightarrow [\hspace{0.05in} \mathbf{A \inperp_\kappa B \hspace{0.05in} \vert \hspace{0.05in} C \hspace{0.05in}}]$. The graph $\kappa$ may be also  termed an \emph{independence mapping} of a distribution $P$ if $I [\kappa] \subseteq I[P]$; an independence mapping characterization denotes that the two disjoint nodes A and B separated by C in the graph model in fact correspond to independent sets of random variables. Markovian Networks and Bayesian Networks fit a more terse characterisation of a \emph{minimal Independence-map} (or minimal I-map) in that they present a graphical model that has a minimal number of edges. Note a minimal I-Map $\mathcal{G}$ of a distribution $P$ is defined by two conditions \cite{pearl1988probabilistic}: 
\begin{itemize}
	\item \emph{Markov condition}; that every variable $X$ in $\mathcal{G}$ is conditionally independent of its nondescendants given its parents $\mathbf{pre} [ X]$. This is equivalent to saying that every conditional independence implied by d-separation in the DAG is present in the joint probability distribution $P[X]$. Importantly, this condition means that the joint probability distribution $P$ can be decomposed as $P[X]  = \prod_{i=1}^{n} P [ X | \mathbf{pre} [ X] ]$. Note that $\{\mathcal{G}, P\}$ is called a Bayesian Network only when it satisfies the Markov Condition. 
	\item \emph{Minimality condition}; that we cannot remove edges at this point in the graph $\mathcal{G}$, without the graph requiring a conditional independency that would  no longer be present in $P$.
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.25\textwidth]{DAG_5_.pdf}
	\caption{A simple \st{the} fully connected DAG  obtained \st{by the} through the complete factorisation of \st{a} the probability density function  $P[X_1,X_2,X_3,X_4] = P[X_2]P[X_1|X_2]P[X_3|X_1,X_2]P[X_4|X_1,X_2]$.}
	\label{DAG-example}
\end{figure}
\subsubsection{Probabilistic graphical models}
Probabilistic graphical models may be viewed as representing \st{a representation of} the \emph{independence} of observations - $I(\mathcal{G})$; for example given observations $\mathbf{A,B,C}$ we may represent that $\mathbf{A}$ as independent of $\mathbf{B}$ given an observation of $\mathbf{C}$ as $[\hspace{0.05in} \mathbf{A \inperp B \hspace{0.05in} \vert \hspace{0.05in} C \hspace{0.05in}}]$ \cite{pearl1988probabilistic}. However in order to then reason probabilistically, we are required to model \st{of a} \emph{joint distributions} over a given space, the set of random variables $\chi = \{X_0,…,X_i, … X_{n-1}\}$. Note that an assumption of \emph{conditional independence} allows for a factorisation of the probability density function; i.e. $P[X_1,X_2,X_3] = P[X_1] P[X_2] P[X_3]$. By application of the product rule in an iterative manner, joint probability density functions may be factorized into sets of conditional probability density functions:
 \begin{equation}
 \begin{split}
 P[X] & = P[X_1] P[X_2,..,X_d|X_1] \\
 P[X] & = P[X_1] P[X_2|X_1]  P[X_2,..,X_d|X_1,X_2] \\
 P[X] & = P[X_1] P[X_2|X_1]  P[X_3|X_1,X_2]   P[X_4,..,X_d|X_1,X_2,X_3] \\
 & \vdots \\
 P[X] & = P[X_1] \prod_{i=2}^p P[X_i|X_1,...,X_{i-1}] \\
 P[X] & = \prod_{i=1}^p P[X_i|X_1,...,X_{i-1}] = \prod_{i=1}^p P[X_i|\Pi_{X_i}] \\
 %%P[X] & = \prod_{i=1}^p P[X_i|\text{pre}[x_i]] = \prod_{i=1}^p P[X_i|\Pi_{X_i}]  \\
 P[X] & = \prod_{i=1}^p P[X_i|\text{pre}[x_i]] \\
 \end{split}
 \end{equation}
 $\Pi_{X_i}$ are the \emph{ordered parents}  of node $X_i$ in a graphical model where  $\Pi_{X_i} \subseteq \{ \text{pre}[x_i]\}$; the chain rule is the special case where $\Pi_{X_i} = \text{pre}[x_i]$; this \emph{ordered Markov property}  allows for the factorisation of the joint probability distribution.  It is noted that for a given DAG $\mathcal{G}$, it's joint distribution will not necessarily equal the product of its conditional distributions, unless it meets the Markov condition. But \st{network} dependence structures with characteristics of high-dimensional distribution may be considered fixed and therefore scalable,  \st{that is its corresponding graphical model is \emph{chordal} and may be decomposed into local distributions. Alternatively probabilistic graphical models may be considered \emph{chordal}, that  structures with characteristics of a high-dimensional distribution;} i.e. a  joint-distribution $\Pi_{X_i}$ combines local distributions, where each node in the graph model maps to a single random variable $\{X_i\}$ that in itself depicts specific features; the local distribution or nodes being dependent solely on the joint-distribution $\Pi_{X_i}$. This elemental finding is the \emph{Markov property}, i.e. a parametrization of a global distribution by a set of local distributions, which holds within Bayesian Networks; if $X_i \inperp \{ \text{pre}_i \backslash \Pi_{X_i} \} | \Pi_{X_i}$, then  %%$p[X_1,...,X_d] = \prod_{i=1}^p P[X_i|\Pi_{X_i}]$ and therefore %%https://slideplayer.com/slide/4757683/
	\begin{equation}
		P[X] = \prod_{i=1}^p P[X_i|\Pi_{X_i}]
	\end{equation}
The Markov property may be ensured by constructing \emph{causal DAGs}, a DAG where there is an edge from $X$ to $Y$, such that a variation in $X$ results in a change in $P[Y]$. Similarly in a Markov Network $\mathcal{H} $, global distributions will factorize over the set of \emph{maximal cliques} $\{C_0,...,C_{n-1} \} \in \mathcal{H} $ in a parametrization, such that for a set of fully connected nodes; any factorization in terms of subgraphs can be parametrized by assigning a \emph{Gibbs potential} $\Psi_l$, the relative mass of probability for each maximal clique $C_l$.
\begin{equation}
P[X] = \prod_{i=1}^l \Psi_l[C_l]
\end{equation}
Note also there is a duality in the interpretation of graph models; i.e. the independence of observations property is what allows therefore for a parametrization using multiple random variables in the Markov property of maximal cliques. Therefore when $	P[X] = \prod_{i=1}^p P[X_i|\Pi_{X_i}]$ where $\Pi_{X_i} \subseteq \{ \text{pre}[x_i]\}$, any probabilistic model may be represented as a graph model with  random variables $X_i$ as their nodes and a set of directed edges from $X_j \in \Pi_{X_i}$ towards $X_i$; this graph model is called a \emph{directed acyclic graph} (DAG). For example from the chain rule a fully connected DAG  considered in Figure \ref{DAG-example}. \st{A graph model is always ordered such that it has a unique topology.}

	\begin{landscape}
		\begin{figure*}	
			\centering
			\begin{tabular}{ccc}
				\subcaptionbox{\label{fig:Name:a}}{\includegraphics[width=0.3\textwidth]{moralized_2.pdf}} &
				\subcaptionbox{\label{fig:Name:a}}{\includegraphics[width=0.3\textwidth]{moralized_3.pdf}} 
				
			\end{tabular}
			\vspace{0.66in}
			\caption{(a) Note that where the node $X3$ is hidden within our topology;  nodes $X1$ and $X2$ are therefore \emph{marginally independent}; (b) \st{note} the same model but where $X3$ is then removed from  our \st{the} final {Markovian Network}.}
			\label{Bayesian-Network-0}
		\end{figure*}	
		
		
		\begin{figure*}	
			\centering
			\begin{tabular}{ccc}
				\subcaptionbox{\label{fig:Name:a}}{\includegraphics[width=0.3\textwidth]{moralized_1.pdf}} &
				\subcaptionbox{\label{fig:Name:a}}{\includegraphics[width=0.3\textwidth]{moralized_4.pdf}} 
				
			\end{tabular}
			\vspace{0.66in}
			\caption{\st{For a Bayesian Network where node $X3$ is visible in our topology nodes $X1$ and $X2$, it's parents would be separated by the parent. Therefore in our the undirected graph the otherwise separated parents must be linked or \emph{moralized}. } (a) A Bayesian Network topology where node $X3$  has parents $X1$ and $X2$; (b) therefore, for the equivalent undirected graph the separated parents of $X1$ and $X2$ must be linked or \emph{normalized}. The second network topology details the moralized graphical model. }
			\label{Bayesian-Network-1}
		\end{figure*}	
	\end{landscape}
	
	
	\begin{landscape}
	\begin{figure*}[tb] 
		\centering
		\includegraphics[width=0.35\textwidth]{DAG_1_.pdf} 
		\includegraphics[width=0.35\textwidth]{DAG_3_.pdf} 
		\includegraphics[width=0.45\textwidth]{DAG_4_.pdf} 
		\caption{Topology of $\kappa_{A}$ a \emph{Directed graphical models} or \emph{Bayesian network} with a \emph{Markov blanket} for node A and the topology of $\kappa_{B}$ an equivalent \emph{Undirected graphical models} or \emph{Markov network}, the moralised graph of $\kappa_{A}$. We note that the class of the probabilistic model is defined as $P[A, ..... , G] = P[B].P[C].P[F].P[D|B].P[E|C].P[A|E,D].P[A'|E].P[G|A,F]$; this corresponds to the undirected Markov network Factor Graph $\kappa_{C}$ on the right. }
		\label{markov-blankets}
	\end{figure*}
\end{landscape}



\subsubsection{Factor Graphs}
The requirement in Bayesian Networks and \st{Markov Networks} that $\mathcal{\kappa}$ is a perfect map holds when $\mathcal{I}_G = \mathcal{I}_P$. However $P$ will not always be representable as a perfect map under a DAG.  Factor Graphs \cite{kschischang2001factor} combine directed and undirected graphical models; they are an undirected bipartite graph, squares represent \emph{factor nodes}, round nodes representing random variables and edges from all random variables to any such node where it is represented. A joint probability distribution of a factor graph is given by,
\begin{equation}
P[X_1,X_2,...,X_n] = \dfrac{1}{Z} \prod_{j=1}^{m} f_i (x_{f_j})
\end{equation}
The factor graph is not constrained therefore to have factors that are maximal cliques allowing for a more explicit representation through factorization. (add a picture	)

\section{Inference models.}
Probabilistic inference on a graphical models, i.e. using a distribution as our model in series of queries, considers techniques such as 1) \emph{evidence propagation algorithms}, consideration for the impact of belief propagation  on the parameters of models in the context of conditional probabilities and 2) \emph{model validation} which considers whether the outputs of a graphical models  are accurate for new data or known data.  Therefore for a graphical model where $\theta \rightarrow \mathcal{D}$, an inference algorithms compute $p[\theta|x]$, the posterior probability of parameter $\theta$ given evidence $x_i$. Probabilistic inference with graph models is typically faster than explicitly solving a joint distribution. 

%\st{A path is a sequence of distinct adjacent vertices. A directed path is a path along directed edges that follows the direction of the arrowheads. A directed cycle is formed by a directed path from Xi to Xj together with the edge Xj → Xi. A (partially) directed graph is called a (partially) directed acyclic graph if it does not contain directed cycles.}%

\begin{figure}
	\centering
	\includegraphics[width=0.30\textwidth]{y-to-x.pdf}
	\caption{ \st{Additionally score-search based} Score-based paradigms are generally less efficient than constraint-based algorithms; score-based paradigms when provided a semantic framework will tend to produce a single graph although without a relative degree of confidence.}
	\label{score-based-paradigm}
\end{figure}
	
\subsection{Evidence-based algorithms.}
Evidence-based propagation algorithms includes both soft evidence and hard evidence techniques, in a sensitivity analysis of specific inputs is called  \emph{soft evidence} or when setting a specific input to a new fixed value then thereby conditioning on the choice of parameters in the network to specific values of a node, this is called \emph{hard evidence} or \emph{conditional probability query}. Two standard algorithms for conditional probability queries are \emph{variable elimination} and \emph{message passing}; these algorithms are not the same \cite{cozman2013robustness,castillo2012expert,pearl1988probabilistic}.
	
\subsection{Model-based algorithms.}
Model-based validation algorithms involve \emph{cross-validation} or \emph{bootstrapping} large cohorts of network structures from a limited dataset. Model adequacy is then determined based off of a goodness-of-fit or loss criteria as the measure of structural confidence; the optimal model is \st{considered as having} that which has the  highest  validation likelihood or network structural confidence \cite{koller2009probabilistic,pena2005learning}.  highly-generalized settings where a posterior probability $p[\theta|x]$ is assumed Gaussian distributed, approximate inference algorithms may be used. 

\section{Learning models.}
For any probabilistic graphical model a network structure $\mathcal{G}$, with corresponding {parameters} \st{variables} $\theta$, this \st{network} dependence structure must be parametrized or learned before it may be used to provide any feedback  in a probabilistic inference query. The process of parametrizing $\theta$ in the graphical model is termed \emph{learning} and  \emph{estimation}. Learning  \st{This may} considers 1) \emph{structured learning}, that is encoding any conditional independences within a global distribution thereby achieving the minimal I-map $\mathcal{M}$ or otherwise identify a less complex \emph{network structure} $\tilde{\mathcal{M}}$ that best matches the target distribution ${\tilde{P}}$, or 2) \emph{parametrized learning}, estimating the parameters of the local distribution in our probability space. Even though both techniques may yield strong performance, there is still considerable scope for the use of established knowledge for the purposes of building \emph{expert systems}, which achieve considerable gains in performance by accelerating the learning process \cite{andreassen1996evaluation, beinlich1989alarm, abramson1996hailfinder}.
	
The goal of finding a \st{perfect} match given a target distribution ${\tilde{P}}$ is  however not achievable computational due to limited data; we often therefore aim for a suboptimal goal of \emph{density estimation}, i.e. finding an approximate "best-fit" ${\mathcal{M}^*}$ such that for the generating distribution ${P^*}$:
	\begin{equation}
	{\tilde{P}} \approxeq  {P^*} 
	\end{equation}
note \st{therefore that} this best-fit ${\mathcal{M}^*}$ requires  we define a performance criteria \emph{a priori} using a  function $D$ , such as the \st{Kullback-Leibler divergence (KL-divergence) or a} \emph{relative entropy} as the performance criteria. We can therefore compute the performance metric over an instance $\xi$ as,
	\begin{equation}
	\begin{split}
	\arg \min D[\mathcal{{P^*}}||\mathcal{\tilde{P}}] & = \arg \min \mathbf{E}_{\xi \sim P^*} \Biggr[ \log  \Bigr[ \dfrac{{P^*}[\xi]}{\tilde{P}[\xi]} \Bigr] \Biggl] \\
	& = \arg \min \mathbf{E}_{\xi \sim {P^*}}  \Biggr[ \log {P^*}[\xi] - \log \tilde{P}[\xi]   \Biggl] \\
	& = \arg \min \Biggr[  \mathbf{E}_{\xi \sim {P^*}} \ \Bigr[ \log {{P^*}}[\xi]  \Bigl] - \mathbf{E}_{\xi \sim {P^*}}  \Bigr[  \log \tilde{P}[\xi] \Bigl] \Biggr]  \\
	& = \arg \min  \Biggr[ - \mathbf{H}[\chi] - \mathbf{E}_{\xi \sim{P^*}}  \Bigr[  \log {P^*}[\xi] \Bigl]  \Biggl] 
	\end{split} 
	\label{mark}
	\end{equation}
evaluating \st{this expression} \eqref{mark} \st{for a metric} we \st{may} interpret this as minimising the difference in bits between the sequence transmitted at the source and the sequence obtained by our model. Note that the second term in this relative entropy $ \mathbf{E}_{\xi \sim {P^*}}  \Bigr[  \log {P^*}[\xi] \Bigl]$, \st{which is called} the \emph{expected log-likelihood} should be maximized. \st{Therefore,} The higher a probability the model ${\mathcal{M}^*}$ assigns to points  from the true distribution, the more reflective ${{P^*}}$ is of a generating distribution  ${\tilde{P}}$. More generally, given data $\mathcal{D} = \{x_n\}_{n=1}^N$, during learning we consider the \emph{likelihood} $P[\mathcal{D,M}]$ or the log-likelihood $ \mathcal{l} [\mathcal{M}] = \log P[\mathcal{D,M}] $.
\st{However, we are limited} In practice because we do not know the form of the target distribution $P^*$, given $\mathcal{D}$ we therefore can approximate the expected log-likelihood with the \emph{empirical risk minimizer} \st{which averages}  by averaging  across a set sampled from a distribution $\tilde{P}$. The effectiveness or accuracy of a classifier $f: \mathcal{R}^d$ is determined using {empirical risk minimization},
	\begin{equation}
	 \arg \min \Biggr[  \log P[\mathcal{M}] \Biggl] =  \arg \min \Biggr[ \sum_{m=1}^M \log P[\xi_m,\mathcal{M}] \Biggl] 
	\end{equation}
however  this measure considers an entire probability over an instance $\xi$ in fitting a target distribution ${P}^*$; practically we are interested rather in using our model $\mathcal{M}$ to consider a random variable $X$ evaluated over the distribution ${{P}^*}$.  In the special case we might consider random pairs of labelled date $[X,Y]$; set of received variables $Y$ given the features $X$.  A learned model can be developed to provide feedback from $P[Y | X]$, which is typically the goal of artificial intelligence and pattern recognition \st{machine learning}. The model when trained in prediction for an instance of the random variable $X=x$ is therefore characterized by the  probability distribution $\tilde{P}[Y | x]$. An error \st{therefore  is said to occur when} $f(X) \neq Y$  in the classification of a single point is given $\mathcal{L}[ y, f(x)]$ occurs with the corresponding probability,
	\begin{equation*}
	\mathcal{L}[ y, f(x)] = \mathbf{Pr}[f(X) \neq Y]
	\end{equation*}
\st{where the error obtained.} Therefore the \emph{error rate} or \emph{training error rate} across all responses when observing observations of $y$ is \st{would be} given by
	\begin{equation*}
	\varepsilon_{} = \dfrac{1}{N} \sum_{i=1}^{N} \mathcal{L} [ y_i, f(x_i) ]
	\end{equation*}
for example a \emph{mean-squared-error} in this form is given by $\mathcal{L} [ (y, f(x) ] = 1/N \sum_{i=1}^{N} (y_i - \hat{y}_i)$. 
%%	\st{Fitting probabilistic graph models using} 


%\subsection{\st{Structured learning.}}
\subsection{Solving a Bayesian Network}
\emph{"What if $\mathcal{G}$ is not given?"}. Solving a Bayesian Network will require two main phases: 1) determination of the most appropriate graphical structure of $\mathcal{G}$ and 2) a determination of its parameters $\theta$. The graphical structure of the Bayesian Network and its parametrisation can be determined through \emph{structured learning}. Structured learning, is the encoding of any conditional independences within a global distribution thereby achieving the minimal I-map $\mathcal{M}$ or otherwise identifying a less complex \emph{network structure} $\tilde{\mathcal{M}}$ that best matches the target distribution ${\tilde{P}}$. Suppose therefore we are learning \st{the parameters in the} a network structure $\mathcal{G}$  with corresponding {parameters} \st{variables} $\theta$; note that the number of possible directed graphs given $\kappa_n$ variables grows exponentially as $|\kappa_n| = 3^{{n(n-1)}/{2}}$;
however learning \st{algorithms} using naïve exhaustive search in itself would be considered NP-hard {as}  the solution space of graph models \st{also grows} will grow exponential with $n$ variables.  \st{the number of variables} \cite{robinson1973counting},
\begin{equation}
|\mathcal{G}_n| = \sum_{i=1}^n (-1)^{i-1}  \begin{pmatrix}
n\\ 
i 
\end{pmatrix}
2^{i(n-1)} |\mathcal{G}_{n-i}|
\end{equation}
obtaining $|\mathcal{G}_n|$ a \emph{directed acyclic graphs} (DAGs) of $n$ variables. We observe  in Table \ref{DAGS1} that for \st{structured} learning, the \emph{naive exhaustive search} \st{algorithms} \st{provide no measurable performance improvement} become increasingly intractable for problems where $n\gg1$.   A number of structured learning algorithms have therefore been developed in the literature,  the families of \emph{score-based learning} and \emph{constraint based learning} algorithms. In general score-based learning maximizes an objective function during a search over candidate graph models; this  thereby assumes that a winning \st{resulting} graph model's representation of the probability distribution \st{perfectly maps a network structure} is the best map of a dependence \st{network} structure.  \emph{Constraint-based learning} algorithms \st{which} utilize statistical and information theoretic conditional independence (CI) tests, \st{The CI tests allow therefore for a} effectively restricting  {the} search space \st{for any subsequent score-based search}. A third subclass of algorithms are called \emph{hybrid algorithms} which combine elements of both score-based and constraint-based learning \cite{scutari2019learns, scanagatta2019survey}. Note \st{therefore} however that structure learning algorithms in Bayesian Networks and Markov Networks, share common assumptions:
\begin{enumerate}
	\item A one-to-one correspondence between nodes $\mathcal{V}$ of $\mathcal{\kappa}$ and random variables of the probabilistic model $\chi$; multiple nodes which are based on a single random variable are not catered for in a graph; therefore a graphical model $\mathcal{\kappa}$  is a \emph{dependency map} (or \emph{D-map)} of a probabilistic model $P$ of $\chi$ such for the subsets $X_1, X_2,X_3 \in \chi$ iff $X_1 \inperp_P X_2 | X_3 \Longrightarrow X_1 \inperp_G X_2 | X_3$. Correspondingly $\mathcal{\kappa}$ is an \emph {independence map} (or \emph{I-map}) of a probabilistic model $P$ of $\chi$ iff $X_1 \inperp_P X_2 | X_3 \Longleftarrow X_1 \inperp_G X_2 | X_3$; an I-map condition guarantees that two nodes $X_1$ and $X_2$ will correspond to seperate random variables, when separated by a third node $X_3$ in a mapping\footnote{A more rigorous requirement is that of a \emph{minimal I-map}; that is where a probabilistic model $P$ of $\chi$ can be represented by more than one graph we adopt the graph with the least number of edges; any removal of additional edges and the graph is no longer an I-map}.  Also $\mathcal{\kappa}$ is called a \emph{perfect map} of a probabilistic model $P$ of $\chi$ if it is both its I-map and its D-map; i.e. $X_1 \inperp_P X_2 | X_3 \Longleftrightarrow X_1 \inperp_G X_2 | X_3$. 
	\item A node therefore in a Bayesian network may not have parent nodes that are based on latent or hidden variables; this would result in a topology \st{dependency structure} that is not fully observable \cite{elidan2005learning, binder1997adaptive}. Therefore we often test graphical models using a sets of algorithms \cite{dempster1977maximum, friedman1997bayesian}.
	\item  Relationships between random variables in the model must be conditionally independent; this assumption is a requirement for probabilistic graphical models. 
	\item Any combination of random variables in $\chi$ must represent an identifiable event, even if an observable event is unlikely. {(REMOVE)This assumption implies a strictly positive global distribution, which is needed to have uniquely determined Markov blankets and, therefore, a uniquely identifiable model. Note constraint-based algorithms do not have this as a requirement, as the existence of a  is a sufficient condition for the uniqueness of the Markov blankets.(REMOVE)}
\end{enumerate}

\begin{table}
\begin{center}
\begin{tabular}{ccc}
	\toprule
	variables, & no. of directed graphs,  & no. of DAGs,  \\
	$n$  & $|\mathcal{\kappa}_n|$ &  $|\mathcal{G}_n|$  \\
	\toprule
	2 & 3 & 3    					\\
	3 & 27 & 25   				  \\
    4 & 729 & 543  				\\
    5 & 59,049 & 29,281 	 \\
    \toprule

    %%https://arxiv.org/pdf/2109.11415.pdf
\end{tabular}
\caption{Quantity of directed graphs and directed acyclic graphs for $n$ variables.}
 \label{DAGS1}
\end{center}
\end{table}
\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{DAGs.pdf} \\
		\caption{Note, DAG $\mathcal{\kappa}_1$ of an ordering $\{A, B, C, D\}$ where the probability function has a factorisations $p[A,B,C,D,E] = p[A]p[B|A]p[C|A]p[D|C]$ and DAG $\mathcal{\kappa}_2$  with the ordering $\{B,A,C,D \}$ where the probability function has a  factorisations $p[A,B,C,D,E] = p[B]p[A|B]p[C|B]p[D|C]$. The DAGs of Bayesian Networks that are \emph{I-equivalent} in terms of their scoring as they share the same skeleton. ***add the skeleton here and two more I-equivalents }
		\label{DAGs}
				\includegraphics[width=0.25\textwidth]{Skeleton_DAG.pdf} 
	\includegraphics[width=0.25\textwidth]{PDAGs.pdf} 
		\caption{An equivalent Partially Directed Acyclic Graph (PDAG) from the DAGs. (add it's skeleton)}
	\label{PDAGs}
	\includegraphics[width=0.25\textwidth]{CDAGs.pdf}
	\caption{A Complete Partially Directed Acyclic Graph (CPDAG) from the PDAG.}
	\label{CPDAGs}
\end{figure}



\subsubsection{Constraint-based  algorithms.}	
Constraint-based \st{learning algorithms use} adopt CI tests to construct a \emph{minimal I-map} of network structures: the goal of these algorithms is to examine the conditional-independence relationships  between variables of a network structure, that is the minimal I-map of $P$. {Constraint-based algorithms} \st{which} adopt statistical and information theoretic criterion\st{frameworks}. For a given distribution $P$ the constraint-based learning algorithm runs queries on candidate DAGs such as \emph{"Does P satisfy $(X_3 \inperp X_4,X_5 | X_6)$?"} and receives \emph{"1/0"} as feedback. Often for a given objective set of independences there will be several candidate \emph{directed acyclic graphs} (DAGs), wherein the constraint-based algorithm will isolate a set of candidate DAGs or an \emph{equivalence class}. Multiple DAGs are considered as belonging to the an equivalence class if they share 1) adjacencies and 2) sets of v-structures. Figure \ref{DAGs} describes two DAGs belonging to the same equivalence class. A \emph{Partially Directed Acyclic Graph} (PDAG) then provides a compact representations for multiple candidate DAGs; this class PDAG has limited variables thus limiting the number of independence queries. Figure (a) details the PDAG, with  V-structure $A \rightarrow C \leftarrow B$. This PDAG representation however does not assume any other implicit v-structures; a later consideration for implicit directed arrows we obtain a \emph{Complete Partially Directed Acyclic Graph} (CPDAG) or equivalence class. Figure (a) details the CPDAG; note therefore that the corresponding DAGs must have a directed edge consistent with the CPDAG but where there is an undirected edge, the DAGs may adopt a directed or undirected edges.

Developed initially for Bayesian Networks, constraint-based algorithms evolved from the \emph{Inductive Causation} algorithms that were used in learning causal networks \cite{verma1991equivalence}. However,  \st{the}  {Inductive Causation} \st{algorithm} is computationally intractable for large problems as they consider an exponential number of conditional independence relationships with the number of variables. The PC algorithm addressed these limitations \cite{spirtes1991algorithm, colombo2014order, spirtes2000causation}; this algorithm learns from  DAGs under an assumption of \emph{casual sufficiency}. Note that when the output of a PC algorithm is applied to data, it is generally order-dependent, in the sense that its output depends on the order in which the variables are given to the PC algorithm. An implementation of the PC algorithm is highlighted in \textbf{Algorithm 1} and \textbf{Algorithm 2}. 
	
\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwInOut{Require}{Require}
	\Require{conditional independence information among all variables in $\mathcal{V}$, and an ordering order ($\mathcal{V}$) on the variables.}
	\Input{input features, $\mathbf{X}$} 
	\Input{weights, $\mathbf{W}$}
	\Output{output features,$\mathbf{Y}$}
	\vspace{0.1in}
	\vspace{0.05in}
\emph{
Adjacency search: $\mathcal{C} \leftarrow$  is a skeleton graph and separation sets found using the 'Adjacency search algorithm'; \\
Compute: $\mathcal{C} \leftarrow$ is a skeleton graph with the orientated unshielded triples based on the separation sets; \\
Compute: $\mathcal{C} \leftarrow$ is a skeleton graph with as many of the remaining undirected edges orientated as possible by \st{repeated application of the following rules} iterating through the following steps:\\
\begin{itemize}
\item Step 1: orient  into whenever there is a directed edge such that and  are not adjacent (otherwise a new v-structure is created);\\
\item Step 2: orient $X_i  - X_j$ into $X_i \rightarrow X_j$ whenever there is a chain $X_i \rightarrow X_k \rightarrow X_j $ (otherwise a directed cycle is created);\\
\item Step 3: orient $X_i - X_j $ into $X_i \rightarrow X_j$ whenever there are two chains $X_i - X_k \rightarrow X_j$ and
$X_i  - X_l  \rightarrow X_j$ such that $X_k$ and $X_l$ are not adjacent (otherwise a new v-structure or
a directed cycle is created). \\
\end{itemize}
}
4: \textbf{return}: $\mathcal{C} \leftarrow$  is the output graph $\mathcal{C}$  and the separation sets. \\
	\caption{The PC-algorithm.}
\end{algorithm}

\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwInOut{Require}{Require}
	\SetKwInOut{If}{If}
	\Require{conditional independence information among all variables in $\mathcal{V}$, and an ordering order ($\mathcal{V}$) on the variables.} 
	\Input{input graph, $\mathcal{C} $} 
	\Output{output graph with separation sets, $\mathcal{C} $}
	
	\vspace{0.1in}
	\vspace{0.05in}
	\emph{Form the complete undirected graph $\mathcal{C}$ on the vertex set $\mathcal{V}$;} \\
		\vspace{0.05in}
			Let $l$ = -1; \\
	\While{ there exists pairs of adjacent vertices ($X_i, X_j$) in $\mathcal{C}$  satisfy adj$|(\mathcal{C},X_i)  X_j| \geq l$}
	{
		Let $l$ = $l$  + 1; \\
		\While{there exists ordered pairs of adjacent vertices ($X_i, X_j$) in $\mathcal{C}$  that satisfy adj$|(\mathcal{C},X_i) \setminus X_j| \leq l$} 		
		{
			Select a (new) ordered pair of vertices ($X_i, X_j$) that are adjacent in $\mathcal{C}$  and satisfy $|adj(\mathcal{C}, X_i) \setminus \{X_j\}| \geq l$, using order($\mathcal{V}$);\\
			\While{$X_i$ and $X_j$  are longer adjacent in  $\mathcal{C}$ or all $S \subseteq adj(\mathcal{C}, X_i) \setminus \{X_j\}$ with
				$|S| = l$ have not been considered} 
			{
			Choose a (new) set $S \subseteq adj(\mathcal{C}, X_i) \setminus {X_j}$ with $|S| = l$, using order($\mathcal{V}$); \\
			\textbf{if} $X_i$ \emph{and $X_j$  are conditionally independent given $\mathbf{S}$}  \textbf{then} \\
			\hspace{0.25in} \emph{Delete edge $X_i - X_j$ from $\mathcal{C}$}; \\
			\hspace{0.25in} \emph{Let sepset ($X_i, X_j$) = sepset($X_i, X_j$)= $\mathbf{S}$};\\
			\textbf{end if} \\
			\textbf{until} $Xi$ and $Xj$ are no longer adjacent in C or all $S \subseteq adj(\mathcal{C}, X_i) \setminus {X_j}$ with $|S|  = l$ have been considered
		}
		\textbf{until} all ordered pairs of adjacent vertices ($X_i, X_j$) in $\mathcal{C}$ satisfy $| adj(\mathcal{C}, X_i) \setminus \{ {X_j} \}| \leq l$
	}
	}

 \textbf{return}: $\mathcal{C}$, sepset.
 \caption{Adjacency search algorithm.}
 (also have the above in C++)
\end{algorithm}
\subsubsection{Score-based  algorithms.}
\st{The family of }Score-based learning algorithms search a cohort of candidate network structures and assign a score or degree of confidence reflecting a \st{the} goodness of fit which then serves as the objective function when evaluating a search space of candidate graphs. This a multiple-step process \st{which involves} will involve determining 1) a path that must be followed from a set of candidate graphs and 2) scoring off of an objective function. Score-based algorithms find the graphical structure  \st{the Bayesian Network structure} that maximizes a \st{given} score function, while constraint-based algorithms use statistical tests to learn conditional independence relationships (called constraints) from the data and infer the presence or absence of particular arcs. A score-based algorithm is depicted in Figure \ref{score-based-paradigm}.

Generally the class or subset of graphs that \st{whose network structure will} fit the data $\mathcal{D}$ \st{these graphs} are termed \emph{Markov equivalents}. Due to a search considering numerous undirected graphs and directed acyclic graphs, \st{score-search based} score-based algorithms will tend to be less sensitive than constraint-based approaches to any error propagation  (Sprites 2010).  However as the number of network structures  is exponential in the number of nodes, typically such naive searches are not exhaustive \st{an exhaustive search is typically not feasible} \cite{harary2014graphical} unless the graphs have implicitly a limited number of variables. Heuristics optimisation algorithms are usually adopted in score-based \st{although these} algorithms but they usually achieve only local optimum as opposed to a global optimum due to noise or limited data.  Additionally \st{score-search based} score-based  paradigms are generally less efficient than constraint-based algorithms, which provided a semantic framework, will tend to produce a single graph although without a relative degree of confidence.

%%https://www.cs.cmu.edu/~epxing/Class/10708-17/homework-17/hw1/HW1-sol.pdf
%%https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture2.pdf
Different directed graphs are considered as equivalent or \emph{I-equivalent} when they encode the same set of conditional independence properties; an I-equivalence of two graphs indicates that for a given distribution P that may be factorized over one directed graphs can also be factorized over other. For example the Markov conditions in Figure \ref{DAGs}, all imply the following conditional independence $D \inperp \{A,B\} | C$ and the graphs are therefore I-equivalent as they share the same conditional independence. A relevant concept to describe I-equivalence is a \emph{skeleton}. For example the graphs in Figure \ref{DAGs} share a skeleton depicted in Figure \ref{Skeleton}. \st{that is the skeleton of a Bayesian network graph $\mathcal{\kappa}$ over $\mathcal{X}$, is an undirected graph over $\mathcal{X}$ that contains an edge \{X, Y\} for every edge (X, Y ) in G.}
		
\subsection{Parametrized learning.}
%%\subsection{Parameter estimation.} https://www.cs.cmu.edu/~awm/15781/slides/Param_Struct_Learning05v1.pdf
Parametrized learning algorithms condense a given data set using $\theta$ a set of parameters of constant size; irrespective of the size in the subsequent out-of-sample data our parametrized model will not increase the number of parameters.  Given $\mathcal{D} = \{x_n \}_{n=1}^N$  the complete data set of variables for $\mathcal{\chi}$ and $\mathcal{G}$ the \st{network} dependence structure over the variables, any local distributions will tend to have a limited number of parameters $p$ that must be estimated from a sample and a constant dimensionality\footnote{The inherent difficulties in parameter estimation as the dimensionality of $X$ increases limits our ability to learn Bayesian Networks, particularly where $p_g$ is a large set space such as in image processing.}; consequently estimates from the local distribution will  tend to be more accurate estimating $p_g$, the number of the parameters from a single global distribution. However for a global distribution $p_g= \sum_{i} p_i $; further $p_g$  this can typically be reduced for example due to conditionally dependence relationships inherent in the local distributions. In practice parameter estimation $p_g$  is typically an involved process particularly where for example  $n[\mathcal{D}] \ll p_g$, leading to \emph{overfitting};  we define  $n[\mathcal{D}]$  as the number of entries in our data sample \cite{castelo2006robust, schafer2005learning, hastie2009elements}. Additionally when $n[\mathcal{V}]< n[\mathcal{E}]$  parameter estimation will also tend to be computational infeasible \cite{korb2010bayesian, koller2009probabilistic};  we define $n[\mathcal{V}]$ as the number of nodes in the network structure and $n[\mathcal{E}]$ as the number of edges. Note depending on prior assumptions, parametrized learning algorithms can be solved at each node using \emph{frequentist parameter estimation} techniques where we assume $\theta$ are unknown but constant, such as \emph{Maximum Likelihood Estimation} or \emph{Maximum A Posteriori} estimation given data $\mathcal{D}$ and a subjective prior. Alternatively \emph{Bayesian parameter estimation} techniques may also be considered where we assume $\theta$ are unknown and random variables; estimates from Bayesian estimation techniques would therefore differ based on the prior.
\begin{figure}
	\centering
	\includegraphics[width=0.40\textwidth]{plate_diagram.pdf}
		\caption{Additionally \st{score-search based} score-based  paradigms are generally less efficient than constraint-based algorithms, which provided a semantic framework, will tend to produce a single graph although without a relative degree of confidence.}
\end{figure}
		% Uncomment the following two lines if you want to have a bibliography
		% 	\bibliographystyle{apalike}
		
\bibliographystyle{amsplain}
\bibliography{graphmodels.bib}

\textbf{topological ordering, maximal length, descendants, non-descendants, }
	
	
	\end{document}